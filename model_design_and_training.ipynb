{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuVfTj6iyn1-"
   },
   "source": [
    "# Dual CNN Cross-Attention Transformer Training\n",
    "\n",
    "Training pipeline for anomalous diffusion parameter estimation (D, Î±) and model classification.\n",
    "\n",
    "Architecture: Dual CNN branches â†’ Transformer encoders â†’ Cross-attention â†’ Multi-task outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3366,
     "status": "ok",
     "timestamp": 1763315799739,
     "user": {
      "displayName": "Enow George",
      "userId": "04500551558329927866"
     },
     "user_tz": 0
    },
    "id": "guiE-7U4y5qJ",
    "outputId": "e481af6b-e283-40c6-fd47-ed3bf5621d28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Running on Google Colab\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "âœ“ Google Drive mounted\n"
     ]
    }
   ],
   "source": [
    "# Check if running on Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running on Google Colab\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1763315799751,
     "user": {
      "displayName": "Enow George",
      "userId": "04500551558329927866"
     },
     "user_tz": 0
    },
    "id": "XLkbgtfB7ypi",
    "outputId": "2a9bebe6-de02-432a-f872-4503971a2e4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleanup:\n",
      "  GPU memory allocated: 11.75 GB\n",
      "  GPU memory reserved: 11.96 GB\n"
     ]
    }
   ],
   "source": [
    "# Memory cleanup (run if re-executing cells)\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    del model\n",
    "if 'train_loader' in locals() or 'train_loader' in globals():\n",
    "    del train_loader, val_loader, test_loader\n",
    "if 'train_dataset' in locals() or 'train_dataset' in globals():\n",
    "    del train_dataset, val_dataset, test_dataset\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB allocated, {torch.cuda.memory_reserved()/1024**3:.2f} GB reserved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGS93VRbyn2A"
   },
   "source": [
    "## Cell 0: Environment Detection (Local vs Google Colab)\n",
    "\n",
    "This cell automatically detects whether you're running locally or on Google Colab and sets up paths accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3299,
     "status": "ok",
     "timestamp": 1763315803101,
     "user": {
      "displayName": "Enow George",
      "userId": "04500551558329927866"
     },
     "user_tz": 0
    },
    "id": "jRBeSSxcyn2A",
    "outputId": "c36d71ab-98e5-444a-9eca-8b32da4ba3ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Google Colab\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Changed working directory to: /content/drive/MyDrive/ERP_Shrey\n",
      "Current working directory: /content/drive/MyDrive/ERP_Shrey\n",
      "IN_COLAB: True\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT DETECTION (Local vs Google Colab)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running on Google Colab\")\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Set base directory to your Drive folder\n",
    "    BASE_DIR = '/content/drive/MyDrive/ERP_Shrey'\n",
    "\n",
    "    # Change working directory\n",
    "    os.chdir(BASE_DIR)\n",
    "    print(f\"Changed working directory to: {BASE_DIR}\")\n",
    "\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")\n",
    "\n",
    "    # Get the directory where the notebook is located\n",
    "    try:\n",
    "        # Jupyter notebook environment\n",
    "        BASE_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "        if BASE_DIR == '' or BASE_DIR == '/home/magjun/Documents/ERP_Shrey/Report_V2_Preprocessing_and_training':\n",
    "            BASE_DIR = os.getcwd()\n",
    "    except:\n",
    "        BASE_DIR = os.getcwd()\n",
    "\n",
    "    print(f\"Base directory: {BASE_DIR}\")\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"IN_COLAB: {IN_COLAB}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_8lBlepyn2B"
   },
   "source": [
    "## Cell 1: Environment Setup & Library Installation\n",
    "\n",
    "- Installs required libraries\n",
    "- Checks CUDA availability and GPU specifications  \n",
    "- Sets up reproducibility (random seeds)\n",
    "- Prints system information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1763315803284,
     "user": {
      "displayName": "Enow George",
      "userId": "04500551558329927866"
     },
     "user_tz": 0
    },
    "id": "q_xjO2obyn2B",
    "outputId": "526e99ef-29e9-4819-f65d-3a0c51b7a957"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHECKING & INSTALLING REQUIRED PACKAGES\n",
      "======================================================================\n",
      "âœ“ NVIDIA GPU detected\n",
      "  Installing PyTorch 2.0.1 with CUDA 11.7\n",
      "  (Last version with full support for GTX 1070 Ti / compute 6.1)\n",
      "\n",
      "âœ“ PyTorch 2.8.0+cu126 already installed\n",
      "  âœ“ CUDA 12.6 support enabled\n",
      "  GPU compute capability: (8, 0)\n",
      "  âœ“ GPU is working correctly\n",
      "\n",
      "âœ“ h5py already installed\n",
      "âœ“ matplotlib already installed\n",
      "âœ“ seaborn already installed\n",
      "âœ“ scikit-learn already installed\n",
      "âœ“ tqdm already installed\n",
      "âœ“ pandas already installed\n",
      "âœ“ numpy already installed\n",
      "\n",
      "âœ“ All required packages are installed!\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ENVIRONMENT INFORMATION\n",
      "======================================================================\n",
      "Python version: 3.12.12\n",
      "PyTorch version: 2.8.0+cu126\n",
      "NumPy version: 2.0.2\n",
      "\n",
      "GPU INFORMATION:\n",
      "âœ“ CUDA is available\n",
      "  CUDA version: 12.6\n",
      "  Device count: 1\n",
      "  Current device: 0\n",
      "  Device name: NVIDIA A100-SXM4-80GB\n",
      "  Device memory: 79.32 GB\n",
      "  Compute capability: (8, 0)\n",
      "  âœ“ CUDA operations working correctly\n",
      "\n",
      "âœ“ Random seeds set to 42 for reproducibility\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Auto-install missing packages\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_if_missing(package_name, import_name=None, pip_args=None):\n",
    "    \"\"\"Install package if not already installed\"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package_name\n",
    "\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        print(f\"{package_name} already installed\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package_name}...\")\n",
    "        try:\n",
    "            cmd = [sys.executable, \"-m\", \"pip\", \"install\"]\n",
    "            if pip_args:\n",
    "                cmd.extend(pip_args)\n",
    "            else:\n",
    "                cmd.append(package_name)\n",
    "            subprocess.check_call(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "            print(f\"{package_name} installed\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"Failed to install {package_name}\")\n",
    "            return False\n",
    "\n",
    "def check_cuda_available():\n",
    "    \"\"\"Check if NVIDIA GPU is available\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        return result.returncode == 0\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "\n",
    "print(\"Checking and installing packages...\")\n",
    "\n",
    "has_nvidia_gpu = check_cuda_available()\n",
    "if has_nvidia_gpu:\n",
    "    print(\"NVIDIA GPU detected - installing PyTorch 2.0.1 with CUDA 11.7\")\n",
    "else:\n",
    "    print(\"No NVIDIA GPU - installing CPU-only PyTorch\")\n",
    "\n",
    "# Install PyTorch with CUDA support if GPU is available\n",
    "torch_installed = False\n",
    "need_reinstall = False\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch {torch.__version__} already installed\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA {torch.version.cuda} support enabled\")\n",
    "        capability = torch.cuda.get_device_capability(0)\n",
    "        print(f\"GPU compute capability: {capability}\")\n",
    "\n",
    "        try:\n",
    "            test_tensor = torch.randn(10, 10).cuda()\n",
    "            del test_tensor\n",
    "            print(\"GPU working correctly\")\n",
    "        except Exception:\n",
    "            print(\"GPU not compatible - need PyTorch 2.0.1 for compute 6.1\")\n",
    "            need_reinstall = True\n",
    "    else:\n",
    "        if has_nvidia_gpu:\n",
    "            print(\"PyTorch installed but CUDA not available - reinstalling...\")\n",
    "            need_reinstall = True\n",
    "\n",
    "    if need_reinstall:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"torch\", \"torchvision\", \"torchaudio\", \"-y\"],\n",
    "                            stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "        raise ImportError(\"Need to reinstall PyTorch 2.0.1\")\n",
    "\n",
    "    torch_installed = True\n",
    "\n",
    "except (ImportError, RuntimeError):\n",
    "    print(\"Installing PyTorch 2.0.1 with CUDA 11.7 (supports compute 6.1)...\")\n",
    "    try:\n",
    "        if has_nvidia_gpu:\n",
    "            # Install PyTorch 2.0.1 with CUDA 11.7 - last version with sm_61 support\n",
    "            subprocess.check_call([\n",
    "                sys.executable, \"-m\", \"pip\", \"install\",\n",
    "                \"torch==2.0.1\", \"torchvision==0.15.2\", \"torchaudio==2.0.2\",\n",
    "                \"--index-url\", \"https://download.pytorch.org/whl/cu117\"\n",
    "            ])\n",
    "            print(\"PyTorch 2.0.1 with CUDA 11.7 installed\")\n",
    "        else:\n",
    "            subprocess.check_call([\n",
    "                sys.executable, \"-m\", \"pip\", \"install\",\n",
    "                \"torch==2.0.1\", \"torchvision==0.15.2\", \"torchaudio==2.0.2\"\n",
    "            ])\n",
    "            print(\"PyTorch 2.0.1 (CPU) installed\")\n",
    "        torch_installed = True\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"Failed to install PyTorch\")\n",
    "\n",
    "# Install other packages\n",
    "packages = [\n",
    "    ('h5py', 'h5py'),\n",
    "    ('matplotlib', 'matplotlib'),\n",
    "    ('seaborn', 'seaborn'),\n",
    "    ('scikit-learn', 'sklearn'),\n",
    "    ('tqdm', 'tqdm'),\n",
    "    ('pandas', 'pandas'),\n",
    "    ('numpy', 'numpy'),\n",
    "]\n",
    "\n",
    "all_installed = torch_installed\n",
    "for package_name, import_name in packages:\n",
    "    if not install_if_missing(package_name, import_name):\n",
    "        all_installed = False\n",
    "\n",
    "if not all_installed:\n",
    "    print(\"Warning: Some packages failed to install\")\n",
    "else:\n",
    "    print(\"All packages installed\")\n",
    "\n",
    "# Import libraries\n",
    "\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import h5py\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix, r2_score\n",
    "\n",
    "# Print versions\n",
    "print(\"=\"*70)\n",
    "print(\"ENVIRONMENT INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print()\n",
    "\n",
    "# Check CUDA\n",
    "print(\"GPU INFORMATION:\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ CUDA is available\")\n",
    "    print(f\"  CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"  Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"  Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"  Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Device memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"  Compute capability: {torch.cuda.get_device_capability(0)}\")\n",
    "\n",
    "    # Test CUDA functionality\n",
    "    try:\n",
    "        test = torch.randn(100, 100).cuda()\n",
    "        result = test @ test.T\n",
    "        del test, result\n",
    "        print(f\"  âœ“ CUDA operations working correctly\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— CUDA test failed: {e}\")\n",
    "else:\n",
    "    print(\"âš  CUDA is NOT available - will use CPU (training will be slow)\")\n",
    "    print(\"  If you have an NVIDIA GPU:\")\n",
    "    print(\"  1. Ensure NVIDIA drivers are installed (run 'nvidia-smi' in terminal)\")\n",
    "    print(\"  2. Restart the Jupyter kernel after running this cell\")\n",
    "print()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"âœ“ Random seeds set to {RANDOM_SEED} for reproducibility\")\n",
    "print(\"=\"*70)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wI6gYKeTyn2C"
   },
   "source": [
    "## Cell 2: Configuration & Hyperparameters\n",
    "\n",
    "Central configuration for all training parameters. Toggle `TEST_MODE` to quickly test the pipeline with a small subset of data before full training.\n",
    "\n",
    "**TEST_MODE = True**: Uses 5K train, 500 val, 1K test samples, 5 epochs (pipeline testing)  \n",
    "**TEST_MODE = False**: Full dataset 1.35M train, 150K val, 500K test, 100 epochs (production training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 102,
     "status": "ok",
     "timestamp": 1763315803387,
     "user": {
      "displayName": "Enow George",
      "userId": "04500551558329927866"
     },
     "user_tz": 0
    },
    "id": "SSlu4on1yn2C",
    "outputId": "967bae09-5973-4413-aed5-f997a0f052fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Training Mode: ðŸš€ PRODUCTION (Full Training)\n",
      "\n",
      "Model Architecture:\n",
      "  d_model: 64\n",
      "  nhead: 8\n",
      "  Transformer layers: 2\n",
      "  Dropout: 0.1\n",
      "\n",
      "Training Hyperparameters:\n",
      "  Batch size: 128\n",
      "  Learning rate: 0.0004\n",
      "  Num epochs: 100\n",
      "  Gradient clip: 1.0\n",
      "  Device: cuda\n",
      "\n",
      "Loss Weights (Balanced Multi-Task):\n",
      "  Î»_D: 2.0 (D contribution: 20%)\n",
      "  Î»_Î±: 2.0 (Î± contribution: 20%)\n",
      "  Î»_model: 6.0 (model_type contribution: 60%)\n",
      "  Label smoothing: 0.1\n",
      "\n",
      "Normalization Constants:\n",
      "  D range: [0.0001, 100.0]\n",
      "  Î± range: [0.05, 2.0]\n",
      "\n",
      "Checkpointing:\n",
      "  Save every: 1 epoch(s)\n",
      "  Resume from: None (fresh start)\n",
      "\n",
      "ðŸš€ PRODUCTION MODE ENABLED:\n",
      "  Using full dataset (2M trajectories)\n",
      "  Checkpoints: Every 10 epochs (10, 20, 30, ..., 100)\n",
      "  Estimated training time: ~158 hours (6.6 days) on GTX 1070 Ti\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # ============= TRAINING MODE =============\n",
    "    'TEST_MODE': False,  # SET TO False FOR FULL TRAINING\n",
    "\n",
    "    # ============= PATHS =============\n",
    "    'dataset_path': os.path.join(BASE_DIR, 'data/andi/andi_dataset_table2_true__20251115_201643.h5'),\n",
    "    'output_dir': os.path.join(BASE_DIR, 'outputs3/'),\n",
    "    'checkpoint_dir': os.path.join(BASE_DIR, 'outputs3/checkpoints/'),\n",
    "    'plots_dir': os.path.join(BASE_DIR, 'outputs3/plots/'),\n",
    "    'logs_dir': os.path.join(BASE_DIR, 'outputs3/logs/'),\n",
    "\n",
    "    # ============= MODEL ARCHITECTURE =============\n",
    "    'd_model': 64,  # Reduced from 128 for 4Ã— faster attention (ANDI baseline)\n",
    "    'nhead': 8,  # Reduced proportionally (d_model must be divisible by nhead)\n",
    "    'dim_feedforward': 512,\n",
    "    'dropout': 0.1,\n",
    "    'num_transformer_layers': 2,  # Following Firbas et al.\n",
    "    'hidden_dim': 64,  # Output head hidden dimension\n",
    "    'num_classes': 5,  # ATTM, CTRW, FBM, LW, SBM\n",
    "\n",
    "    # ============= TRAINING HYPERPARAMETERS =============\n",
    "    'batch_size': 128 if IN_COLAB else 4,  # Larger batch on Colab (T4 GPU has more VRAM)\n",
    "    'gradient_accumulation_steps': 1 if IN_COLAB else 2,  # No accumulation needed with larger batch  # Accumulate over N steps (memory efficiency)\n",
    "    'learning_rate': 4e-4 if IN_COLAB else 1e-4,  # Scaled for larger batch (linear scaling rule)\n",
    "    'num_epochs': None,  # Set below based on TEST_MODE\n",
    "    'weight_decay': 0, # set to 1e-4 for L2 regularisation\n",
    "    'gradient_clip_norm': 1.0,\n",
    "    'patience': 20,  # Early stopping patience\n",
    "    'lr_scheduler_patience': 5,\n",
    "    'lr_scheduler_factor': 0.5,\n",
    "\n",
    "    # ============= LOSS WEIGHTS (Balanced Multi-Task) =============\n",
    "    'lambda_D': 2.0,  # D contribution: 20% (rebalanced after removing H)\n",
    "    'lambda_alpha': 2.0,  # Î± contribution: 20% (rebalanced after removing H)\n",
    "    'lambda_model': 6.0,  # model_type contribution: 60% (rebalanced after removing H)\n",
    "    'label_smoothing': 0.1, # Added label smoothing for classification loss\n",
    "\n",
    "    # ============= NORMALIZATION CONSTANTS (For Loss Calculation) =============\n",
    "    # Ground truth value ranges (from preprocessing)\n",
    "    'D_min': 1e-4,        # Minimum D value in dataset\n",
    "    'D_max': 100.0,       # Maximum D value in dataset\n",
    "    'alpha_min': 0.05,    # Minimum Î± value (ANDI discrete values)\n",
    "    'alpha_max': 2.0,     # Maximum Î± value (ANDI discrete values)\n",
    "    # ============= DATA SUBSET (TEST MODE) =============\n",
    "    'test_mode_train_size': 1000,\n",
    "    'test_mode_val_size': 500,\n",
    "    'test_mode_test_size': 1000,\n",
    "\n",
    "    # ============= CHECKPOINTING =============\n",
    "    'save_checkpoint_every': None,  # Set below based on TEST_MODE\n",
    "    'resume_from_checkpoint': None,  # Path to checkpoint to resume from (or None)\n",
    "\n",
    "    # ============= DEVICE =============\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "\n",
    "    # ============= RANDOM SEED =============\n",
    "    'random_seed': 42,\n",
    "\n",
    "    # ============= Number of Workers =============\n",
    "    'num_workers': 0,\n",
    "}\n",
    "\n",
    "# Set num_epochs and checkpoint frequency based on TEST_MODE\n",
    "CONFIG['num_epochs'] = 10 if CONFIG['TEST_MODE'] else 100\n",
    "CONFIG['save_checkpoint_every'] = 1 if IN_COLAB else (2 if CONFIG['TEST_MODE'] else 10)  # Every epoch on Colab or in test mode\n",
    "\n",
    "# Print configuration\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training Mode: {'ðŸ§ª TEST (Pipeline Validation)' if CONFIG['TEST_MODE'] else 'ðŸš€ PRODUCTION (Full Training)'}\")\n",
    "print()\n",
    "print(\"Model Architecture:\")\n",
    "print(f\"  d_model: {CONFIG['d_model']}\")\n",
    "print(f\"  nhead: {CONFIG['nhead']}\")\n",
    "print(f\"  Transformer layers: {CONFIG['num_transformer_layers']}\")\n",
    "print(f\"  Dropout: {CONFIG['dropout']}\")\n",
    "print()\n",
    "print(\"Training Hyperparameters:\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Num epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"  Gradient clip: {CONFIG['gradient_clip_norm']}\")\n",
    "print(f\"  Device: {CONFIG['device']}\")\n",
    "print()\n",
    "print(\"Loss Weights (Balanced Multi-Task):\")\n",
    "print(f\"  Î»_D: {CONFIG['lambda_D']} (D contribution: 20%)\")\n",
    "print(f\"  Î»_Î±: {CONFIG['lambda_alpha']} (Î± contribution: 20%)\")\n",
    "print(f\"  Î»_model: {CONFIG['lambda_model']} (model_type contribution: 60%)\")\n",
    "print(f\"  Label smoothing: {CONFIG['label_smoothing']}\")\n",
    "print()\n",
    "print(\"Normalization Constants:\")\n",
    "print(f\"  D range: [{CONFIG['D_min']}, {CONFIG['D_max']}]\")\n",
    "print(f\"  Î± range: [{CONFIG['alpha_min']}, {CONFIG['alpha_max']}]\")\n",
    "print()\n",
    "print(\"Checkpointing:\")\n",
    "print(f\"  Save every: {CONFIG['save_checkpoint_every']} epoch(s)\")\n",
    "print(f\"  Resume from: {CONFIG['resume_from_checkpoint'] if CONFIG['resume_from_checkpoint'] else 'None (fresh start)'}\")\n",
    "print()\n",
    "\n",
    "if CONFIG['TEST_MODE']:\n",
    "    print(\"âš  TEST MODE ENABLED:\")\n",
    "    print(f\"  Train samples: {CONFIG['test_mode_train_size']:,}\")\n",
    "    print(f\"  Val samples: {CONFIG['test_mode_val_size']:,}\")\n",
    "    print(f\"  Test samples: {CONFIG['test_mode_test_size']:,}\")\n",
    "    print(f\"  Epochs: {CONFIG['num_epochs']}\")\n",
    "    print(f\"  Checkpoints: Every epoch (epochs 1, 2, 3, 4, 5)\")\n",
    "    print(\"  Use this mode to verify the pipeline before full training.\")\n",
    "else:\n",
    "    print(\"ðŸš€ PRODUCTION MODE ENABLED:\")\n",
    "    print(\"  Using full dataset (2M trajectories)\")\n",
    "    print(f\"  Checkpoints: Every 10 epochs (10, 20, 30, ..., 100)\")\n",
    "    print(f\"  Estimated training time: ~158 hours (6.6 days) on GTX 1070 Ti\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8T8oWwQGyn2E"
   },
   "source": [
    "## Cell 3: Output Directory Setup\n",
    "\n",
    "Creates organized folder structure for:\n",
    "- Model checkpoints (best per head + final)\n",
    "- Training plots (separate folders per output head)\n",
    "- Training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1704,
     "status": "ok",
     "timestamp": 1763315805110,
     "user": {
      "displayName": "Enow George",
      "userId": "04500551558329927866"
     },
     "user_tz": 0
    },
    "id": "o9F_aMRmyn2E",
    "outputId": "772a2307-3f00-4828-c809-6604749c52b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CREATING OUTPUT DIRECTORIES\n",
      "======================================================================\n",
      "Directory structure created:\n",
      "/content/drive/MyDrive/ERP_Shrey/outputs3/\n",
      "â”œâ”€â”€ checkpoints/\n",
      "â”‚   â”œâ”€â”€ best_D_model.pth\n",
      "â”‚   â”œâ”€â”€ best_alpha_model.pth\n",
      "â”‚   â”œâ”€â”€ best_model_type_model.pth\n",
      "â”‚   â””â”€â”€ final_model.pth\n",
      "â”œâ”€â”€ plots/\n",
      "â”‚   â”œâ”€â”€ D_head/\n",
      "â”‚   â”‚   â””â”€â”€ epoch_XXX.png\n",
      "â”‚   â”œâ”€â”€ alpha_head/\n",
      "â”‚   â”‚   â””â”€â”€ epoch_XXX.png\n",
      "â”‚   â”œâ”€â”€ model_type_head/\n",
      "â”‚   â”‚   â””â”€â”€ epoch_XXX.png\n",
      "â””â”€â”€ logs/\n",
      "    â”œâ”€â”€ training_log.txt\n",
      "    â”œâ”€â”€ training_history.json\n",
      "â””â”€â”€ best_metrics.json\n",
      "âœ“ Training log initialized: /content/drive/MyDrive/ERP_Shrey/outputs3/logs/training_log.txt\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create output directories\n",
    "print(\"=\"*70)\n",
    "print(\"CREATING OUTPUT DIRECTORIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Main directories\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['plots_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['logs_dir'], exist_ok=True)\n",
    "\n",
    "# Create subdirectories for plots (one per output head)\n",
    "head_names = ['D_head', 'alpha_head', 'model_type_head']\n",
    "for head_name in head_names:\n",
    "    os.makedirs(os.path.join(CONFIG['plots_dir'], head_name), exist_ok=True)\n",
    "\n",
    "# Print directory tree\n",
    "print(\"Directory structure created:\")\n",
    "print(f\"{CONFIG['output_dir']}\")\n",
    "print(f\"â”œâ”€â”€ checkpoints/\")\n",
    "\n",
    "print(f\"â”‚   â”œâ”€â”€ best_D_model.pth\")\n",
    "print(f\"â”‚   â”œâ”€â”€ best_alpha_model.pth\")\n",
    "print(f\"â”‚   â”œâ”€â”€ best_model_type_model.pth\")\n",
    "print(f\"â”‚   â””â”€â”€ final_model.pth\")\n",
    "print(f\"â”œâ”€â”€ plots/\")\n",
    "for head_name in head_names:\n",
    "    print(f\"â”‚   â”œâ”€â”€ {head_name}/\")\n",
    "    print(f\"â”‚   â”‚   â””â”€â”€ epoch_XXX.png\")\n",
    "print(f\"â””â”€â”€ logs/\")\n",
    "print(f\"    â”œâ”€â”€ training_log.txt\")\n",
    "print(f\"    â”œâ”€â”€ training_history.json\")\n",
    "print(f\"â””â”€â”€ best_metrics.json\")\n",
    "\n",
    "# Initialize training log\n",
    "log_file = os.path.join(CONFIG['logs_dir'], 'training_log.txt')\n",
    "with open(log_file, 'w') as f:\n",
    "    f.write(f\"Training Log - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Mode: {'TEST' if CONFIG['TEST_MODE'] else 'PRODUCTION'}\\n\")\n",
    "    f.write(f\"Device: {CONFIG['device']}\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "\n",
    "print(f\"âœ“ Training log initialized: {log_file}\")\n",
    "print(\"=\"*70)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBpXIYP3yn2E"
   },
   "source": [
    "## Cell 4: Dataset Loading & Validation\n",
    "\n",
    "Loads the HDF5 dataset from preprocessing notebook and creates PyTorch DataLoaders.\n",
    "\n",
    "**TEST_MODE behavior:**\n",
    "- Loads only a small subset of data for fast pipeline testing\n",
    "- Maintains train/val/test split ratios\n",
    "- Prints subset sizes\n",
    "\n",
    "**PRODUCTION behavior:**\n",
    "- Loads full 2M trajectory dataset\n",
    "- Uses efficient batching and multi-worker loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69565,
     "status": "ok",
     "timestamp": 1763315874677,
     "user": {
      "displayName": "Enow George",
      "userId": "04500551558329927866"
     },
     "user_tz": 0
    },
    "id": "WssEDB1Tyn2F",
    "outputId": "945d4665-caaf-469e-99b3-1af85f8bc0ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING ENTIRE DATASET TO RAM\n",
      "======================================================================\n",
      "\n",
      "Reading from: /content/drive/MyDrive/ERP_Shrey/data/andi/andi_dataset_table2_true__20251115_201643.h5\n",
      "\n",
      "Loading training split...\n",
      "  âœ“ Train: 1,349,949 samples\n",
      "Loading validation split...\n",
      "  âœ“ Val: 149,995 samples\n",
      "Loading test split...\n",
      "  âœ“ SNR loaded for test set\n",
      "  âœ“ Test: 498,576 samples\n",
      "\n",
      "Total RAM usage: 14.92 GB\n",
      "\n",
      "======================================================================\n",
      "âœ“ DATASET LOADED TO RAM\n",
      "======================================================================\n",
      "\n",
      "Creating RAM-based datasets...\n",
      "Full dataset sizes:\n",
      "  Train: 1,349,949\n",
      "  Val:   149,995\n",
      "  Test:  498,576\n",
      "\n",
      "\n",
      "Creating DataLoaders...\n",
      "âœ“ DataLoaders created\n",
      "  Batch size: 128\n",
      "  Train batches: 10,547\n",
      "  Val batches: 1,172\n",
      "  Test batches: 3,896\n",
      "\n",
      "Validating dataset...\n",
      "Sample batch shapes:\n",
      "  positions                : (128, 1000, 1)\n",
      "  displacements_raw        : (128, 999, 1)\n",
      "  displacements_scaled     : (128, 999, 1)\n",
      "  mask_positions           : (128, 1000)\n",
      "  mask_displacements       : (128, 999)\n",
      "  D_true                   : (128,)\n",
      "  alpha_true               : (128,)\n",
      "  model_type               : (128,)\n",
      "  length                   : (128,)\n",
      "  snr                      : (128,)\n",
      "\n",
      "âœ“ No NaN or Inf values found in sample batch\n",
      "\n",
      "Ground truth statistics (sample batch):\n",
      "  D:     min=1.0000e-10, max=1.0334e+02, mean=5.6975e+00\n",
      "  Alpha: min=0.1000, max=1.9000, mean=0.9180\n",
      "  Model: [0, 1, 2, 3, 4]\n",
      "\n",
      "âœ“ All ground truth values are in valid ranges\n",
      "======================================================================\n",
      "\n",
      "Testing forward pass with sample batch...\n",
      "(Skipping forward pass test - model not created yet)\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD ENTIRE DATASET TO RAM\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING ENTIRE DATASET TO RAM\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Check if dataset file exists\n",
    "if not os.path.exists(CONFIG['dataset_path']):\n",
    "    raise FileNotFoundError(f\"Dataset not found: {CONFIG['dataset_path']}\")\n",
    "\n",
    "print(f\"Reading from: {CONFIG['dataset_path']}\")\n",
    "print()\n",
    "\n",
    "# Load all data into RAM\n",
    "print(\"Loading training split...\")\n",
    "with h5py.File(CONFIG['dataset_path'], 'r') as f:\n",
    "    train_data = {\n",
    "        'positions': f['train']['positions'][:],\n",
    "        'displacements_raw': f['train']['displacements_raw'][:],\n",
    "        'displacements_scaled': f['train']['displacements_scaled'][:],\n",
    "        'mask_positions': f['train']['mask_positions'][:],\n",
    "        'mask_displacements': f['train']['mask_displacements'][:],\n",
    "        'D': f['train']['D'][:],\n",
    "        'alpha': f['train']['alpha'][:],\n",
    "        'model_id': f['train']['model_id'][:],\n",
    "        'length': f['train']['length'][:]\n",
    "    }\n",
    "    # Add SNR if available (may not be in train/val splits)\n",
    "    if 'snr' in f['train']:\n",
    "        train_data['snr'] = f['train']['snr'][:]\n",
    "    else:\n",
    "        train_data['snr'] = None\n",
    "print(f\"  âœ“ Train: {len(train_data['D']):,} samples\")\n",
    "\n",
    "print(\"Loading validation split...\")\n",
    "with h5py.File(CONFIG['dataset_path'], 'r') as f:\n",
    "    val_data = {\n",
    "        'positions': f['val']['positions'][:],\n",
    "        'displacements_raw': f['val']['displacements_raw'][:],\n",
    "        'displacements_scaled': f['val']['displacements_scaled'][:],\n",
    "        'mask_positions': f['val']['mask_positions'][:],\n",
    "        'mask_displacements': f['val']['mask_displacements'][:],\n",
    "        'D': f['val']['D'][:],\n",
    "        'alpha': f['val']['alpha'][:],\n",
    "        'model_id': f['val']['model_id'][:],\n",
    "        'length': f['val']['length'][:]\n",
    "    }\n",
    "    # Add SNR if available (may not be in train/val splits)\n",
    "    if 'snr' in f['val']:\n",
    "        val_data['snr'] = f['val']['snr'][:]\n",
    "    else:\n",
    "        val_data['snr'] = None\n",
    "print(f\"  âœ“ Val: {len(val_data['D']):,} samples\")\n",
    "\n",
    "print(\"Loading test split...\")\n",
    "with h5py.File(CONFIG['dataset_path'], 'r') as f:\n",
    "    test_data = {\n",
    "        'positions': f['test']['positions'][:],\n",
    "        'displacements_raw': f['test']['displacements_raw'][:],\n",
    "        'displacements_scaled': f['test']['displacements_scaled'][:],\n",
    "        'mask_positions': f['test']['mask_positions'][:],\n",
    "        'mask_displacements': f['test']['mask_displacements'][:],\n",
    "        'D': f['test']['D'][:],\n",
    "        'alpha': f['test']['alpha'][:],\n",
    "        'model_id': f['test']['model_id'][:],\n",
    "        'length': f['test']['length'][:]\n",
    "    }\n",
    "    # Add SNR (test split should have SNR)\n",
    "    if 'snr' in f['test']:\n",
    "        test_data['snr'] = f['test']['snr'][:]\n",
    "        print(f\"  âœ“ SNR loaded for test set\")\n",
    "    else:\n",
    "        test_data['snr'] = None\n",
    "        print(f\"  âš  Warning: SNR not found in test set\")\n",
    "print(f\"  âœ“ Test: {len(test_data['D']):,} samples\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Calculate memory usage (skip None values like SNR for train/val)\n",
    "total_bytes = sum(v.nbytes for v in train_data.values() if v is not None)\n",
    "total_bytes += sum(v.nbytes for v in val_data.values() if v is not None)\n",
    "total_bytes += sum(v.nbytes for v in test_data.values() if v is not None)\n",
    "total_gb = total_bytes / (1024**3)\n",
    "\n",
    "print(f\"Total RAM usage: {total_gb:.2f} GB\")\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"âœ“ DATASET LOADED TO RAM\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# RAM-BASED DATASET CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class AnDiDatasetRAM(Dataset):\n",
    "    \"\"\"PyTorch Dataset that reads from RAM instead of HDF5\"\"\"\n",
    "\n",
    "    def __init__(self, data_dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dict: Dictionary with numpy arrays for all data fields\n",
    "        \"\"\"\n",
    "        self.data = data_dict\n",
    "        self.n_samples = len(data_dict['D'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'positions': torch.from_numpy(self.data['positions'][idx]).float(),\n",
    "            'displacements_raw': torch.from_numpy(self.data['displacements_raw'][idx]).float(),\n",
    "            'displacements_scaled': torch.from_numpy(self.data['displacements_scaled'][idx]).float(),\n",
    "            'mask_positions': torch.from_numpy(self.data['mask_positions'][idx]).bool(),\n",
    "            'mask_displacements': torch.from_numpy(self.data['mask_displacements'][idx]).bool(),\n",
    "            'D_true': torch.tensor(self.data['D'][idx], dtype=torch.float32),\n",
    "            'alpha_true': torch.tensor(self.data['alpha'][idx], dtype=torch.float32),\n",
    "            'model_type': torch.tensor(self.data['model_id'][idx], dtype=torch.long),\n",
    "            'length': torch.tensor(self.data['length'][idx], dtype=torch.long),\n",
    "        }\n",
    "        # Add SNR if available (metadata only, not used in model forward pass)\n",
    "        if self.data.get('snr') is not None:\n",
    "            item['snr'] = torch.tensor(self.data['snr'][idx], dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE DATASETS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Creating RAM-based datasets...\")\n",
    "\n",
    "train_dataset_full = AnDiDatasetRAM(train_data)\n",
    "val_dataset_full = AnDiDatasetRAM(val_data)\n",
    "test_dataset_full = AnDiDatasetRAM(test_data)\n",
    "\n",
    "print(f\"Full dataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset_full):,}\")\n",
    "print(f\"  Val:   {len(val_dataset_full):,}\")\n",
    "print(f\"  Test:  {len(test_dataset_full):,}\")\n",
    "print()\n",
    "\n",
    "# Apply TEST_MODE subsetting if enabled\n",
    "if CONFIG['TEST_MODE']:\n",
    "    print(\"âš  TEST MODE: Creating subsets...\")\n",
    "\n",
    "    train_indices = np.random.choice(len(train_dataset_full),\n",
    "                                     size=CONFIG['test_mode_train_size'],\n",
    "                                     replace=False)\n",
    "    val_indices = np.random.choice(len(val_dataset_full),\n",
    "                                   size=CONFIG['test_mode_val_size'],\n",
    "                                   replace=False)\n",
    "    test_indices = np.random.choice(len(test_dataset_full),\n",
    "                                    size=CONFIG['test_mode_test_size'],\n",
    "                                    replace=False)\n",
    "\n",
    "    train_dataset = Subset(train_dataset_full, train_indices)\n",
    "    val_dataset = Subset(val_dataset_full, val_indices)\n",
    "    test_dataset = Subset(test_dataset_full, test_indices)\n",
    "\n",
    "    print(f\"Subset sizes:\")\n",
    "    print(f\"  Train: {len(train_dataset):,}\")\n",
    "    print(f\"  Val:   {len(val_dataset):,}\")\n",
    "    print(f\"  Test:  {len(test_dataset):,}\")\n",
    "else:\n",
    "    train_dataset = train_dataset_full\n",
    "    val_dataset = val_dataset_full\n",
    "    test_dataset = test_dataset_full\n",
    "\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE DATALOADERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Creating DataLoaders...\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Can use workers now!\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "print(f\"âœ“ DataLoaders created\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Train batches: {len(train_loader):,}\")\n",
    "print(f\"  Val batches: {len(val_loader):,}\")\n",
    "print(f\"  Test batches: {len(test_loader):,}\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# VALIDATE DATASET\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Validating dataset...\")\n",
    "\n",
    "# Load a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"Sample batch shapes:\")\n",
    "for key, value in sample_batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key:25s}: {tuple(value.shape)}\")\n",
    "print()\n",
    "\n",
    "# Check for NaN/Inf\n",
    "has_nan = False\n",
    "has_inf = False\n",
    "for key, value in sample_batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        if torch.isnan(value).any():\n",
    "            print(f\"  âŒ ERROR: {key} contains NaN values!\")\n",
    "            has_nan = True\n",
    "        if torch.isinf(value).any():\n",
    "            print(f\"  âŒ ERROR: {key} contains Inf values!\")\n",
    "            has_inf = True\n",
    "\n",
    "if not has_nan and not has_inf:\n",
    "    print(\"âœ“ No NaN or Inf values found in sample batch\")\n",
    "print()\n",
    "\n",
    "# Check ground truth ranges\n",
    "print(\"Ground truth statistics (sample batch):\")\n",
    "print(f\"  D:     min={sample_batch['D_true'].min():.4e}, max={sample_batch['D_true'].max():.4e}, mean={sample_batch['D_true'].mean():.4e}\")\n",
    "print(f\"  Alpha: min={sample_batch['alpha_true'].min():.4f}, max={sample_batch['alpha_true'].max():.4f}, mean={sample_batch['alpha_true'].mean():.4f}\")\n",
    "print(f\"  Model: {sample_batch['model_type'].unique().tolist()}\")\n",
    "print()\n",
    "\n",
    "# Verify ranges\n",
    "assert (sample_batch['D_true'] > 0).all(), \"D must be positive\"\n",
    "assert (sample_batch['alpha_true'] >= 0).all() and (sample_batch['alpha_true'] <= 2).all(), \"Alpha out of range [0,2]\"\n",
    "assert (sample_batch['model_type'] >= 0).all() and (sample_batch['model_type'] <= 4).all(), \"Model type out of range [0,4]\"\n",
    "\n",
    "print(\"âœ“ All ground truth values are in valid ranges\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# TEST FORWARD PASS (if model exists in namespace)\n",
    "# =============================================================================\n",
    "\n",
    "try:\n",
    "    # Test forward pass\n",
    "    print(\"Testing forward pass with sample batch...\")\n",
    "    sample_batch_gpu = {k: v.to(CONFIG['device']) for k, v in sample_batch.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        D_pred, alpha_pred, model_logits, attn_weights = model(sample_batch_gpu)\n",
    "\n",
    "    print(\"Forward pass successful!\")\n",
    "    print(f\"  D_pred shape: {D_pred.shape}\")\n",
    "    print(f\"  alpha_pred shape: {alpha_pred.shape}\")\n",
    "    print(f\"  model_logits shape: {model_logits.shape}\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "except NameError:\n",
    "    # Model doesn't exist yet - skip this test\n",
    "    print(\"(Skipping forward pass test - model not created yet)\")\n",
    "    print(\"=\"*70)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubQ7b_VGyn2F"
   },
   "source": [
    "## Cell 5: Model Architecture Implementation\n",
    "\n",
    "Implements the complete Dual CNN Cross-Attention Transformer following `architecture_diagram.md`:\n",
    "\n",
    "**Components:**\n",
    "1. **Alpha-CNN Branch**: Conv1D layers (k=7,5,3) for temporal correlations\n",
    "2. **D-CNN Branch**: Conv1D layers (k=3,3,3) for amplitude patterns\n",
    "3. **Transformer Encoders**: 2 stacked blocks per branch (8 heads, following Firbas et al.)\n",
    "4. **Cross-Attention**: Alpha queries D to model Dâ†’Alpha dependency (Alpha enriched with D's magnitude info)\n",
    "5. **Output Heads**: D (ReLU, uses D_enc), Î± (SigmoidÃ—2, uses alpha_cross), Model Type (Softmax, uses alpha_cross)\n",
    "\n",
    "**Model Size:** ~1.0M parameters (~4MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 182,
     "status": "ok",
     "timestamp": 1763315874862,
     "user": {
      "displayName": "Enow George",
      "userId": "04500551558329927866"
     },
     "user_tz": 0
    },
    "id": "0481-NvFyn2F",
    "outputId": "0ebb0037-f959-4c04-9928-ddf9151d37ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CREATING MODEL\n",
      "======================================================================\n",
      "Model: DualCNNCrossAttentionTransformer\n",
      "Total parameters: 470,215 (0.47M)\n",
      "Trainable parameters: 470,215\n",
      "Model size: 1.79 MB (float32)\n",
      "Device: cuda\n",
      "\n",
      "Testing forward pass with sample batch...\n",
      "Output shapes:\n",
      "  D_pred: torch.Size([128]) (expected: [128])\n",
      "  alpha_pred: torch.Size([128]) (expected: [128])\n",
      "  model_logits: torch.Size([128, 5]) (expected: [128, 5])\n",
      "  attn_weights: torch.Size([128, 8, 499, 499])\n",
      "\n",
      "âœ“ Model created and forward pass successful!\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODEL ARCHITECTURE COMPONENTS\n",
    "# =============================================================================\n",
    "\n",
    "class Alpha_CNN(nn.Module):\n",
    "    \"\"\"Alpha-CNN Branch: Temporal correlation specialist with larger kernels\"\"\"\n",
    "\n",
    "    def __init__(self, d_model=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Layer 1: Broad temporal patterns\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=7, stride=1, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        # Layer 2: Intermediate temporal correlations\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        # Layer 3: Fine temporal patterns\n",
    "        self.conv3 = nn.Conv1d(64, d_model, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(d_model)\n",
    "        # Pooling layer (following ANDI/Firbas): reduces sequence length by 2x\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, L, 1] â†’ [B, 1, L] for Conv1d\n",
    "        x = x.transpose(1, 2)\n",
    "        # Layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        # Layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        # Layer 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        # Apply pooling: [B, d_model, L] â†’ [B, d_model, L/2]\n",
    "        x = self.pool(x)\n",
    "        # [B, d_model, L] â†’ [B, L, d_model]\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "\n",
    "class D_CNN(nn.Module):\n",
    "    \"\"\"D-CNN Branch: Amplitude pattern specialist with smaller kernels\"\"\"\n",
    "\n",
    "    def __init__(self, d_model=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Layer 1: Local displacement patterns\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        # Layer 2: Amplitude variations\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        # Layer 3: Fine amplitude patterns\n",
    "        self.conv3 = nn.Conv1d(64, d_model, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(d_model)\n",
    "        # Pooling layer (following ANDI/Firbas): reduces sequence length by 2x\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, L, 1] â†’ [B, 1, L] for Conv1d\n",
    "        x = x.transpose(1, 2)\n",
    "        # Layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        # Layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        # Layer 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        # Apply pooling: [B, d_model, L] â†’ [B, d_model, L/2]\n",
    "        x = self.pool(x)\n",
    "        # [B, d_model, L] â†’ [B, L, d_model]\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"Single Transformer Encoder Block (following Firbas et al.)\"\"\"\n",
    "\n",
    "    def __init__(self, d_model=128, nhead=16, dim_feedforward=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Multi-head self-attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-head self-attention with residual\n",
    "        attn_output, _ = self.self_attn(x, x, x, key_padding_mask=mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        # Feed-forward with residual\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Stacked Transformer Encoder (Ã—2 blocks following Firbas)\"\"\"\n",
    "\n",
    "    def __init__(self, d_model=128, nhead=16, dim_feedforward=512, dropout=0.1, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, nhead, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    \"\"\"Cross-Attention Layer: Alpha queries D (KEY INNOVATION)\"\"\"\n",
    "\n",
    "    def __init__(self, d_model=128, nhead=16, dim_feedforward=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Multi-head cross-attention (Alpha queries D)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, D_enc, alpha_enc, D_mask=None, alpha_mask=None):\n",
    "        # Cross-attention: Alpha queries D\n",
    "        cross_output, attn_weights = self.cross_attn(\n",
    "            query=alpha_enc,\n",
    "            key=D_enc,\n",
    "            value=D_enc,\n",
    "            key_padding_mask=D_mask,\n",
    "            average_attn_weights=False  # Keep all heads for interpretability\n",
    "        )\n",
    "        # Residual connection + LayerNorm\n",
    "        alpha_cross = self.norm1(alpha_enc + self.dropout(cross_output))\n",
    "        # Feed-forward with residual\n",
    "        ffn_output = self.ffn(alpha_cross)\n",
    "        alpha_cross = self.norm2(alpha_cross + ffn_output)\n",
    "        return alpha_cross, attn_weights\n",
    "\n",
    "\n",
    "class D_Head(nn.Module):\n",
    "    \"\"\"D estimation head (outputs positive values)\"\"\"\n",
    "\n",
    "    def __init__(self, d_model=128, hidden_dim=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout), # Updated dropout for D_Head\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.ReLU()  # D > 0\n",
    "        )\n",
    "\n",
    "    def forward(self, D_enc):\n",
    "        # Max pooling over sequence dimension\n",
    "        D_pooled = torch.max(D_enc, dim=1)[0]  # [B, d_model]\n",
    "        D_pred = self.head(D_pooled).squeeze(-1)  # [B], squeeze to remove last dimension\n",
    "        # Clamp D predictions to be within the expected range [1e-4, 100.0]\n",
    "        # This prevents extreme values and matches the data distribution bounds.\n",
    "        D_pred = torch.clamp(D_pred, min=1e-4, max=100.0)\n",
    "        return D_pred\n",
    "\n",
    "\n",
    "class Alpha_Head(nn.Module):\n",
    "    \"\"\"Alpha estimation head (outputs in [0, 2])\"\"\"\n",
    "\n",
    "    def __init__(self, d_model=128, hidden_dim=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout), # Updated dropout for Alpha_Head\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, alpha_cross):\n",
    "        # Max pooling over sequence dimension\n",
    "        alpha_pooled = torch.max(alpha_cross, dim=1)[0]  # [B, d_model]\n",
    "        alpha_pred = 2.0 * self.head(alpha_pooled).squeeze(-1)  # [B], scaled to [0, 2]\n",
    "        # Clamp alpha predictions to be within the expected range [0.05, 2.0]\n",
    "        # This prevents extreme values and matches the data distribution bounds.\n",
    "        alpha_pred = torch.clamp(alpha_pred, min=0.05, max=2.0)\n",
    "        return alpha_pred\n",
    "\n",
    "\n",
    "class ModelType_Head(nn.Module):\n",
    "    \"\"\"Model classification head (5 classes)\"\"\"\n",
    "\n",
    "    def __init__(self, d_model=128, hidden_dim=64, num_classes=5, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout), # Updated dropout for ModelType_Head\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, alpha_cross):\n",
    "        # Max pooling over sequence dimension\n",
    "        model_pooled = torch.max(alpha_cross, dim=1)[0]  # [B, d_model]\n",
    "        logits = self.head(model_pooled)  # [B, num_classes]\n",
    "        return logits  # Return logits for CrossEntropyLoss\n",
    "\n",
    "\n",
    "class DualCNNCrossAttentionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Main Model: Dual CNN Cross-Attention Transformer\n",
    "    Architecture:\n",
    "        Input â†’ Dual CNN Branches â†’ Transformer Encoders â†’ Cross-Attention â†’ Multi-Task Outputs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model=128, nhead=16, dim_feedforward=512, dropout=0.1,\n",
    "                 num_transformer_layers=2, hidden_dim=64, num_classes=5):\n",
    "        super().__init__()\n",
    "        # Dual CNN branches\n",
    "        self.alpha_cnn = Alpha_CNN(d_model, dropout)\n",
    "        self.d_cnn = D_CNN(d_model, dropout)\n",
    "        # Transformer encoders (Ã—2 stacked blocks following Firbas)\n",
    "        self.alpha_transformer = TransformerEncoder(d_model, nhead, dim_feedforward, dropout, num_transformer_layers)\n",
    "        self.d_transformer = TransformerEncoder(d_model, nhead, dim_feedforward, dropout, num_transformer_layers)\n",
    "        # Cross-attention layer\n",
    "        self.cross_attention = CrossAttentionBlock(d_model, nhead, dim_feedforward, dropout)\n",
    "        # Output heads\n",
    "        self.d_head = D_Head(d_model, hidden_dim, dropout)\n",
    "        self.alpha_head = Alpha_Head(d_model, hidden_dim, dropout)\n",
    "        self.model_head = ModelType_Head(d_model, hidden_dim, num_classes, dropout)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: Dictionary containing:\n",
    "                - displacements_scaled: [B, L, 1] (Alpha-CNN input)\n",
    "                - displacements_raw: [B, L, 1] (D-CNN input)\n",
    "                - mask_displacements: [B, L] (attention mask)\n",
    "        Returns:\n",
    "            D_pred, alpha_pred, model_pred_logits, attn_weights\n",
    "        \"\"\"\n",
    "        # Get inputs\n",
    "        X_alpha = batch['displacements_scaled']  # [B, L, 1]\n",
    "        X_D = batch['displacements_raw']     # [B, L, 1]\n",
    "        mask = batch['mask_displacements']   # [B, L]\n",
    "\n",
    "        # Convert mask: True (valid) â†’ False, False (padding) â†’ True for PyTorch attention\n",
    "        attn_mask = ~mask  # Invert: padding positions should be True\n",
    "\n",
    "        # Dual CNN feature extraction\n",
    "        # Note: CNN includes MaxPool1d(kernel_size=2), so output is [B, L//2, d_model]\n",
    "        F_alpha = self.alpha_cnn(X_alpha)  # [B, L//2, d_model]\n",
    "        F_D = self.d_cnn(X_D)  # [B, L//2, d_model]\n",
    "\n",
    "        # Pool mask to match pooled sequence length: [B, L] â†’ [B, L/2]\n",
    "        # Logic: if ANY position in window [i, i+1] is padding, pooled position is padding\n",
    "        # attn_mask: True=padding (mask out), False=valid (keep)\n",
    "        # We want: pooled[i] = True if ANY of [i*2, i*2+1] was padding\n",
    "        # This is max pooling on the padding mask (True=1.0, False=0.0)\n",
    "        attn_mask_pooled = torch.nn.functional.max_pool1d(\n",
    "            attn_mask.unsqueeze(1).float(),  # [B, 1, L] â†’ [B, 1, L], Trueâ†’1.0, Falseâ†’0.0\n",
    "            kernel_size=2\n",
    "        ).squeeze(1).bool()  # [B, 1, L/2] â†’ [B, L/2], True if ANY was padding\n",
    "        # Result: True=padding (mask out), False=valid (keep) - correct for PyTorch attention\n",
    "\n",
    "        # Transformer encoders\n",
    "        alpha_enc = self.alpha_transformer(F_alpha, mask=attn_mask_pooled)  # [B, L/2, d_model]\n",
    "        D_enc = self.d_transformer(F_D, mask=attn_mask_pooled)  # [B, L/2, d_model]\n",
    "\n",
    "        # Cross-attention (Alpha queries D - Alpha enriched with D magnitude info)\n",
    "        alpha_cross, attn_weights = self.cross_attention(D_enc, alpha_enc, D_mask=attn_mask_pooled, alpha_mask=attn_mask_pooled)\n",
    "\n",
    "        # Output heads\n",
    "        # Note: Alpha and model heads use alpha_cross (enriched with D magnitude info)\n",
    "        # D head uses D_enc directly (D estimation is independent of alpha)\n",
    "        D_pred = self.d_head(D_enc)                      # D uses its own features (magnitude/variance specialist)\n",
    "        alpha_pred = self.alpha_head(alpha_cross)        # Alpha uses enriched features (with D magnitude context)\n",
    "        model_pred_logits = self.model_head(alpha_cross) # Model classification uses enriched alpha features\n",
    "\n",
    "        return D_pred, alpha_pred, model_pred_logits, attn_weights\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# INSTANTIATE MODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREATING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = DualCNNCrossAttentionTransformer(\n",
    "    d_model=CONFIG['d_model'],\n",
    "    nhead=CONFIG['nhead'],\n",
    "    dim_feedforward=CONFIG['dim_feedforward'],\n",
    "    dropout=CONFIG['dropout'],\n",
    "    num_transformer_layers=CONFIG['num_transformer_layers'],\n",
    "    hidden_dim=CONFIG['hidden_dim'],\n",
    "    num_classes=CONFIG['num_classes']\n",
    ").to(CONFIG['device'])\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model: DualCNNCrossAttentionTransformer\")\n",
    "print(f\"Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1024**2:.2f} MB (float32)\")\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print()\n",
    "\n",
    "# Test forward pass\n",
    "print(\"Testing forward pass with sample batch...\")\n",
    "sample_batch_gpu = {k: v.to(CONFIG['device']) for k, v in sample_batch.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    D_pred, alpha_pred, model_logits, attn_weights = model(sample_batch_gpu)\n",
    "\n",
    "print(\"Output shapes:\")\n",
    "print(f\"  D_pred: {D_pred.shape} (expected: [{CONFIG['batch_size']}])\")\n",
    "print(f\"  alpha_pred: {alpha_pred.shape} (expected: [{CONFIG['batch_size']}])\")\n",
    "print(f\"  model_logits: {model_logits.shape} (expected: [{CONFIG['batch_size']}, 5])\")\n",
    "print(f\"  attn_weights: {attn_weights.shape}\")\n",
    "print()\n",
    "print(\"âœ“ Model created and forward pass successful!\")\n",
    "print(\"=\"*70)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLQQ4ln1yn2G"
   },
   "source": [
    "## Cell 6: Loss Functions & Evaluation Metrics\n",
    "\n",
    "**Multi-Task Loss (Balanced with Normalization):**\n",
    "- D and Î± normalized to [0,1] for loss calculation only\n",
    "- L_total = 2.0Ã—MSE(D_norm) + 2.0Ã—MSE(Î±_norm) + 6.0Ã—CrossEntropy(model_type)\n",
    "- **Contributions:** D=20%, Î±=20%, model_type=60%\n",
    "\n",
    "**Normalization (Loss Calculation Only):**\n",
    "- D: [1e-4, 100] â†’ [0, 1] (prevents D from dominating gradients)\n",
    "- Î±: [0.05, 2.0] â†’ [0, 1] (consistent scale with D)\n",
    "\n",
    "**Evaluation Metrics (Original Scale):**\n",
    "- **Regression heads (D, Î±):** MSE (loss, normalized), MAE (evaluation, original scale)\n",
    "- **Classification head (model_type):** CrossEntropy (loss), Accuracy, F1-score (weighted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1763315874921,
     "user": {
      "displayName": "Enow George",
      "userId": "04500551558329927866"
     },
     "user_tz": 0
    },
    "id": "zfjUGVj3yn2G",
    "outputId": "9a42d1a2-517b-441e-cb89-bb8bf5ee1926"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOSS FUNCTIONS & METRICS DEFINED\n",
      "======================================================================\n",
      "Loss weights:\n",
      "  D:     2.0\n",
      "  Alpha: 2.0\n",
      "  Model: 6.0\n",
      "  Label smoothing: 0.1\n",
      "\n",
      "Metrics tracked:\n",
      "  Regression (D, Î±): MSE (loss), MAE (evaluation)\n",
      "  Classification (model): CrossEntropy (loss), Accuracy (evaluation)\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOSS FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def compute_loss(D_pred, alpha_pred, model_logits,\n",
    "                 D_true, alpha_true, model_true, config):\n",
    "    \"\"\"\n",
    "    Multi-task loss function with on-the-fly normalization for balanced gradients\n",
    "\n",
    "    Normalizes D and Î± to [0,1] range for loss calculation only.\n",
    "    Evaluation metrics (MAE) remain in original scale for interpretability.\n",
    "\n",
    "    Args:\n",
    "        D_pred: Predicted diffusion coefficient values\n",
    "        alpha_pred: Predicted anomalous exponent values\n",
    "        model_logits: Predicted model classification logits\n",
    "        D_true: Ground truth diffusion coefficient values\n",
    "        alpha_true: Ground truth anomalous exponent values\n",
    "        model_true: Ground truth model class labels\n",
    "        config: Configuration dictionary with loss weights\n",
    "\n",
    "    Returns:\n",
    "        L_total: Total weighted loss for backpropagation\n",
    "        metrics_dict: Dictionary of individual losses (L_D, L_alpha, L_model, L_total) and evaluation metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # =========================================================================\n",
    "    # NORMALIZE D AND ALPHA FOR LOSS CALCULATION (Balanced Gradients)\n",
    "    # =========================================================================\n",
    "    # D: [1e-4, 100] â†’ [0, 1]\n",
    "    D_min = config['D_min']\n",
    "    D_max = config['D_max']\n",
    "    D_pred_norm = (D_pred - D_min) / (D_max - D_min)\n",
    "    D_true_norm = (D_true - D_min) / (D_max - D_min)\n",
    "\n",
    "    # Î±: [0.05, 2.0] â†’ [0, 1]\n",
    "    alpha_min = config['alpha_min']\n",
    "    alpha_max = config['alpha_max']\n",
    "    alpha_pred_norm = (alpha_pred - alpha_min) / (alpha_max - alpha_min)\n",
    "    alpha_true_norm = (alpha_true - alpha_min) / (alpha_max - alpha_min)\n",
    "\n",
    "    # =========================================================================\n",
    "    # COMPUTE LOSSES (MSE on normalized values, CrossEntropy unchanged)\n",
    "    # =========================================================================\n",
    "    L_D = F.mse_loss(D_pred_norm, D_true_norm)          # D normalized to [0,1]\n",
    "    L_alpha = F.mse_loss(alpha_pred_norm, alpha_true_norm)  # Î± normalized to [0,1]\n",
    "    L_model = F.cross_entropy(model_logits, model_true, label_smoothing=config['label_smoothing'])     # CrossEntropy with label smoothing\n",
    "\n",
    "    # Weighted combination (balanced contributions)\n",
    "    L_total = (config['lambda_D'] * L_D +\n",
    "               config['lambda_alpha'] * L_alpha +\n",
    "               config['lambda_model'] * L_model)\n",
    "\n",
    "    # Compute evaluation metrics (no gradients)\n",
    "    with torch.no_grad():\n",
    "        # MAE for regression\n",
    "        MAE_D = torch.mean(torch.abs(D_pred - D_true))\n",
    "        MAE_alpha = torch.mean(torch.abs(alpha_pred - alpha_true))\n",
    "\n",
    "        # Accuracy for classification\n",
    "        model_pred_classes = torch.argmax(model_logits, dim=1)\n",
    "        accuracy_model = (model_pred_classes == model_true).float().mean()\n",
    "\n",
    "    metrics_dict = {\n",
    "        'L_D': L_D.item(),\n",
    "        'L_alpha': L_alpha.item(),\n",
    "        'L_model': L_model.item(),\n",
    "        'L_total': L_total.item(),\n",
    "        'MAE_D': MAE_D.item(),\n",
    "        'MAE_alpha': MAE_alpha.item(),\n",
    "        'accuracy_model': accuracy_model.item(),\n",
    "    }\n",
    "\n",
    "    return L_total, metrics_dict\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# METRIC TRACKER CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class MetricTracker:\n",
    "    \"\"\"Tracks best metrics for each output head\"\"\"\n",
    "    def __init__(self):\n",
    "        self.best_metrics = {\n",
    "            'D': {'mae': float('inf'), 'epoch': 0},\n",
    "            'alpha': {'mae': float('inf'), 'epoch': 0},\n",
    "            'model': {'accuracy': 0.0, 'epoch': 0}\n",
    "        }\n",
    "        self.history = {\n",
    "            'train': [],\n",
    "            'val': []\n",
    "        }\n",
    "\n",
    "    def update(self, epoch, metrics_dict, mode='val'):\n",
    "        \"\"\"Update history and check for new best models\"\"\"\n",
    "        self.history[mode].append(metrics_dict)\n",
    "\n",
    "        improved = {}\n",
    "        if mode == 'val':\n",
    "            # Check for improvements (lower MAE or higher accuracy)\n",
    "            if metrics_dict['MAE_D'] < self.best_metrics['D']['mae']:\n",
    "                self.best_metrics['D']['mae'] = metrics_dict['MAE_D']\n",
    "                self.best_metrics['D']['epoch'] = epoch\n",
    "                improved['D'] = True\n",
    "\n",
    "            if metrics_dict['MAE_alpha'] < self.best_metrics['alpha']['mae']:\n",
    "                self.best_metrics['alpha']['mae'] = metrics_dict['MAE_alpha']\n",
    "                self.best_metrics['alpha']['epoch'] = epoch\n",
    "                improved['alpha'] = True\n",
    "\n",
    "            if metrics_dict['accuracy_model'] > self.best_metrics['model']['accuracy']:\n",
    "                self.best_metrics['model']['accuracy'] = metrics_dict['accuracy_model']\n",
    "                self.best_metrics['model']['epoch'] = epoch\n",
    "                improved['model'] = True\n",
    "\n",
    "        return improved\n",
    "\n",
    "    def get_best_summary(self):\n",
    "        \"\"\"Get summary of best metrics\"\"\"\n",
    "        return {\n",
    "            'D_MAE': self.best_metrics['D']['mae'],\n",
    "            'D_epoch': self.best_metrics['D']['epoch'],\n",
    "            'alpha_MAE': self.best_metrics['alpha']['mae'],\n",
    "            'alpha_epoch': self.best_metrics['alpha']['epoch'],\n",
    "            'model_accuracy': self.best_metrics['model']['accuracy'],\n",
    "            'model_epoch': self.best_metrics['model']['epoch'],\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOSS FUNCTIONS & METRICS DEFINED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Loss weights:\")\n",
    "\n",
    "print(f\"  D:     {CONFIG['lambda_D']}\")\n",
    "print(f\"  Alpha: {CONFIG['lambda_alpha']}\")\n",
    "print(f\"  Model: {CONFIG['lambda_model']}\")\n",
    "print(f\"  Label smoothing: {CONFIG['label_smoothing']}\")\n",
    "print()\n",
    "print(\"Metrics tracked:\")\n",
    "print(\"  Regression (D, Î±): MSE (loss), MAE (evaluation)\")\n",
    "print(\"  Classification (model): CrossEntropy (loss), Accuracy (evaluation)\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Initialize metric tracker\n",
    "metric_tracker = MetricTracker()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGhhpHnCyn2G"
   },
   "source": [
    "## Cell 7: Training Loop with Real-Time Visualization\n",
    "\n",
    "**Training procedure:**\n",
    "1. Forward pass through model\n",
    "2. Compute multi-task loss\n",
    "3. Backward pass with gradient clipping\n",
    "4. Optimizer step\n",
    "5. Validation after each epoch\n",
    "6. Generate plots for all 4 heads\n",
    "7. Save best checkpoint per head\n",
    "8. Learning rate scheduling\n",
    "9. Early stopping\n",
    "\n",
    "**Plots generated per epoch (3 heads Ã— 1 figure each):**\n",
    "- **D, Î± heads:** 2 subplots (Train: Loss+MAE | Val: Loss+MAE)\n",
    "- **Model Type head:** 2 subplots (Train: Loss+Accuracy | Val: Loss+Accuracy)\n",
    "\n",
    "**Best model checkpointing:**\n",
    "- `best_D_model.pth`: Best validation MAE for D\n",
    "- `best_alpha_model.pth`: Best validation MAE for Î±\n",
    "- `best_model_type_model.pth`: Best validation accuracy for model classification\n",
    "\n",
    "**TEST_MODE behavior:**\n",
    "- Runs only 5 epochs with small dataset\n",
    "- Still generates all plots and checkpoints for pipeline verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 80,
     "status": "ok",
     "timestamp": 1763315875003,
     "user": {
      "displayName": "Enow George",
      "userId": "04500551558329927866"
     },
     "user_tz": 0
    },
    "id": "VY86kLyh3mDz",
    "outputId": "0af39796-6e5b-49d4-cab7-ac21f8c453cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 11.76 GB\n",
      "GPU memory reserved: 11.97 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Clear all GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Print current memory usage\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "print(f\"GPU memory reserved: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1815,
     "status": "ok",
     "timestamp": 1763315876833,
     "user": {
      "displayName": "Enow George",
      "userId": "04500551558329927866"
     },
     "user_tz": 0
    },
    "id": "P_mrcBQZKkY3",
    "outputId": "2082eb79-c47c-4eec-ea91-67c9efb0e0a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreating DataLoaders for profiling...\n",
      "âœ“ Fresh DataLoader created\n",
      "\n",
      "======================================================================\n",
      "PROFILING TRAINING LOOP - 10 BATCHES\n",
      "======================================================================\n",
      "\n",
      "Batch 1/10: 0.25s total\n",
      "Batch 2/10: 0.16s total\n",
      "Batch 3/10: 0.16s total\n",
      "Batch 4/10: 0.16s total\n",
      "Batch 5/10: 0.18s total\n",
      "Batch 6/10: 0.16s total\n",
      "Batch 7/10: 0.16s total\n",
      "Batch 8/10: 0.16s total\n",
      "Batch 9/10: 0.17s total\n",
      "Batch 10/10: 0.16s total\n",
      "\n",
      "======================================================================\n",
      "PROFILING RESULTS (Average over 10 batches)\n",
      "======================================================================\n",
      "\n",
      "Component                 Time (s)     % of Total  \n",
      "----------------------------------------------------------------------\n",
      "data_loading                 0.022          12.8%\n",
      "to_device                    0.000           0.2%\n",
      "forward_pass                 0.051          29.2%\n",
      "loss_computation             0.001           0.9%\n",
      "backward_pass                0.098          56.4%\n",
      "----------------------------------------------------------------------\n",
      "TOTAL                        0.174      100.0%\n",
      "\n",
      "======================================================================\n",
      "BOTTLENECK: backward_pass (0.10s, 56.4%)\n",
      "======================================================================\n",
      "\n",
      "DIAGNOSIS: Backward pass is the bottleneck\n",
      "SOLUTIONS:\n",
      "  - This is unusual - may indicate gradient checkpoint issues\n",
      "  - Check model architecture\n",
      "\n",
      "Expected batch time on A100: 0.5-1.0 seconds (with RAM dataset)\n",
      "Your actual batch time: 0.17 seconds\n",
      "\n",
      "âœ“ Batch time is within expected range\n",
      "======================================================================\n",
      "\n",
      "NOTE: You can now proceed with training using the original train_loader\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PROFILING CELL - Run this BEFORE starting training (or after restarting DataLoaders)\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# =============================================================================\n",
    "# RECREATE DATALOADERS (to avoid persistent_workers issues)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Recreating DataLoaders for profiling...\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader_profile = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(\"âœ“ Fresh DataLoader created\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# PROFILING CODE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROFILING TRAINING LOOP - 10 BATCHES\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "model.eval()  # Use eval mode to avoid gradient accumulation\n",
    "\n",
    "times = {\n",
    "    'data_loading': [],\n",
    "    'to_device': [],\n",
    "    'forward_pass': [],\n",
    "    'loss_computation': [],\n",
    "    'backward_pass': [],\n",
    "    'total_batch': []\n",
    "}\n",
    "\n",
    "# Profile 10 batches\n",
    "train_iter = iter(train_loader_profile)\n",
    "for i in range(10):\n",
    "    batch_start = time.time()\n",
    "\n",
    "    # 1. Data loading\n",
    "    load_start = time.time()\n",
    "    try:\n",
    "        batch = next(train_iter)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    load_time = time.time() - load_start\n",
    "    times['data_loading'].append(load_time)\n",
    "\n",
    "    # 2. Move to device\n",
    "    device_start = time.time()\n",
    "    batch = {k: v.to(CONFIG['device']) for k, v in batch.items()}\n",
    "    device_time = time.time() - device_start\n",
    "    times['to_device'].append(device_time)\n",
    "\n",
    "    # 3. Forward pass\n",
    "    forward_start = time.time()\n",
    "    with torch.set_grad_enabled(True):\n",
    "        D_pred, alpha_pred, model_logits, _ = model(batch)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()  # Wait for GPU to finish\n",
    "    forward_time = time.time() - forward_start\n",
    "    times['forward_pass'].append(forward_time)\n",
    "\n",
    "    # 4. Loss computation\n",
    "    loss_start = time.time()\n",
    "    loss, metrics = compute_loss(\n",
    "        D_pred, alpha_pred, model_logits,\n",
    "        batch['D_true'], batch['alpha_true'], batch['model_type'],\n",
    "        CONFIG\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    loss_time = time.time() - loss_start\n",
    "    times['loss_computation'].append(loss_time)\n",
    "\n",
    "    # 5. Backward pass\n",
    "    backward_start = time.time()\n",
    "    loss.backward()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    backward_time = time.time() - backward_start\n",
    "    times['backward_pass'].append(backward_time)\n",
    "\n",
    "    # Clear gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Total batch time\n",
    "    batch_time = time.time() - batch_start\n",
    "    times['total_batch'].append(batch_time)\n",
    "\n",
    "    print(f\"Batch {i+1}/10: {batch_time:.2f}s total\")\n",
    "\n",
    "# =============================================================================\n",
    "# PRINT RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"PROFILING RESULTS (Average over 10 batches)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Calculate averages\n",
    "avg_times = {k: sum(v)/len(v) for k, v in times.items()}\n",
    "\n",
    "# Print breakdown\n",
    "print(f\"{'Component':<25} {'Time (s)':<12} {'% of Total':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "total = avg_times['total_batch']\n",
    "for component in ['data_loading', 'to_device', 'forward_pass', 'loss_computation', 'backward_pass']:\n",
    "    time_val = avg_times[component]\n",
    "    pct = (time_val / total) * 100\n",
    "    print(f\"{component:<25} {time_val:>8.3f}      {pct:>8.1f}%\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'TOTAL':<25} {total:>8.3f}      100.0%\")\n",
    "print()\n",
    "\n",
    "# Identify bottleneck\n",
    "bottleneck = max(avg_times.items(), key=lambda x: x[1] if x[0] != 'total_batch' else 0)\n",
    "print(\"=\" * 70)\n",
    "print(f\"BOTTLENECK: {bottleneck[0]} ({bottleneck[1]:.2f}s, {(bottleneck[1]/total)*100:.1f}%)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Recommendations\n",
    "if bottleneck[0] == 'data_loading':\n",
    "    print(\"DIAGNOSIS: Data loading is the bottleneck\")\n",
    "    print(\"SOLUTIONS:\")\n",
    "    print(\"  - Already using RAM-based dataset\")\n",
    "    print(\"  - This is unusual - check if dataset is actually in RAM\")\n",
    "\n",
    "elif bottleneck[0] == 'forward_pass':\n",
    "    print(\"DIAGNOSIS: GPU forward pass computation is the bottleneck\")\n",
    "    print(\"SOLUTIONS:\")\n",
    "    print(\"  - This is NORMAL and EXPECTED with 1000-step trajectories\")\n",
    "    print(\"  - Model is correctly utilizing GPU\")\n",
    "    print(\"  - Current speed is appropriate for this model complexity\")\n",
    "\n",
    "elif bottleneck[0] == 'to_device':\n",
    "    print(\"DIAGNOSIS: CPU->GPU data transfer is the bottleneck\")\n",
    "    print(\"SOLUTIONS:\")\n",
    "    print(\"  - Ensure pin_memory=True in DataLoader\")\n",
    "    print(\"  - Check GPU memory isn't full\")\n",
    "\n",
    "elif bottleneck[0] == 'backward_pass':\n",
    "    print(\"DIAGNOSIS: Backward pass is the bottleneck\")\n",
    "    print(\"SOLUTIONS:\")\n",
    "    print(\"  - This is unusual - may indicate gradient checkpoint issues\")\n",
    "    print(\"  - Check model architecture\")\n",
    "\n",
    "print()\n",
    "print(\"Expected batch time on A100: 0.5-1.0 seconds (with RAM dataset)\")\n",
    "print(f\"Your actual batch time: {total:.2f} seconds\")\n",
    "print()\n",
    "\n",
    "if total > 10:\n",
    "    print(\"âš  CRITICAL: Batch time > 10s - RAM loading may have failed!\")\n",
    "elif total > 2:\n",
    "    print(\"âš  WARNING: Batch time is slower than expected\")\n",
    "else:\n",
    "    print(\"âœ“ Batch time is within expected range\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"NOTE: You can now proceed with training using the original train_loader\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694,
     "referenced_widgets": [
      "ae52d480bd0447269a2fd6779aa456bb",
      "440896f191d84362b39f916d91077c39",
      "5df191cbc6bc49b2b072edc52f64fbc1",
      "e1bc5a426da041239b987bab279b32be",
      "23056f8c916c46a08a1f6ec3cfe35d6e",
      "b003d98710394cf4878db6eba19849df",
      "2b0b9bbdaaa1478488cfb7f3332fd257",
      "9d312988f5f0434fbaec647a528ebff9",
      "bdb24edb65754595b421ff30befa5510",
      "5640282838804b1e91ffe40d8c625971",
      "375810e5b14943af97227ec1c1c87634"
     ]
    },
    "executionInfo": {
     "elapsed": 8338,
     "status": "error",
     "timestamp": 1763315885173,
     "user": {
      "displayName": "Enow George",
      "userId": "04500551558329927866"
     },
     "user_tz": 0
    },
    "id": "EH__31dsyn2G",
    "outputId": "dd9c2330-52bb-41ce-dfe8-f0b17e0197e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING SETUP\n",
      "======================================================================\n",
      "âœ“ Mixed Precision Training (AMP) enabled with GradScaler\n",
      "Optimizer: Adam (lr=0.0004, weight_decay=0.0001)\n",
      "Scheduler: ReduceLROnPlateau (factor=0.5, patience=5)\n",
      "Early stopping patience: 20 epochs\n",
      "Gradient clipping: 1.0\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STARTING TRAINING\n",
      "======================================================================\n",
      "Total epochs: 100\n",
      "Train batches: 10,547\n",
      "Val batches: 1,172\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae52d480bd0447269a2fd6779aa456bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/100 [Train]:   0%|          | 0/10547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3544424936.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             loss, metrics = compute_loss(\n\u001b[0m\u001b[1;32m    233\u001b[0m                 \u001b[0mD_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_logits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'D_true'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alpha_true'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2730354801.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(D_pred, alpha_pred, model_logits, D_true, alpha_true, model_true, config)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     metrics_dict = {\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;34m'L_D'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mL_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;34m'L_alpha'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mL_alpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;34m'L_model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mL_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# =============================================================================\n",
    "# PLOTTING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def generate_training_plots(epoch, train_metrics, val_metrics, config):\n",
    "    \"\"\"Generate plots for all 4 output heads\"\"\"\n",
    "\n",
    "    # Set style\n",
    "    sns.set_theme(style=\"whitegrid\", context=\"paper\")\n",
    "\n",
    "    # Head configurations\n",
    "    heads_config = [\n",
    "        {\n",
    "            'name': 'D_head',\n",
    "            'title': 'D-Head (Diffusion Coefficient)',\n",
    "            'loss_key': 'L_D',\n",
    "            'metric_key': 'MAE_D',\n",
    "            'metric_name': 'MAE',\n",
    "        },\n",
    "        {\n",
    "            'name': 'alpha_head',\n",
    "            'title': 'Alpha-Head (Anomalous Exponent)',\n",
    "            'loss_key': 'L_alpha',\n",
    "            'metric_key': 'MAE_alpha',\n",
    "            'metric_name': 'MAE',\n",
    "        },\n",
    "        {\n",
    "            'name': 'model_type_head',\n",
    "            'title': 'Model-Type-Head (Classification)',\n",
    "            'loss_key': 'L_model',\n",
    "            'metric_key': 'accuracy_model',\n",
    "            'metric_name': 'Accuracy (%)',\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for head_config in heads_config:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        fig.suptitle(f\"{head_config['title']} - Epoch {epoch+1}\",\n",
    "                     fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "        # Left subplot: Training\n",
    "        ax1_twin = ax1.twinx()\n",
    "\n",
    "        # Plot loss\n",
    "        train_loss_history = [m[head_config['loss_key']] for m in metric_tracker.history['train'][:epoch+1]]\n",
    "        ax1.plot(range(1, epoch+2), train_loss_history, 'b-', linewidth=2, label='Loss (MSE)' if 'model' not in head_config['name'] else 'Loss (CE)')\n",
    "        ax1.set_xlabel('Epoch', fontweight='bold')\n",
    "        ax1.set_ylabel('Loss', color='b', fontweight='bold')\n",
    "        ax1.tick_params(axis='y', labelcolor='b')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot metric (MAE or Accuracy)\n",
    "        train_metric_history = [m[head_config['metric_key']] for m in metric_tracker.history['train'][:epoch+1]]\n",
    "        if head_config['metric_name'] == 'Accuracy (%)':\n",
    "            train_metric_history = [m * 100 for m in train_metric_history]  # Convert to percentage\n",
    "        ax1_twin.plot(range(1, epoch+2), train_metric_history, 'orange', linewidth=2, label=head_config['metric_name'])\n",
    "        ax1_twin.set_ylabel(head_config['metric_name'], color='orange', fontweight='bold')\n",
    "        ax1_twin.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "        ax1.set_title('Training', fontweight='bold')\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax1_twin.legend(loc='upper right')\n",
    "\n",
    "        # Right subplot: Validation\n",
    "        ax2_twin = ax2.twinx()\n",
    "\n",
    "        # Plot loss\n",
    "        val_loss_history = [m[head_config['loss_key']] for m in metric_tracker.history['val'][:epoch+1]]\n",
    "        ax2.plot(range(1, epoch+2), val_loss_history, 'b-', linewidth=2, label='Loss (MSE)' if 'model' not in head_config['name'] else 'Loss (CE)')\n",
    "        ax2.set_xlabel('Epoch', fontweight='bold')\n",
    "        ax2.set_ylabel('Loss', color='b', fontweight='bold')\n",
    "        ax2.tick_params(axis='y', labelcolor='b')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot metric\n",
    "        val_metric_history = [m[head_config['metric_key']] for m in metric_tracker.history['val'][:epoch+1]]\n",
    "        if head_config['metric_name'] == 'Accuracy (%)':\n",
    "            val_metric_history = [m * 100 for m in val_metric_history]\n",
    "        ax2_twin.plot(range(1, epoch+2), val_metric_history, 'orange', linewidth=2, label=head_config['metric_name'])\n",
    "        ax2_twin.set_ylabel(head_config['metric_name'], color='orange', fontweight='bold')\n",
    "        ax2_twin.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "        ax2.set_title('Validation', fontweight='bold')\n",
    "        ax2.legend(loc='upper left')\n",
    "        ax2_twin.legend(loc='upper right')\n",
    "\n",
    "        # Save figure\n",
    "        plot_path = os.path.join(config['plots_dir'], head_config['name'], f\"epoch_{epoch+1:03d}.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Save CSV data alongside plot\n",
    "        csv_path = os.path.join(config['plots_dir'], head_config['name'], f\"epoch_{epoch+1:03d}.csv\")\n",
    "        csv_data = pd.DataFrame({\n",
    "            'epoch': list(range(1, epoch+2)),\n",
    "            'train_loss': train_loss_history,\n",
    "            'train_metric': train_metric_history,\n",
    "            'val_loss': val_loss_history,\n",
    "            'val_metric': val_metric_history\n",
    "        })\n",
    "        csv_data.to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING SETUP\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING SETUP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']  # L2 regularization to prevent overfitting\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=CONFIG['lr_scheduler_factor'],\n",
    "    patience=CONFIG['lr_scheduler_patience']\n",
    ")\n",
    "\n",
    "# Initialize GradScaler for Mixed Precision Training (AMP)\n",
    "scaler = torch.amp.GradScaler('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"âœ“ Mixed Precision Training (AMP) enabled with GradScaler\")\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "start_epoch = 0\n",
    "\n",
    "print(f\"Optimizer: Adam (lr={CONFIG['learning_rate']}, weight_decay={CONFIG['weight_decay']})\")\n",
    "print(f\"Scheduler: ReduceLROnPlateau (factor={CONFIG['lr_scheduler_factor']}, patience={CONFIG['lr_scheduler_patience']})\")\n",
    "print(f\"Early stopping patience: {CONFIG['patience']} epochs\")\n",
    "print(f\"Gradient clipping: {CONFIG['gradient_clip_norm']}\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD CHECKPOINT IF RESUMING\n",
    "# =============================================================================\n",
    "\n",
    "if CONFIG.get('resume_from_checkpoint') is not None:\n",
    "    print(\"=\"*70)\n",
    "    print(\"LOADING CHECKPOINT\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    checkpoint_path = CONFIG['resume_from_checkpoint']\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=CONFIG['device'], weights_only=False )\n",
    "\n",
    "        # Load model state\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"âœ“ Model state loaded\")\n",
    "\n",
    "        # Load optimizer state\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(\"âœ“ Optimizer state loaded\")\n",
    "\n",
    "        # Load scheduler state\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        print(\"âœ“ Scheduler state loaded\")\n",
    "\n",
    "        # Load training state\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        metric_tracker = checkpoint['metric_tracker']\n",
    "        best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "        epochs_without_improvement = checkpoint.get('epochs_without_improvement', 0)\n",
    "\n",
    "        print(f\"âœ“ Resuming from epoch {start_epoch}\")\n",
    "        print(f\"  Best val loss: {best_val_loss:.4f}\")\n",
    "        print(f\"  Epochs without improvement: {epochs_without_improvement}\")\n",
    "        print(\"=\"*70)\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"âš  Checkpoint not found: {checkpoint_path}\")\n",
    "        print(\"  Starting fresh training...\")\n",
    "        print(\"=\"*70)\n",
    "        print()\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"Train batches: {len(train_loader):,}\")\n",
    "print(f\"Val batches: {len(val_loader):,}\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, CONFIG['num_epochs']):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    # =========================================================================\n",
    "    # TRAINING PHASE\n",
    "    # =========================================================================\n",
    "    model.train()\n",
    "    train_metrics_accum = {\n",
    "        'L_D': [], 'L_alpha': [], 'L_model': [],\n",
    "        'MAE_D': [], 'MAE_alpha': [], 'accuracy_model': []\n",
    "    }\n",
    "\n",
    "    # Gradient accumulation for memory efficiency\n",
    "    accumulation_steps = CONFIG['gradient_accumulation_steps']\n",
    "\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['num_epochs']} [Train]\")\n",
    "    for batch_idx, batch in enumerate(train_pbar):\n",
    "        # Move to device\n",
    "        batch = {k: v.to(CONFIG['device']) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass and loss computation with mixed precision\n",
    "        with torch.amp.autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            # Forward pass\n",
    "            D_pred, alpha_pred, model_logits, _ = model(batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss, metrics = compute_loss(\n",
    "                D_pred, alpha_pred, model_logits,\n",
    "                batch['D_true'], batch['alpha_true'], batch['model_type'],\n",
    "                CONFIG\n",
    "            )\n",
    "\n",
    "            # Normalize loss for gradient accumulation\n",
    "            loss = loss / accumulation_steps\n",
    "\n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Only step optimizer every accumulation_steps\n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            # Unscale gradients for clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['gradient_clip_norm'])\n",
    "\n",
    "            # Optimizer step with scaler\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Accumulate metrics (multiply back for logging)\n",
    "        for key in train_metrics_accum:\n",
    "            train_metrics_accum[key].append(metrics[key])\n",
    "\n",
    "        # Update progress bar\n",
    "        train_pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'MAE_D': f\"{metrics['MAE_D']:.4e}\"\n",
    "        })\n",
    "\n",
    "    # Average training metrics\n",
    "    train_metrics_avg = {k: np.nanmean(v) if len(v) > 0 else np.nan for k, v in train_metrics_accum.items()}\n",
    "    metric_tracker.update(epoch, train_metrics_avg, mode='train')\n",
    "\n",
    "    # =========================================================================\n",
    "    # VALIDATION PHASE\n",
    "    # =========================================================================\n",
    "    model.eval()\n",
    "    val_metrics_accum = {\n",
    "        'L_D': [], 'L_alpha': [], 'L_model': [],\n",
    "        'MAE_D': [], 'MAE_alpha': [], 'accuracy_model': []\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{CONFIG['num_epochs']} [Val]  \")\n",
    "        for batch in val_pbar:\n",
    "            # Move to device\n",
    "            batch = {k: v.to(CONFIG['device']) for k, v in batch.items()}\n",
    "\n",
    "            # Forward pass with mixed precision\n",
    "            with torch.amp.autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                D_pred, alpha_pred, model_logits, _ = model(batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss, metrics = compute_loss(\n",
    "                D_pred, alpha_pred, model_logits,\n",
    "                batch['D_true'], batch['alpha_true'], batch['model_type'],\n",
    "                CONFIG\n",
    "            )\n",
    "\n",
    "            # Accumulate metrics\n",
    "            for key in val_metrics_accum:\n",
    "                val_metrics_accum[key].append(metrics[key])\n",
    "\n",
    "            # Update progress bar\n",
    "            val_pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'MAE_D': f\"{metrics['MAE_D']:.4e}\"\n",
    "            })\n",
    "\n",
    "    # Average validation metrics\n",
    "    val_metrics_avg = {k: np.nanmean(v) if len(v) > 0 else np.nan for k, v in val_metrics_accum.items()}\n",
    "    improved = metric_tracker.update(epoch, val_metrics_avg, mode='val')\n",
    "\n",
    "    # =========================================================================\n",
    "    # LOGGING & CHECKPOINTING\n",
    "    # =========================================================================\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "    print()\n",
    "    print(f\"Epoch {epoch+1}/{CONFIG['num_epochs']} Summary ({epoch_time:.1f}s):\")\n",
    "    print(f\"  Train Loss: {train_metrics_avg['L_D']:.4f} (D), \"\n",
    "            f\"{train_metrics_avg['L_alpha']:.4f} (Î±), {train_metrics_avg['L_model']:.4f} (model)\")\n",
    "    print(f\"  Val Loss:   {val_metrics_avg['L_D']:.4f} (D), \"\n",
    "            f\"{val_metrics_avg['L_alpha']:.4f} (Î±), {val_metrics_avg['L_model']:.4f} (model)\")\n",
    "    print(f\"  Val MAE:    {val_metrics_avg['MAE_D']:.4e} (D), \"\n",
    "            f\"{val_metrics_avg['MAE_alpha']:.4f} (Î±)\")\n",
    "    print(f\"  Val Acc:    {val_metrics_avg['accuracy_model']*100:.2f}% (model)\")\n",
    "\n",
    "    # Save best models\n",
    "    if improved.get('D'):\n",
    "        torch.save(model.state_dict(), os.path.join(CONFIG['checkpoint_dir'], 'best_D_model.pth'))\n",
    "        print(f\"  âœ“ New best D model saved (MAE: {val_metrics_avg['MAE_D']:.4e})\")\n",
    "\n",
    "    if improved.get('alpha'):\n",
    "        torch.save(model.state_dict(), os.path.join(CONFIG['checkpoint_dir'], 'best_alpha_model.pth'))\n",
    "        print(f\"  âœ“ New best Î± model saved (MAE: {val_metrics_avg['MAE_alpha']:.4f})\")\n",
    "\n",
    "    if improved.get('model'):\n",
    "        torch.save(model.state_dict(), os.path.join(CONFIG['checkpoint_dir'], 'best_model_type_model.pth'))\n",
    "        print(f\"  âœ“ New best model-type model saved (Acc: {val_metrics_avg['accuracy_model']*100:.2f}%)\")\n",
    "\n",
    "    # Generate plots\n",
    "    generate_training_plots(epoch, train_metrics_avg, val_metrics_avg, CONFIG)\n",
    "    print(f\"  âœ“ Plots generated for epoch {epoch+1}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # PERIODIC CHECKPOINT SAVING\n",
    "    # =========================================================================\n",
    "\n",
    "    if (epoch + 1) % CONFIG.get('save_checkpoint_every', 10) == 0:\n",
    "        checkpoint_path = os.path.join(CONFIG['checkpoint_dir'], f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'metric_tracker': metric_tracker,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'epochs_without_improvement': epochs_without_improvement,\n",
    "            'config': CONFIG,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"  âœ“ Checkpoint saved: checkpoint_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # LEARNING RATE SCHEDULING\n",
    "# =========================================================================\n",
    "\n",
    "    val_loss_total = (val_metrics_avg['L_D'] +\n",
    "                     val_metrics_avg['L_alpha'] + val_metrics_avg['L_model'])\n",
    "    scheduler.step(val_loss_total)\n",
    "\n",
    "    # =========================================================================\n",
    "    # EARLY STOPPING CHECK\n",
    "    # =========================================================================\n",
    "\n",
    "    if val_loss_total < best_val_loss:\n",
    "        best_val_loss = val_loss_total\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= CONFIG['patience']:\n",
    "        print(f\"\\nâš  Early stopping triggered (no improvement for {CONFIG['patience']} epochs)\")\n",
    "        break\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total training time: {training_time/3600:.2f} hours\")\n",
    "print(f\"Average time per epoch: {training_time/(epoch+1):.1f}s\")\n",
    "print()\n",
    "print(\"Best validation metrics:\")\n",
    "best_summary = metric_tracker.get_best_summary()\n",
    "\n",
    "print(f\"  D MAE:     {best_summary['D_MAE']:.4e} (epoch {best_summary['D_epoch']+1})\")\n",
    "print(f\"  Î± MAE:     {best_summary['alpha_MAE']:.4f} (epoch {best_summary['alpha_epoch']+1})\")\n",
    "print(f\"  Model Acc: {best_summary['model_accuracy']*100:.2f}% (epoch {best_summary['model_epoch']+1})\")\n",
    "print(\"=\"*70)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abGv3mt_yn2H"
   },
   "source": [
    "## Cell 8: Final Model & Training History\n",
    "\n",
    "Saves the final model state and complete training history for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 89642,
     "status": "aborted",
     "timestamp": 1763315885170,
     "user": {
      "displayName": "Enow George",
      "userId": "04500551558329927866"
     },
     "user_tz": 0
    },
    "id": "S-eGLpXCyn2H"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE FINAL MODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SAVING FINAL MODEL & TRAINING HISTORY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save final model state\n",
    "final_model_path = os.path.join(CONFIG['checkpoint_dir'], 'final_model.pth')\n",
    "torch.save({\n",
    "    'epoch': epoch + 1,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': CONFIG,\n",
    "    'best_metrics': metric_tracker.best_metrics,\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"âœ“ Final model saved: {final_model_path}\")\n",
    "print()\n",
    "\n",
    "# Save training history\n",
    "history_path = os.path.join(CONFIG['logs_dir'], 'training_history.json')\n",
    "history_data = {\n",
    "    'config': {k: str(v) if isinstance(v, torch.device) else v for k, v in CONFIG.items()},\n",
    "    'training_history': metric_tracker.history,\n",
    "    'best_metrics': metric_tracker.get_best_summary(),\n",
    "    'total_epochs': epoch + 1,\n",
    "    'training_time_hours': training_time / 3600,\n",
    "}\n",
    "\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history_data, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Training history saved: {history_path}\")\n",
    "print()\n",
    "\n",
    "# Save best metrics summary\n",
    "best_metrics_path = os.path.join(CONFIG['logs_dir'], 'best_metrics.json')\n",
    "with open(best_metrics_path, 'w') as f:\n",
    "    json.dump(metric_tracker.get_best_summary(), f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Best metrics saved: {best_metrics_path}\")\n",
    "print()\n",
    "\n",
    "# Print summary table\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Mode: {'TEST' if CONFIG['TEST_MODE'] else 'PRODUCTION'}\")\n",
    "print(f\"Total epochs: {epoch + 1}\")\n",
    "print(f\"Training time: {training_time/3600:.2f} hours\")\n",
    "print(f\"Avg time per epoch: {training_time/(epoch+1):.1f} seconds\")\n",
    "print()\n",
    "print(\"Best Validation Metrics:\")\n",
    "\n",
    "print(f\"  D MAE:         {best_summary['D_MAE']:.4e} (epoch {best_summary['D_epoch']+1})\")\n",
    "print(f\"  Î± MAE:         {best_summary['alpha_MAE']:.4f} (epoch {best_summary['alpha_epoch']+1})\")\n",
    "print(f\"  Model Accuracy: {best_summary['model_accuracy']*100:.2f}% (epoch {best_summary['model_epoch']+1})\")\n",
    "print()\n",
    "print(\"Saved Files:\")\n",
    "print(f\"  Final model: {final_model_path}\")\n",
    "print(f\"  Best D model: {os.path.join(CONFIG['checkpoint_dir'], 'best_D_model.pth')}\")\n",
    "print(f\"  Best Î± model: {os.path.join(CONFIG['checkpoint_dir'], 'best_alpha_model.pth')}\")\n",
    "print(f\"  Best model-type model: {os.path.join(CONFIG['checkpoint_dir'], 'best_model_type_model.pth')}\")\n",
    "print(f\"  Training history: {history_path}\")\n",
    "print(\"=\"*70)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnTa9P4oyn2H"
   },
   "source": [
    "## Cell 9: Test Set Evaluation\n",
    "\n",
    "Evaluates best models on held-out test set.\n",
    "\n",
    "**Loads:** Best checkpoint for each head\n",
    "**Computes:** Final performance metrics\n",
    "**Generates:**\n",
    "- Scatter plots (predicted vs true) for D, Î±\n",
    "- Confusion matrix for model classification\n",
    "- Performance comparison table vs. Firbas et al. baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29743,
     "status": "ok",
     "timestamp": 1763315930294,
     "user": {
      "displayName": "Enow George",
      "userId": "04500551558329927866"
     },
     "user_tz": 0
    },
    "id": "51MBXaBs-UbX",
    "outputId": "27fd72fd-f74a-4817-f8dd-6e484e7eaa63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREVIEW: ALPHA REGRESSION HEATMAPS (RANDOM DATA)\n",
      "======================================================================\n",
      "\n",
      "Generating preview plots with random synthetic data...\n",
      "\n",
      "Generating preview heatmap for ATTM...\n",
      "  âœ“ Preview saved: /content/drive/MyDrive/ERP_Shrey/outputs3/PREVIEW_alpha_heatmap_ATTM.png\n",
      "Generating preview heatmap for CTRW...\n",
      "  âœ“ Preview saved: /content/drive/MyDrive/ERP_Shrey/outputs3/PREVIEW_alpha_heatmap_CTRW.png\n",
      "Generating preview heatmap for FBM...\n",
      "  âœ“ Preview saved: /content/drive/MyDrive/ERP_Shrey/outputs3/PREVIEW_alpha_heatmap_FBM.png\n",
      "Generating preview heatmap for LW...\n",
      "  âœ“ Preview saved: /content/drive/MyDrive/ERP_Shrey/outputs3/PREVIEW_alpha_heatmap_LW.png\n",
      "Generating preview heatmap for SBM...\n",
      "  âœ“ Preview saved: /content/drive/MyDrive/ERP_Shrey/outputs3/PREVIEW_alpha_heatmap_SBM.png\n",
      "\n",
      "======================================================================\n",
      "GENERATING PREVIEW: D (DIFFUSION COEFFICIENT) HEATMAPS\n",
      "======================================================================\n",
      "\n",
      "Generating preview D heatmap for ATTM...\n",
      "  âœ“ Preview saved: /content/drive/MyDrive/ERP_Shrey/outputs3/PREVIEW_D_heatmap_ATTM.png\n",
      "Generating preview D heatmap for CTRW...\n",
      "  âœ“ Preview saved: /content/drive/MyDrive/ERP_Shrey/outputs3/PREVIEW_D_heatmap_CTRW.png\n",
      "Generating preview D heatmap for FBM...\n",
      "  âœ“ Preview saved: /content/drive/MyDrive/ERP_Shrey/outputs3/PREVIEW_D_heatmap_FBM.png\n",
      "Generating preview D heatmap for LW...\n",
      "  âœ“ Preview saved: /content/drive/MyDrive/ERP_Shrey/outputs3/PREVIEW_D_heatmap_LW.png\n",
      "Generating preview D heatmap for SBM...\n",
      "  âœ“ Preview saved: /content/drive/MyDrive/ERP_Shrey/outputs3/PREVIEW_D_heatmap_SBM.png\n",
      "\n",
      "======================================================================\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "PREVIEW COMPLETE\n",
      "======================================================================\n",
      "Check the generated preview plots in the outputs directory.\n",
      "These show the visual style with random data before running actual evaluation.\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PREVIEW: ALPHA REGRESSION HEATMAPS WITH RANDOM DATA (VISUAL PREVIEW)\n",
    "# =============================================================================\n",
    "# This cell generates sample heatmaps with random data to preview the visual style\n",
    "# before running the actual evaluation. Run this cell to see how the plots will look.\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PREVIEW: ALPHA REGRESSION HEATMAPS (RANDOM DATA)\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"Generating preview plots with random synthetic data...\")\n",
    "print()\n",
    "\n",
    "# Model-specific alpha ranges (ANDI Table 2)\n",
    "model_alpha_ranges = {\n",
    "    0: (0.10, 1.00),   # ATTM\n",
    "    1: (0.10, 1.00),   # CTRW\n",
    "    2: (0.10, 1.90),   # FBM\n",
    "    3: (1.00, 1.90),   # LW\n",
    "    4: (0.10, 1.90),   # SBM\n",
    "}\n",
    "\n",
    "# Create white-to-red colormap\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors_white_red = ['white', '#ffcccc', '#ff9999', '#ff6666', '#ff3333', '#ff0000', '#cc0000']\n",
    "n_bins = 256\n",
    "white_red_cmap = LinearSegmentedColormap.from_list('white_red', colors_white_red, N=n_bins)\n",
    "\n",
    "# Model names and SNR values for preview\n",
    "model_names_preview = ['ATTM', 'CTRW', 'FBM', 'LW', 'SBM']\n",
    "snr_values_preview = [1, 2]\n",
    "snr_bins_preview = {1: 'Low SNR (SNR=1)', 2: 'High SNR (SNR=2)'}\n",
    "\n",
    "# Generate preview plots for each model type\n",
    "for model_idx, model_name in enumerate(model_names_preview):\n",
    "    print(f\"Generating preview heatmap for {model_name}...\")\n",
    "\n",
    "    # Get model-specific alpha range\n",
    "    alpha_min, alpha_max = model_alpha_ranges[model_idx]\n",
    "\n",
    "    # Define alpha bins for this model (0.05 step, but only within model range)\n",
    "    alpha_bins_model = np.arange(alpha_min, alpha_max + 0.05, 0.05)\n",
    "\n",
    "    # Generate random synthetic data\n",
    "    np.random.seed(42 + model_idx)  # Different seed per model for variety\n",
    "    n_samples = 5000\n",
    "\n",
    "    # Generate true alpha values within model range\n",
    "    alpha_true_synthetic = np.random.uniform(alpha_min, alpha_max, n_samples)\n",
    "\n",
    "    # Generate predicted alpha with some correlation to true alpha (add noise)\n",
    "    # Add some bias and noise to make it realistic\n",
    "    correlation = 0.85  # Strong but not perfect correlation\n",
    "    noise_scale = (alpha_max - alpha_min) * 0.15\n",
    "    alpha_pred_synthetic = (alpha_true_synthetic * correlation +\n",
    "                           np.random.normal(0, noise_scale, n_samples))\n",
    "\n",
    "    # Clip to model range\n",
    "    alpha_pred_synthetic = np.clip(alpha_pred_synthetic, alpha_min, alpha_max)\n",
    "\n",
    "    # Split data into two SNR groups\n",
    "    n_snr1 = n_samples // 2\n",
    "    alpha_true_snr1 = alpha_true_synthetic[:n_snr1]\n",
    "    alpha_pred_snr1 = alpha_pred_synthetic[:n_snr1]\n",
    "    alpha_true_snr2 = alpha_true_synthetic[n_snr1:]\n",
    "    alpha_pred_snr2 = alpha_pred_synthetic[n_snr1:]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6), facecolor='white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    fig.suptitle(f'PREVIEW: Alpha Regression Heatmap: {model_name}\\n(Predicted vs True Alpha - Random Data)',\n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "    for snr_idx, snr in enumerate(snr_values_preview):\n",
    "        ax = axes[snr_idx]\n",
    "        ax.set_facecolor('white')\n",
    "\n",
    "        # Select data for this SNR\n",
    "        if snr == 1:\n",
    "            alpha_true_filtered = alpha_true_snr1\n",
    "            alpha_pred_filtered = alpha_pred_snr1\n",
    "        else:\n",
    "            alpha_true_filtered = alpha_true_snr2\n",
    "            alpha_pred_filtered = alpha_pred_snr2\n",
    "\n",
    "        # Create 2D histogram (heatmap) using model-specific bins\n",
    "        H, xedges, yedges = np.histogram2d(\n",
    "            alpha_true_filtered,\n",
    "            alpha_pred_filtered,\n",
    "            bins=[alpha_bins_model, alpha_bins_model]\n",
    "        )\n",
    "\n",
    "        # Transpose to have true on x-axis, predicted on y-axis\n",
    "        H = H.T\n",
    "\n",
    "        # Create heatmap with white-to-red colormap (no text annotations, just color)\n",
    "        # Use actual bin edges from histogram for accurate scaling (no gaps)\n",
    "        # extent: [left, right, bottom, top] where left/right = x-axis (true alpha), bottom/top = y-axis (predicted alpha)\n",
    "        im = ax.imshow(H, extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]],\n",
    "                      origin='lower', aspect='auto', cmap=white_red_cmap,\n",
    "                      interpolation='nearest', vmin=0)\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Sample Count', fontweight='bold', fontsize=10)\n",
    "\n",
    "        # Add diagonal line (predicted = true, ideal case)\n",
    "        ax.plot([alpha_bins_model[0], alpha_bins_model[-1]],\n",
    "               [alpha_bins_model[0], alpha_bins_model[-1]],\n",
    "               'k--', linewidth=2, label='Ideal (Predicted = True)', alpha=0.8)\n",
    "\n",
    "        # Add average prediction line (binned by true alpha in 0.1 steps)\n",
    "        if len(alpha_true_filtered) > 1:\n",
    "            # Bin true alpha values in 0.1 steps\n",
    "            alpha_bin_edges = np.arange(alpha_min, alpha_max + 0.1, 0.1)\n",
    "            alpha_bin_centers = (alpha_bin_edges[:-1] + alpha_bin_edges[1:]) / 2\n",
    "\n",
    "            # Compute mean predicted alpha for each true alpha bin\n",
    "            mean_pred_per_bin = []\n",
    "            valid_bin_centers = []\n",
    "\n",
    "            for i in range(len(alpha_bin_edges) - 1):\n",
    "                bin_mask = (alpha_true_filtered >= alpha_bin_edges[i]) & \\\n",
    "                          (alpha_true_filtered < alpha_bin_edges[i+1])\n",
    "\n",
    "                if np.sum(bin_mask) > 0:\n",
    "                    mean_pred = np.mean(alpha_pred_filtered[bin_mask])\n",
    "                    mean_pred_per_bin.append(mean_pred)\n",
    "                    valid_bin_centers.append(alpha_bin_centers[i])\n",
    "\n",
    "            # Plot connected line through all averages\n",
    "            if len(mean_pred_per_bin) > 0:\n",
    "                ax.plot(valid_bin_centers, mean_pred_per_bin, 'b-', linewidth=2,\n",
    "                       marker='o', markersize=4, label='Average (binned)', alpha=0.8)\n",
    "\n",
    "        # Formatting\n",
    "        ax.set_xlabel('True Alpha (Î±)', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('Predicted Alpha (Î±)', fontweight='bold', fontsize=12)\n",
    "        ax.set_title(f'{snr_bins_preview[snr]} (N={len(alpha_true_filtered):,} samples)',\n",
    "                    fontweight='bold', fontsize=14)\n",
    "        ax.legend(loc='upper left', fontsize=9, framealpha=0.9, facecolor='white')\n",
    "        ax.grid(True, alpha=0.3, linestyle=':')\n",
    "        ax.set_xlim([alpha_bins_model[0], alpha_bins_model[-1]])\n",
    "        ax.set_ylim([alpha_bins_model[0], alpha_bins_model[-1]])\n",
    "        ax.set_facecolor('white')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    preview_path = os.path.join(CONFIG['output_dir'], f'PREVIEW_alpha_heatmap_{model_name}.png')\n",
    "    plt.savefig(preview_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "    print(f\"  âœ“ Preview saved: {preview_path}\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"GENERATING PREVIEW: D (DIFFUSION COEFFICIENT) HEATMAPS\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# D ranges (log scale)\n",
    "D_min = 1e-4\n",
    "D_max = 100.0\n",
    "n_D_bins = 50\n",
    "D_bins_log = np.logspace(np.log10(D_min), np.log10(D_max), n_D_bins + 1)\n",
    "\n",
    "# Generate preview D heatmaps\n",
    "for model_idx, model_name in enumerate(model_names_preview):\n",
    "    print(f\"Generating preview D heatmap for {model_name}...\")\n",
    "\n",
    "    # Generate random synthetic D data\n",
    "    np.random.seed(42 + model_idx + 100)  # Different seed\n",
    "    n_samples = 5000\n",
    "\n",
    "    # Generate true D values (log-uniform distribution)\n",
    "    D_true_synthetic = np.random.uniform(D_min, D_max, n_samples)\n",
    "    D_true_synthetic = 10**(np.random.uniform(np.log10(D_min), np.log10(D_max), n_samples))\n",
    "\n",
    "    # Generate predicted D with correlation\n",
    "    correlation = 0.85\n",
    "    noise_scale = (np.log10(D_max) - np.log10(D_min)) * 0.15\n",
    "    D_pred_synthetic = 10**(np.log10(D_true_synthetic) * correlation +\n",
    "                            np.random.normal(0, noise_scale, n_samples))\n",
    "    D_pred_synthetic = np.clip(D_pred_synthetic, D_min, D_max)\n",
    "\n",
    "    # Split into SNR groups\n",
    "    n_snr1 = n_samples // 2\n",
    "    D_true_snr1 = D_true_synthetic[:n_snr1]\n",
    "    D_pred_snr1 = D_pred_synthetic[:n_snr1]\n",
    "    D_true_snr2 = D_true_synthetic[n_snr1:]\n",
    "    D_pred_snr2 = D_pred_synthetic[n_snr1:]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6), facecolor='white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    fig.suptitle(f'PREVIEW: D (Diffusion Coefficient) Regression Heatmap: {model_name}\\n(Predicted vs True D - Random Data)',\n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "    for snr_idx, snr in enumerate(snr_values_preview):\n",
    "        ax = axes[snr_idx]\n",
    "        ax.set_facecolor('white')\n",
    "\n",
    "        if snr == 1:\n",
    "            D_true_filtered = D_true_snr1\n",
    "            D_pred_filtered = D_pred_snr1\n",
    "        else:\n",
    "            D_true_filtered = D_true_snr2\n",
    "            D_pred_filtered = D_pred_snr2\n",
    "\n",
    "        # Create 2D histogram\n",
    "        H_D, xedges, yedges = np.histogram2d(D_true_filtered, D_pred_filtered, bins=[D_bins_log, D_bins_log])\n",
    "        H_D = H_D.T\n",
    "\n",
    "        # Create heatmap\n",
    "        im = ax.imshow(H_D, extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]],\n",
    "                      origin='lower', aspect='auto', cmap=white_red_cmap, interpolation='nearest', vmin=0)\n",
    "\n",
    "        cbar = plt.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Sample Count', fontweight='bold', fontsize=10)\n",
    "\n",
    "        ax.plot([D_min, D_max], [D_min, D_max], 'k--', linewidth=2, label='Ideal (Predicted = True)', alpha=0.8)\n",
    "\n",
    "        # Average line\n",
    "        if len(D_true_filtered) > 1:\n",
    "            n_bins_avg = 20\n",
    "            D_bin_edges_avg = np.logspace(np.log10(D_min), np.log10(D_max), n_bins_avg + 1)\n",
    "            D_bin_centers_avg = np.sqrt(D_bin_edges_avg[:-1] * D_bin_edges_avg[1:])\n",
    "\n",
    "            mean_pred_per_bin = []\n",
    "            valid_bin_centers = []\n",
    "            for i in range(len(D_bin_edges_avg) - 1):\n",
    "                bin_mask = (D_true_filtered >= D_bin_edges_avg[i]) & (D_true_filtered < D_bin_edges_avg[i+1])\n",
    "                if np.sum(bin_mask) > 0:\n",
    "                    mean_pred_per_bin.append(np.mean(D_pred_filtered[bin_mask]))\n",
    "                    valid_bin_centers.append(D_bin_centers_avg[i])\n",
    "\n",
    "            if len(mean_pred_per_bin) > 0:\n",
    "                ax.plot(valid_bin_centers, mean_pred_per_bin, 'b-', linewidth=2,\n",
    "                       marker='o', markersize=4, label='Average (binned)', alpha=0.8)\n",
    "\n",
    "        ax.set_xlabel('True D (Diffusion Coefficient)', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('Predicted D (Diffusion Coefficient)', fontweight='bold', fontsize=12)\n",
    "        ax.set_title(f'{snr_bins_preview[snr]} (N={len(D_true_filtered):,} samples)', fontweight='bold', fontsize=14)\n",
    "        ax.legend(loc='upper left', fontsize=9, framealpha=0.9, facecolor='white')\n",
    "        ax.grid(True, alpha=0.3, linestyle=':')\n",
    "        ax.set_xlim([D_min, D_max])\n",
    "        ax.set_ylim([D_min, D_max])\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_facecolor('white')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    preview_path = os.path.join(CONFIG['output_dir'], f'PREVIEW_D_heatmap_{model_name}.png')\n",
    "    plt.savefig(preview_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "    print(f\"  âœ“ Preview saved: {preview_path}\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "# H heatmap preview section removed\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# H heatmap preview section removed - H predictions no longer used\n",
    "# H heatmap preview section removed - H predictions no longer used\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"PREVIEW COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"Check the generated preview plots in the outputs directory.\")\n",
    "print(\"These show the visual style with random data before running actual evaluation.\")\n",
    "print(\"=\"*70)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d8d5375a59844ce6943fabc37edf767e",
      "befb038591c94ee89c21767378ec0b1d",
      "46eee9d38f60457eb026e0609749eab6",
      "543af6d3d8264c63981cbe219a097a97",
      "ef14c3b73fa14adeb5a28b368a8946e2",
      "9f345f86082e4754b3dd48746270090f",
      "8b3abea5b0e74b4cb2e9b709c9849f25",
      "57542eed2daf44018e1a709c28acd980",
      "c2dea17de9ba4848acd1c86a059b023e",
      "b727dbccaa8843faa7e8dc47aa7ed5e9",
      "b640c615420647df86d9fe63ab33485f"
     ]
    },
    "executionInfo": {
     "elapsed": 259357,
     "status": "ok",
     "timestamp": 1763320416607,
     "user": {
      "displayName": "Enow George",
      "userId": "04500551558329927866"
     },
     "user_tz": 0
    },
    "id": "qVZ2h-Tfyn2I",
    "outputId": "22e1e2d4-9880-4a19-b1a9-80ceebcd7f08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "\n",
      "âœ“ Loaded best model_type model from: /content/drive/MyDrive/ERP_Shrey/outputs3/checkpoints/best_model_type_model.pth\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d5375a59844ce6943fabc37edf767e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Evaluation:   0%|          | 0/3896 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Test set evaluation complete\n",
      "\n",
      "======================================================================\n",
      "OVERALL TEST SET METRICS\n",
      "======================================================================\n",
      "Regression Metrics:\n",
      "  D:     MAE=2.9007e+00, MSE=1.1627e+02, RÂ²=0.4403\n",
      "  Î±:     MAE=0.2749, MSE=0.1434, RÂ²=0.5038\n",
      "\n",
      "Classification Metrics:\n",
      "  Accuracy: 68.44%\n",
      "  F1-score (weighted): 0.6599\n",
      "\n",
      "Loading test dataset metadata for SNR analysis...\n",
      "âœ“ SNR loaded from test_data: 498576 samples\n",
      "  SNR values: [1. 2.]\n",
      "\n",
      "Defining SNR bins (ANDI Table 2: SNR âˆˆ [1, 2])...\n",
      "âœ“ SNR bins defined: {1: 'Low SNR (SNR=1)', 2: 'High SNR (SNR=2)'}\n",
      "âœ“ SNR values to analyze: [1, 2]\n",
      "\n",
      "======================================================================\n",
      "GENERATING MAE VS TRAJECTORY LENGTH PLOTS\n",
      "======================================================================\n",
      "\n",
      "âœ“ Alpha MAE vs length plot saved: /content/drive/MyDrive/ERP_Shrey/outputs3/test_alpha_mae_vs_length_by_snr.png\n",
      "âœ“ D MAE vs length plot saved: /content/drive/MyDrive/ERP_Shrey/outputs3/test_D_mae_vs_length_by_snr.png\n",
      "\n",
      "======================================================================\n",
      "PREVIEW: ALPHA REGRESSION HEATMAPS WITH RANDOM DATA\n",
      "======================================================================\n",
      "Generating sample plots with random data to preview visual style...\n",
      "\n",
      "Generating preview heatmap for ATTM...\n",
      "  âœ“ Preview saved: /content/drive/MyDrive/ERP_Shrey/outputs3/PREVIEW_alpha_heatmap_ATTM.png\n",
      "Generating preview heatmap for CTRW...\n",
      "  âœ“ Preview saved: /content/drive/MyDrive/ERP_Shrey/outputs3/PREVIEW_alpha_heatmap_CTRW.png\n",
      "Generating preview heatmap for FBM...\n",
      "  âœ“ Preview saved: /content/drive/MyDrive/ERP_Shrey/outputs3/PREVIEW_alpha_heatmap_FBM.png\n",
      "Generating preview heatmap for LW...\n",
      "  âœ“ Preview saved: /content/drive/MyDrive/ERP_Shrey/outputs3/PREVIEW_alpha_heatmap_LW.png\n",
      "Generating preview heatmap for SBM...\n",
      "  âœ“ Preview saved: /content/drive/MyDrive/ERP_Shrey/outputs3/PREVIEW_alpha_heatmap_SBM.png\n",
      "\n",
      "======================================================================\n",
      "PREVIEW COMPLETE - Check the generated preview plots above\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "GENERATING ALPHA REGRESSION HEATMAPS BY MODEL TYPE\n",
      "======================================================================\n",
      "\n",
      "Generating heatmap for ATTM...\n",
      "  âœ“ Saved: /content/drive/MyDrive/ERP_Shrey/outputs3/test_alpha_heatmap_ATTM.png\n",
      "Generating heatmap for CTRW...\n",
      "  âœ“ Saved: /content/drive/MyDrive/ERP_Shrey/outputs3/test_alpha_heatmap_CTRW.png\n",
      "Generating heatmap for FBM...\n",
      "  âœ“ Saved: /content/drive/MyDrive/ERP_Shrey/outputs3/test_alpha_heatmap_FBM.png\n",
      "Generating heatmap for LW...\n",
      "  âœ“ Saved: /content/drive/MyDrive/ERP_Shrey/outputs3/test_alpha_heatmap_LW.png\n",
      "Generating heatmap for SBM...\n",
      "  âœ“ Saved: /content/drive/MyDrive/ERP_Shrey/outputs3/test_alpha_heatmap_SBM.png\n",
      "\n",
      "======================================================================\n",
      "GENERATING D (DIFFUSION COEFFICIENT) REGRESSION HEATMAPS BY MODEL TYPE\n",
      "======================================================================\n",
      "\n",
      "Generating D heatmap for ATTM...\n",
      "  âœ“ Saved: /content/drive/MyDrive/ERP_Shrey/outputs3/test_D_heatmap_ATTM.png\n",
      "Generating D heatmap for CTRW...\n",
      "  âœ“ Saved: /content/drive/MyDrive/ERP_Shrey/outputs3/test_D_heatmap_CTRW.png\n",
      "Generating D heatmap for FBM...\n",
      "  âœ“ Saved: /content/drive/MyDrive/ERP_Shrey/outputs3/test_D_heatmap_FBM.png\n",
      "Generating D heatmap for LW...\n",
      "  âœ“ Saved: /content/drive/MyDrive/ERP_Shrey/outputs3/test_D_heatmap_LW.png\n",
      "Generating D heatmap for SBM...\n",
      "  âœ“ Saved: /content/drive/MyDrive/ERP_Shrey/outputs3/test_D_heatmap_SBM.png\n",
      "\n",
      "======================================================================\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "  Alpha - MAE: 0.334442, RÂ² (F1 equivalent): 0.318231\n",
      "  D     - MAE: 1.622296e+00, RÂ² (F1 equivalent): 0.563811\n",
      "\n",
      "CLASSIFICATION METRICS:\n",
      "----------------------------------------------------------------------\n",
      "  Overall Model Accuracy: 0.645145\n",
      "  Overall Model F1 (weighted): 0.623899\n",
      "\n",
      "PER-MODEL METRICS:\n",
      "----------------------------------------------------------------------\n",
      "  ATTM  - Accuracy: 0.876747, F1: 0.347081\n",
      "  CTRW  - Accuracy: 0.911426, F1: 0.617877\n",
      "  FBM   - Accuracy: 0.775438, F1: 0.709641\n",
      "  LW    - Accuracy: 0.944493, F1: 0.785585\n",
      "  SBM   - Accuracy: 0.782186, F1: 0.601921\n",
      "âš  No samples found in trajectory length range 0-50\n",
      "\n",
      "======================================================================\n",
      "âœ“ Metrics saved to: /content/drive/MyDrive/ERP_Shrey/outputs3/test_metrics_summary.txt\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD BEST MODEL FOR EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Load the best model_type checkpoint (better indicator of overall performance)\n",
    "best_model_path = os.path.join(CONFIG['checkpoint_dir'], 'best_model_type_model.pth')\n",
    "model.load_state_dict(torch.load(best_model_path, weights_only=False))\n",
    "model.eval()\n",
    "\n",
    "print(f\"âœ“ Loaded best model_type model from: {best_model_path}\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATE ON TEST SET WITH METADATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "\n",
    "test_predictions = {\n",
    "    'D_pred': [], 'D_true': [],\n",
    "    'alpha_pred': [], 'alpha_true': [],\n",
    "    'model_pred': [], 'model_true': [],\n",
    "    'length': [],  # Store trajectory lengths\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Test Evaluation\"):\n",
    "        # Move to device\n",
    "        batch_gpu = {k: v.to(CONFIG['device']) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        D_pred, alpha_pred, model_logits, _ = model(batch_gpu)\n",
    "        model_pred = torch.argmax(model_logits, dim=1)\n",
    "\n",
    "        # Store predictions and metadata\n",
    "        test_predictions['D_pred'].extend(D_pred.cpu().numpy())\n",
    "        test_predictions['D_true'].extend(batch_gpu['D_true'].cpu().numpy())\n",
    "        test_predictions['alpha_pred'].extend(alpha_pred.cpu().numpy())\n",
    "        test_predictions['alpha_true'].extend(batch_gpu['alpha_true'].cpu().numpy())\n",
    "        test_predictions['model_pred'].extend(model_pred.cpu().numpy())\n",
    "        test_predictions['model_true'].extend(batch_gpu['model_type'].cpu().numpy())\n",
    "        test_predictions['length'].extend(batch['length'].cpu().numpy())  # Keep on CPU\n",
    "\n",
    "# Convert to numpy arrays\n",
    "for key in test_predictions:\n",
    "    test_predictions[key] = np.array(test_predictions[key]).flatten()\n",
    "\n",
    "print(\"âœ“ Test set evaluation complete\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# COMPUTE OVERALL TEST METRICS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"OVERALL TEST SET METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score\n",
    "\n",
    "test_metrics = {\n",
    "    'D_MAE': mean_absolute_error(test_predictions['D_true'], test_predictions['D_pred']),\n",
    "    'D_MSE': mean_squared_error(test_predictions['D_true'], test_predictions['D_pred']),\n",
    "    'D_R2': r2_score(test_predictions['D_true'], test_predictions['D_pred']),\n",
    "    'alpha_MAE': mean_absolute_error(test_predictions['alpha_true'], test_predictions['alpha_pred']),\n",
    "    'alpha_MSE': mean_squared_error(test_predictions['alpha_true'], test_predictions['alpha_pred']),\n",
    "    'alpha_R2': r2_score(test_predictions['alpha_true'], test_predictions['alpha_pred']),\n",
    "}\n",
    "\n",
    "# Classification metrics\n",
    "test_metrics['accuracy'] = (test_predictions['model_pred'] == test_predictions['model_true']).mean()\n",
    "test_metrics['f1_weighted'] = f1_score(test_predictions['model_true'], test_predictions['model_pred'], average='weighted')\n",
    "\n",
    "print(\"Regression Metrics:\")\n",
    "print(f\"  D:     MAE={test_metrics['D_MAE']:.4e}, MSE={test_metrics['D_MSE']:.4e}, RÂ²={test_metrics['D_R2']:.4f}\")\n",
    "print(f\"  Î±:     MAE={test_metrics['alpha_MAE']:.4f}, MSE={test_metrics['alpha_MSE']:.4f}, RÂ²={test_metrics['alpha_R2']:.4f}\")\n",
    "print()\n",
    "print(\"Classification Metrics:\")\n",
    "print(f\"  Accuracy: {test_metrics['accuracy']*100:.2f}%\")\n",
    "print(f\"  F1-score (weighted): {test_metrics['f1_weighted']:.4f}\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD TEST DATASET INFO FOR SNR CALCULATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Loading test dataset metadata for SNR analysis...\")\n",
    "\n",
    "# SNR is already loaded in test_data during Cell 4\n",
    "# Extract it from test_data if available\n",
    "if 'snr' in test_data and test_data['snr'] is not None:\n",
    "    # SNR was loaded during data loading\n",
    "    test_predictions['snr'] = test_data['snr']\n",
    "    print(f\"âœ“ SNR loaded from test_data: {len(test_predictions['snr'])} samples\")\n",
    "elif 'snr' in test_predictions:\n",
    "    # Already in test_predictions from batch loading\n",
    "    print(f\"âœ“ SNR already in test_predictions: {len(test_predictions['snr'])} samples\")\n",
    "else:\n",
    "    # Fallback: Load from HDF5 if not already loaded\n",
    "    with h5py.File(CONFIG['dataset_path'], 'r') as f:\n",
    "        if 'snr' in f['test']:\n",
    "            test_predictions['snr'] = f['test']['snr'][:]\n",
    "            print(f\"âœ“ SNR loaded from HDF5: {len(test_predictions['snr'])} samples\")\n",
    "        else:\n",
    "            raise ValueError(\"SNR not found in test dataset! Please regenerate dataset with SNR metadata.\")\n",
    "\n",
    "print(f\"  SNR values: {np.unique(test_predictions['snr'])}\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# DEFINE SNR BINS (ANDI TABLE 2 SPECIFICATION)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Defining SNR bins (ANDI Table 2: SNR âˆˆ [1, 2])...\")\n",
    "\n",
    "# Define SNR bins based on ANDI Table 2 specification\n",
    "snr_bins = {\n",
    "    1: 'Low SNR (SNR=1)',\n",
    "    2: 'High SNR (SNR=2)'\n",
    "}\n",
    "\n",
    "# SNR values from dataset (ANDI Table 2)\n",
    "snr_values = [1, 2]\n",
    "\n",
    "print(f\"âœ“ SNR bins defined: {snr_bins}\")\n",
    "print(f\"âœ“ SNR values to analyze: {snr_values}\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# MAE VS TRAJECTORY LENGTH BY SNR AND MODEL TYPE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENERATING MAE VS TRAJECTORY LENGTH PLOTS\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Set style\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\")\n",
    "\n",
    "# Define trajectory length bins\n",
    "length_bins = np.arange(0, 1001, 50)  # 0-50, 50-100, ..., 950-1000\n",
    "length_labels = [(length_bins[i] + length_bins[i+1]) // 2 for i in range(len(length_bins)-1)]\n",
    "\n",
    "# Model names\n",
    "model_names = ['ATTM', 'CTRW', 'FBM', 'LW', 'SBM']\n",
    "model_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "# SNR values to analyze (ANDI Table 2)\n",
    "snr_values = [1, 2]\n",
    "\n",
    "# Function to compute binned MAE\n",
    "def compute_binned_mae(true_vals, pred_vals, lengths, model_types, snr_value, target_model=None):\n",
    "    \"\"\"Compute MAE for each trajectory length bin\"\"\"\n",
    "    mae_by_length = []\n",
    "\n",
    "    # Filter by SNR value\n",
    "    snr_mask = test_predictions['snr'] == snr_value\n",
    "\n",
    "    # Filter by model type if specified\n",
    "    if target_model is not None:\n",
    "        model_mask = model_types == target_model\n",
    "        mask = snr_mask & model_mask\n",
    "    else:\n",
    "        mask = snr_mask\n",
    "\n",
    "    for i in range(len(length_bins) - 1):\n",
    "        length_mask = (lengths >= length_bins[i]) & (lengths < length_bins[i+1])\n",
    "        combined_mask = mask & length_mask\n",
    "\n",
    "        if np.sum(combined_mask) > 0:\n",
    "            mae = mean_absolute_error(true_vals[combined_mask], pred_vals[combined_mask])\n",
    "            mae_by_length.append(mae)\n",
    "        else:\n",
    "            mae_by_length.append(np.nan)\n",
    "\n",
    "    return mae_by_length\n",
    "\n",
    "# =============================================================================\n",
    "# FIGURE 1: ALPHA MAE VS TRAJECTORY LENGTH\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Alpha (Î±) MAE vs Trajectory Length by SNR and Model Type',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "for idx, snr in enumerate(snr_values):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Plot each model type (ONLY these 5 lines, NO overall line)\n",
    "    for model_idx, model_name in enumerate(model_names):\n",
    "        mae_values = compute_binned_mae(\n",
    "            test_predictions['alpha_true'],\n",
    "            test_predictions['alpha_pred'],\n",
    "            test_predictions['length'],\n",
    "            test_predictions['model_true'],\n",
    "            snr,\n",
    "            target_model=model_idx\n",
    "        )\n",
    "\n",
    "        # Filter out NaN values to keep line connected\n",
    "        valid_mask = ~np.isnan(mae_values)\n",
    "        if np.any(valid_mask):\n",
    "            ax.plot(np.array(length_labels)[valid_mask], np.array(mae_values)[valid_mask],\n",
    "                   marker='o', linewidth=2, label=model_name,\n",
    "                   color=model_colors[model_idx], markersize=6)\n",
    "\n",
    "    ax.set_xlabel('Trajectory Length', fontweight='bold', fontsize=12)\n",
    "    ax.set_ylabel('MAE', fontweight='bold', fontsize=12)\n",
    "    ax.set_title(f'{snr_bins[snr]}', fontweight='bold', fontsize=14)\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([0, 1000])\n",
    "\n",
    "plt.tight_layout()\n",
    "alpha_length_path = os.path.join(CONFIG['output_dir'], 'test_alpha_mae_vs_length_by_snr.png')\n",
    "plt.savefig(alpha_length_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"âœ“ Alpha MAE vs length plot saved: {alpha_length_path}\")\n",
    "\n",
    "# Save plot details to text file\n",
    "alpha_length_txt_path = os.path.join(CONFIG['output_dir'], 'test_alpha_mae_vs_length_by_snr_details.txt')\n",
    "with open(alpha_length_txt_path, 'w') as f:\n",
    "    f.write(\"Alpha (Î±) MAE vs Trajectory Length by SNR and Model Type\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    f.write(f\"Total test samples: {len(test_predictions['alpha_true']):,}\\n\\n\")\n",
    "    \n",
    "    for snr in snr_values:\n",
    "        f.write(f\"{snr_bins[snr]}\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        snr_mask = test_predictions['snr'] == snr\n",
    "        n_samples_snr = np.sum(snr_mask)\n",
    "        f.write(f\"Total samples: {n_samples_snr:,}\\n\\n\")\n",
    "        \n",
    "        f.write(\"MAE by Trajectory Length Bin and Model Type:\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        f.write(f\"{'Length Bin':<15} {'ATTM':<12} {'CTRW':<12} {'FBM':<12} {'LW':<12} {'SBM':<12}\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        \n",
    "        for i in range(len(length_bins) - 1):\n",
    "            length_label = f\"{length_bins[i]}-{length_bins[i+1]}\"\n",
    "            row_data = [length_label]\n",
    "            \n",
    "            for model_idx in range(len(model_names)):\n",
    "                mae_values = compute_binned_mae(\n",
    "                    test_predictions['alpha_true'],\n",
    "                    test_predictions['alpha_pred'],\n",
    "                    test_predictions['length'],\n",
    "                    test_predictions['model_true'],\n",
    "                    snr,\n",
    "                    target_model=model_idx\n",
    "                )\n",
    "                mae_val = mae_values[i]\n",
    "                if not np.isnan(mae_val):\n",
    "                    row_data.append(f\"{mae_val:.6f}\")\n",
    "                else:\n",
    "                    row_data.append(\"N/A\")\n",
    "            \n",
    "            f.write(f\"{row_data[0]:<15} {row_data[1]:<12} {row_data[2]:<12} {row_data[3]:<12} {row_data[4]:<12} {row_data[5]:<12}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "        # Overall statistics by model type\n",
    "        f.write(\"Overall MAE Statistics by Model Type:\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        for model_idx, model_name in enumerate(model_names):\n",
    "            model_mask = test_predictions['model_true'] == model_idx\n",
    "            combined_mask = snr_mask & model_mask\n",
    "            if np.sum(combined_mask) > 0:\n",
    "                mae_overall = mean_absolute_error(\n",
    "                    test_predictions['alpha_true'][combined_mask],\n",
    "                    test_predictions['alpha_pred'][combined_mask]\n",
    "                )\n",
    "                f.write(f\"  {model_name:5s}: MAE = {mae_overall:.6f} (N = {np.sum(combined_mask):,})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"âœ“ Alpha MAE vs length plot details saved: {alpha_length_txt_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FIGURE 2: D MAE VS TRAJECTORY LENGTH\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Diffusion Coefficient (D) MAE vs Trajectory Length by SNR and Model Type',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "for idx, snr in enumerate(snr_values):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Plot each model type (ONLY these 5 lines, NO overall line)\n",
    "    for model_idx, model_name in enumerate(model_names):\n",
    "        mae_values = compute_binned_mae(\n",
    "            test_predictions['D_true'],\n",
    "            test_predictions['D_pred'],\n",
    "            test_predictions['length'],\n",
    "            test_predictions['model_true'],\n",
    "            snr,\n",
    "            target_model=model_idx\n",
    "        )\n",
    "\n",
    "        # Filter out NaN values to keep line connected\n",
    "        valid_mask = ~np.isnan(mae_values)\n",
    "        if np.any(valid_mask):\n",
    "            ax.plot(np.array(length_labels)[valid_mask], np.array(mae_values)[valid_mask],\n",
    "                   marker='o', linewidth=2, label=model_name,\n",
    "                   color=model_colors[model_idx], markersize=6)\n",
    "\n",
    "    ax.set_xlabel('Trajectory Length', fontweight='bold', fontsize=12)\n",
    "    ax.set_ylabel('MAE', fontweight='bold', fontsize=12)\n",
    "    ax.set_title(f'{snr_bins[snr]}', fontweight='bold', fontsize=14)\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([0, 1000])\n",
    "    ax.set_yscale('log')  # Log scale for D values\n",
    "\n",
    "plt.tight_layout()\n",
    "d_length_path = os.path.join(CONFIG['output_dir'], 'test_D_mae_vs_length_by_snr.png')\n",
    "plt.savefig(d_length_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"âœ“ D MAE vs length plot saved: {d_length_path}\")\n",
    "\n",
    "# FIGURE 3 (H MAE) removed - H predictions no longer used\n",
    "\n",
    "# =============================================================================\n",
    "# PREVIEW: ALPHA REGRESSION HEATMAPS WITH RANDOM DATA (VISUAL PREVIEW)\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"PREVIEW: ALPHA REGRESSION HEATMAPS WITH RANDOM DATA\")\n",
    "print(\"=\"*70)\n",
    "print(\"Generating sample plots with random data to preview visual style...\")\n",
    "print()\n",
    "\n",
    "# Add epsilon for floating point robustness in binning\n",
    "epsilon = 1e-9 # A very small value to ensure inclusivity of max values\n",
    "\n",
    "# Model-specific alpha ranges (ANDI Table 2)\n",
    "model_alpha_ranges = {\n",
    "    0: (0.10, 1.00),   # ATTM\n",
    "    1: (0.10, 1.00),   # CTRW\n",
    "    2: (0.10, 1.90),   # FBM\n",
    "    3: (1.00, 1.90),   # LW\n",
    "    4: (0.10, 1.90),   # SBM\n",
    "}\n",
    "\n",
    "# Create white-to-red colormap\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors_white_red = ['white', '#ffcccc', '#ff9999', '#ff6666', '#ff3333', '#ff0000', '#cc0000']\n",
    "n_bins = 256\n",
    "white_red_cmap = LinearSegmentedColormap.from_list('white_red', colors_white_red, N=n_bins)\n",
    "\n",
    "# Generate random test data for preview\n",
    "np.random.seed(42)  # For reproducibility\n",
    "n_samples_per_model_snr = 500  # Samples per model-SNR combination\n",
    "\n",
    "# Create mock test_predictions structure\n",
    "preview_data = {\n",
    "    'alpha_true': [],\n",
    "    'alpha_pred': [],\n",
    "    'model_true': [],\n",
    "    'snr': []\n",
    "}\n",
    "\n",
    "model_names_preview = ['ATTM', 'CTRW', 'FBM', 'LW', 'SBM']\n",
    "snr_values_preview = [1, 2]\n",
    "snr_bins_preview = {1: 'Low SNR (SNR=1)', 2: 'High SNR (SNR=2)'}\n",
    "\n",
    "for model_idx, model_name in enumerate(model_names_preview):\n",
    "    alpha_min, alpha_max = model_alpha_ranges[model_idx]\n",
    "\n",
    "    for snr in snr_values_preview:\n",
    "        # Generate true alpha values within model range\n",
    "        alpha_true_samples = np.random.uniform(alpha_min, alpha_max, n_samples_per_model_snr)\n",
    "\n",
    "        # Generate predicted alpha with some correlation + noise\n",
    "        # Add correlation: predicted = true + small random error\n",
    "        noise_std = 0.15 if snr == 1 else 0.08  # Higher noise for lower SNR\n",
    "        alpha_pred_samples = alpha_true_samples + np.random.normal(0, noise_std, n_samples_per_model_snr)\n",
    "\n",
    "        # Clip predictions to reasonable range\n",
    "        alpha_pred_samples = np.clip(alpha_pred_samples, 0.05, 2.0)\n",
    "\n",
    "        preview_data['alpha_true'].extend(alpha_true_samples)\n",
    "        preview_data['alpha_pred'].extend(alpha_pred_samples)\n",
    "        preview_data['model_true'].extend([model_idx] * n_samples_per_model_snr)\n",
    "        preview_data['snr'].extend([snr] * n_samples_per_model_snr)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "for key in preview_data:\n",
    "    preview_data[key] = np.array(preview_data[key])\n",
    "\n",
    "# Generate preview plots\n",
    "for model_idx, model_name in enumerate(model_names_preview):\n",
    "    print(f\"Generating preview heatmap for {model_name}...\")\n",
    "\n",
    "    # Get model-specific alpha range\n",
    "    alpha_min, alpha_max = model_alpha_ranges[model_idx]\n",
    "\n",
    "    # Define alpha bins for this model with 0.2 step\n",
    "    alpha_bins_model = np.arange(alpha_min, alpha_max + epsilon, 0.2)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6), facecolor='white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    fig.suptitle(f'PREVIEW: Alpha Regression Heatmap: {model_name}\\n(Predicted vs True Alpha) - Random Data',\n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "    for snr_idx, snr in enumerate(snr_values_preview):\n",
    "        ax = axes[snr_idx]\n",
    "        ax.set_facecolor('white')\n",
    "\n",
    "        # Filter data for this model and SNR\n",
    "        model_mask = preview_data['model_true'] == model_idx\n",
    "        snr_mask = preview_data['snr'] == snr\n",
    "        combined_mask = model_mask & snr_mask\n",
    "\n",
    "        if np.sum(combined_mask) == 0:\n",
    "            ax.text(0.5, 0.5, f'No data for {model_name} at SNR={snr}',\n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "            ax.set_xlabel('True Alpha (Î±)', fontweight='bold', fontsize=12)\n",
    "            ax.set_ylabel('Predicted Alpha (Î±)', fontweight='bold', fontsize=12)\n",
    "            ax.set_title(f'{snr_bins_preview[snr]}', fontweight='bold', fontsize=14)\n",
    "            continue\n",
    "\n",
    "        alpha_true_filtered = preview_data['alpha_true'][combined_mask]\n",
    "        alpha_pred_filtered = preview_data['alpha_pred'][combined_mask]\n",
    "\n",
    "        # Create 2D histogram (heatmap) using model-specific bins\n",
    "        H, xedges, yedges = np.histogram2d(\n",
    "            alpha_true_filtered,\n",
    "            alpha_pred_filtered,\n",
    "            bins=[alpha_bins_model, alpha_bins_model]\n",
    "        )\n",
    "\n",
    "        # Transpose to have true on x-axis, predicted on y-axis\n",
    "        H = H.T\n",
    "\n",
    "        # Create heatmap with white-to-red colormap (no text annotations, just color)\n",
    "        # Use actual bin edges from histogram for accurate scaling (no gaps)\n",
    "        # extent: [left, right, bottom, top] where left/right = x-axis (true alpha), bottom/top = y-axis (predicted alpha)\n",
    "        im = ax.imshow(H, extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]],\n",
    "                      origin='lower', aspect='auto', cmap=white_red_cmap,\n",
    "                      interpolation='nearest', vmin=0)\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Sample Count', fontweight='bold', fontsize=10)\n",
    "\n",
    "        # Add diagonal line (predicted = true, ideal case)\n",
    "        ax.plot([alpha_bins_model[0], alpha_bins_model[-1]],\n",
    "               [alpha_bins_model[0], alpha_bins_model[-1]],\n",
    "               'k--', linewidth=2, label='Ideal (Predicted = True)', alpha=0.8)\n",
    "\n",
    "        # Add average prediction line (binned by true alpha in 0.2 steps)\n",
    "        if len(alpha_true_filtered) > 1:\n",
    "            # Bin true alpha values in 0.2 steps\n",
    "            alpha_bin_edges = np.arange(alpha_min, alpha_max + epsilon, 0.2)\n",
    "            alpha_bin_centers = (alpha_bin_edges[:-1] + alpha_bin_edges[1:]) / 2\n",
    "\n",
    "            # Compute mean predicted alpha for each true alpha bin\n",
    "            mean_pred_per_bin = []\n",
    "            valid_bin_centers = []\n",
    "\n",
    "            for i in range(len(alpha_bin_edges) - 1):\n",
    "                bin_mask = (alpha_true_filtered >= alpha_bin_edges[i]) & \\\n",
    "                          (alpha_true_filtered < alpha_bin_edges[i+1])\n",
    "\n",
    "                if np.sum(bin_mask) > 0:\n",
    "                    mean_pred = np.mean(alpha_pred_filtered[bin_mask])\n",
    "                    mean_pred_per_bin.append(mean_pred)\n",
    "                    valid_bin_centers.append(alpha_bin_centers[i])\n",
    "\n",
    "            # Plot connected line through all averages\n",
    "            if len(mean_pred_per_bin) > 0:\n",
    "                ax.plot(valid_bin_centers, mean_pred_per_bin, 'b-', linewidth=2,\n",
    "                       marker='o', markersize=4, label='Average (binned)', alpha=0.8)\n",
    "\n",
    "        # Formatting\n",
    "        ax.set_xlabel('True Alpha (Î±)', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('Predicted Alpha (Î±)', fontweight='bold', fontsize=12)\n",
    "        ax.set_title(f'{snr_bins_preview[snr]} (N={np.sum(combined_mask):,} samples)',\n",
    "                    fontweight='bold', fontsize=14)\n",
    "        ax.legend(loc='upper left', fontsize=9, framealpha=0.9, facecolor='white')\n",
    "        ax.grid(True, alpha=0.3, linestyle=':')\n",
    "        ax.set_xlim([alpha_bins_model[0], alpha_bins_model[-1]])\n",
    "        ax.set_ylim([alpha_bins_model[0], alpha_bins_model[-1]])\n",
    "        ax.set_facecolor('white')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    preview_path = os.path.join(CONFIG['output_dir'], f'PREVIEW_alpha_heatmap_{model_name}.png')\n",
    "    plt.savefig(preview_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "    print(f\"  âœ“ Preview saved: {preview_path}\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"PREVIEW COMPLETE - Check the generated preview plots above\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# FIGURES 4-8: ALPHA REGRESSION HEATMAPS BY MODEL TYPE\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"GENERATING ALPHA REGRESSION HEATMAPS BY MODEL TYPE\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Model-specific alpha ranges (ANDI Table 2)\n",
    "model_alpha_ranges = {\n",
    "    0: (0.10, 1.00),   # ATTM\n",
    "    1: (0.10, 1.00),   # CTRW\n",
    "    2: (0.10, 1.90),   # FBM\n",
    "    3: (1.00, 1.90),   # LW\n",
    "    4: (0.10, 1.90),   # SBM\n",
    "}\n",
    "\n",
    "# Create white-to-red colormap\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors_white_red = ['white', '#ffcccc', '#ff9999', '#ff6666', '#ff3333', '#ff0000', '#cc0000']\n",
    "n_bins = 256\n",
    "white_red_cmap = LinearSegmentedColormap.from_list('white_red', colors_white_red, N=n_bins)\n",
    "\n",
    "# Generate one figure per model type\n",
    "for model_idx, model_name in enumerate(model_names):\n",
    "    print(f\"Generating heatmap for {model_name}...\")\n",
    "\n",
    "    # Get model-specific alpha range\n",
    "    alpha_min, alpha_max = model_alpha_ranges[model_idx]\n",
    "\n",
    "    # Define alpha bins for this model with 0.2 step\n",
    "    alpha_bins_model = np.arange(alpha_min, alpha_max + epsilon, 0.2)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6), facecolor='white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    fig.suptitle(f'Alpha Regression Heatmap: {model_name}\\n(Predicted vs True Alpha)',\n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "    for snr_idx, snr in enumerate(snr_values):\n",
    "        ax = axes[snr_idx]\n",
    "        ax.set_facecolor('white')\n",
    "\n",
    "        # Filter data for this model and SNR\n",
    "        model_mask = test_predictions['model_true'] == model_idx\n",
    "        snr_mask = test_predictions['snr'] == snr\n",
    "        combined_mask = model_mask & snr_mask\n",
    "\n",
    "        if np.sum(combined_mask) == 0:\n",
    "            ax.text(0.5, 0.5, f'No data for {model_name} at SNR={snr}',\n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "            ax.set_xlabel('True Alpha (Î±)', fontweight='bold', fontsize=12)\n",
    "            ax.set_ylabel('Predicted Alpha (Î±)', fontweight='bold', fontsize=12)\n",
    "            ax.set_title(f'{snr_bins[snr]}', fontweight='bold', fontsize=14)\n",
    "            continue\n",
    "\n",
    "        alpha_true_filtered = test_predictions['alpha_true'][combined_mask]\n",
    "        alpha_pred_filtered = test_predictions['alpha_pred'][combined_mask]\n",
    "\n",
    "        # Create 2D histogram (heatmap) using model-specific bins\n",
    "        H, xedges, yedges = np.histogram2d(\n",
    "            alpha_true_filtered,\n",
    "            alpha_pred_filtered,\n",
    "            bins=[alpha_bins_model, alpha_bins_model]\n",
    "        )\n",
    "\n",
    "        # Transpose to have true on x-axis, predicted on y-axis\n",
    "        H = H.T\n",
    "\n",
    "        # Create heatmap with white-to-red colormap (no text annotations, just color)\n",
    "        # Use actual bin edges from histogram for accurate scaling (no gaps)\n",
    "        # extent: [left, right, bottom, top] where left/right = x-axis (true alpha), bottom/top = y-axis (predicted alpha)\n",
    "        im = ax.imshow(H, extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]],\n",
    "                      origin='lower', aspect='auto', cmap=white_red_cmap,\n",
    "                      interpolation='nearest', vmin=0)\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Sample Count', fontweight='bold', fontsize=10)\n",
    "\n",
    "        # Add diagonal line (predicted = true, ideal case)\n",
    "        ax.plot([alpha_bins_model[0], alpha_bins_model[-1]],\n",
    "               [alpha_bins_model[0], alpha_bins_model[-1]],\n",
    "               'k--', linewidth=2, label='Ideal (Predicted = True)', alpha=0.8)\n",
    "\n",
    "        # Add average prediction line (binned by true alpha in 0.2 steps)\n",
    "        if len(alpha_true_filtered) > 1:\n",
    "            # Bin true alpha values in 0.2 steps\n",
    "            alpha_bin_edges = np.arange(alpha_min, alpha_max + epsilon, 0.2)\n",
    "            alpha_bin_centers = (alpha_bin_edges[:-1] + alpha_bin_edges[1:]) / 2\n",
    "\n",
    "            # Compute mean predicted alpha for each true alpha bin\n",
    "            mean_pred_per_bin = []\n",
    "            valid_bin_centers = []\n",
    "\n",
    "            for i in range(len(alpha_bin_edges) - 1):\n",
    "                bin_mask = (alpha_true_filtered >= alpha_bin_edges[i]) & \\\n",
    "                          (alpha_true_filtered < alpha_bin_edges[i+1])\n",
    "\n",
    "                if np.sum(bin_mask) > 0:\n",
    "                    mean_pred = np.mean(alpha_pred_filtered[bin_mask])\n",
    "                    mean_pred_per_bin.append(mean_pred)\n",
    "                    valid_bin_centers.append(alpha_bin_centers[i])\n",
    "\n",
    "            # Plot connected line through all averages\n",
    "            if len(mean_pred_per_bin) > 0:\n",
    "                ax.plot(valid_bin_centers, mean_pred_per_bin, 'b-', linewidth=2,\n",
    "                       marker='o', markersize=4, label='Average (binned)', alpha=0.8)\n",
    "\n",
    "        # Formatting\n",
    "        ax.set_xlabel('True Alpha (Î±)', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('Predicted Alpha (Î±)', fontweight='bold', fontsize=12)\n",
    "        ax.set_title(f'{snr_bins[snr]} (N={np.sum(combined_mask):,} samples)',\n",
    "                    fontweight='bold', fontsize=14)\n",
    "        ax.legend(loc='upper left', fontsize=9, framealpha=0.9, facecolor='white')\n",
    "        ax.grid(True, alpha=0.3, linestyle=':')\n",
    "        ax.set_xlim([alpha_bins_model[0], alpha_bins_model[-1]])\n",
    "        ax.set_ylim([alpha_bins_model[0], alpha_bins_model[-1]])\n",
    "        ax.set_facecolor('white')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    heatmap_path = os.path.join(CONFIG['output_dir'], f'test_alpha_heatmap_{model_name}.png')\n",
    "    plt.savefig(heatmap_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "    print(f\"  âœ“ Saved: {heatmap_path}\")\n",
    "    \n",
    "    # Save plot details to text file\n",
    "    heatmap_txt_path = os.path.join(CONFIG['output_dir'], f'test_alpha_heatmap_{model_name}_details.txt')\n",
    "    with open(heatmap_txt_path, 'w') as f:\n",
    "        f.write(f\"Alpha (Î±) Regression Heatmap: {model_name}\\n\")\n",
    "        f.write(\"=\"*70 + \"\\n\\n\")\n",
    "        f.write(f\"Model Type: {model_name}\\n\")\n",
    "        f.write(f\"Alpha Range: {alpha_min:.2f} to {alpha_max:.2f}\\n\")\n",
    "        f.write(f\"Alpha Bins: {len(alpha_bins_model)-1} bins with step 0.2\\n\\n\")\n",
    "        \n",
    "        for snr in snr_values:\n",
    "            f.write(f\"{snr_bins[snr]}\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            \n",
    "            # Filter data for this model and SNR\n",
    "            model_mask = test_predictions['model_true'] == model_idx\n",
    "            snr_mask = test_predictions['snr'] == snr\n",
    "            combined_mask = model_mask & snr_mask\n",
    "            \n",
    "            if np.sum(combined_mask) == 0:\n",
    "                f.write(\"No data available for this SNR level\\n\\n\")\n",
    "                continue\n",
    "            \n",
    "            alpha_true_filtered = test_predictions['alpha_true'][combined_mask]\n",
    "            alpha_pred_filtered = test_predictions['alpha_pred'][combined_mask]\n",
    "            \n",
    "            f.write(f\"Total samples: {len(alpha_true_filtered):,}\\n\\n\")\n",
    "            \n",
    "            # Overall statistics\n",
    "            from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "            mae = mean_absolute_error(alpha_true_filtered, alpha_pred_filtered)\n",
    "            mse = mean_squared_error(alpha_true_filtered, alpha_pred_filtered)\n",
    "            r2 = r2_score(alpha_true_filtered, alpha_pred_filtered)\n",
    "            \n",
    "            f.write(\"Overall Regression Statistics:\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            f.write(f\"  MAE: {mae:.6f}\\n\")\n",
    "            f.write(f\"  MSE: {mse:.6f}\\n\")\n",
    "            f.write(f\"  RÂ²: {r2:.6f}\\n\")\n",
    "            f.write(f\"  RMSE: {np.sqrt(mse):.6f}\\n\\n\")\n",
    "            \n",
    "            # Statistics by alpha bin\n",
    "            f.write(\"Statistics by Alpha Bin (True Alpha):\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            f.write(f\"{'Alpha Bin':<20} {'N':<10} {'Mean Pred':<15} {'MAE':<12} {'MSE':<12}\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            \n",
    "            alpha_bin_edges = np.arange(alpha_min, alpha_max + epsilon, 0.2)\n",
    "            alpha_bin_centers = (alpha_bin_edges[:-1] + alpha_bin_edges[1:]) / 2\n",
    "            \n",
    "            for i in range(len(alpha_bin_edges) - 1):\n",
    "                bin_mask = (alpha_true_filtered >= alpha_bin_edges[i]) & \\\n",
    "                          (alpha_true_filtered < alpha_bin_edges[i+1])\n",
    "                \n",
    "                if np.sum(bin_mask) > 0:\n",
    "                    bin_true = alpha_true_filtered[bin_mask]\n",
    "                    bin_pred = alpha_pred_filtered[bin_mask]\n",
    "                    bin_mae = mean_absolute_error(bin_true, bin_pred)\n",
    "                    bin_mse = mean_squared_error(bin_true, bin_pred)\n",
    "                    mean_pred = np.mean(bin_pred)\n",
    "                    \n",
    "                    alpha_label = f\"{alpha_bin_edges[i]:.2f}-{alpha_bin_edges[i+1]:.2f}\"\n",
    "                    f.write(f\"{alpha_label:<20} {np.sum(bin_mask):<10} {mean_pred:<15.6f} {bin_mae:<12.6f} {bin_mse:<12.6f}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Additional statistics\n",
    "            f.write(\"Additional Statistics:\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            f.write(f\"  True Alpha - Min: {alpha_true_filtered.min():.6f}, Max: {alpha_true_filtered.max():.6f}, Mean: {alpha_true_filtered.mean():.6f}\\n\")\n",
    "            f.write(f\"  Pred Alpha - Min: {alpha_pred_filtered.min():.6f}, Max: {alpha_pred_filtered.max():.6f}, Mean: {alpha_pred_filtered.mean():.6f}\\n\")\n",
    "            f.write(f\"  Bias (Mean Pred - Mean True): {alpha_pred_filtered.mean() - alpha_true_filtered.mean():.6f}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"  âœ“ Heatmap details saved: {heatmap_txt_path}\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"GENERATING D (DIFFUSION COEFFICIENT) REGRESSION HEATMAPS BY MODEL TYPE\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# D ranges (log scale since D spans several orders of magnitude)\n",
    "# Use log bins from 1e-4 to 100\n",
    "D_min = 1e-4\n",
    "D_max = 100.0\n",
    "n_D_bins = 50\n",
    "D_bins_log = np.logspace(np.log10(D_min), np.log10(D_max), n_D_bins + 1)\n",
    "\n",
    "# Generate one figure per model type for D\n",
    "for model_idx, model_name in enumerate(model_names):\n",
    "    print(f\"Generating D heatmap for {model_name}...\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6), facecolor='white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    fig.suptitle(f'D (Diffusion Coefficient) Regression Heatmap: {model_name}\\n(Predicted vs True D)',\n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "    for snr_idx, snr in enumerate(snr_values):\n",
    "        ax = axes[snr_idx]\n",
    "        ax.set_facecolor('white')\n",
    "\n",
    "        # Filter data for this model and SNR\n",
    "        model_mask = test_predictions['model_true'] == model_idx\n",
    "        snr_mask = test_predictions['snr'] == snr\n",
    "        combined_mask = model_mask & snr_mask\n",
    "\n",
    "        if np.sum(combined_mask) == 0:\n",
    "            ax.text(0.5, 0.5, f'No data for {model_name} at SNR={snr}',\n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "            ax.set_xlabel('True D', fontweight='bold', fontsize=12)\n",
    "            ax.set_ylabel('Predicted D', fontweight='bold', fontsize=12)\n",
    "            ax.set_title(f'{snr_bins[snr]}', fontweight='bold', fontsize=14)\n",
    "            continue\n",
    "\n",
    "        D_true_filtered = test_predictions['D_true'][combined_mask]\n",
    "        D_pred_filtered = test_predictions['D_pred'][combined_mask]\n",
    "\n",
    "        # Create 2D histogram (heatmap) using log bins\n",
    "        H, xedges, yedges = np.histogram2d(\n",
    "            D_true_filtered,\n",
    "            D_pred_filtered,\n",
    "            bins=[D_bins_log, D_bins_log]\n",
    "        )\n",
    "\n",
    "        # Transpose to have true on x-axis, predicted on y-axis\n",
    "        H = H.T\n",
    "\n",
    "        # Create heatmap with white-to-red colormap\n",
    "        im = ax.imshow(H, extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]],\n",
    "                      origin='lower', aspect='auto', cmap=white_red_cmap,\n",
    "                      interpolation='nearest', vmin=0)\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Sample Count', fontweight='bold', fontsize=10)\n",
    "\n",
    "        # Add diagonal line (predicted = true, ideal case)\n",
    "        ax.plot([D_min, D_max], [D_min, D_max],\n",
    "               'k--', linewidth=2, label='Ideal (Predicted = True)', alpha=0.8)\n",
    "\n",
    "        # Add average prediction line (binned by true D in log steps)\n",
    "        if len(D_true_filtered) > 1:\n",
    "            # Bin true D values in log steps\n",
    "            n_bins_avg = 20\n",
    "            D_bin_edges_avg = np.logspace(np.log10(D_min), np.log10(D_max), n_bins_avg + 1)\n",
    "            D_bin_centers_avg = np.sqrt(D_bin_edges_avg[:-1] * D_bin_edges_avg[1:])  # Geometric mean\n",
    "\n",
    "            # Compute mean predicted D for each true D bin\n",
    "            mean_pred_per_bin = []\n",
    "            valid_bin_centers = []\n",
    "\n",
    "            for i in range(len(D_bin_edges_avg) - 1):\n",
    "                bin_mask = (D_true_filtered >= D_bin_edges_avg[i]) & \\\n",
    "                          (D_true_filtered < D_bin_edges_avg[i+1])\n",
    "\n",
    "                if np.sum(bin_mask) > 0:\n",
    "                    mean_pred = np.mean(D_pred_filtered[bin_mask])\n",
    "                    mean_pred_per_bin.append(mean_pred)\n",
    "                    valid_bin_centers.append(D_bin_centers_avg[i])\n",
    "\n",
    "            # Plot connected line through all averages\n",
    "            if len(mean_pred_per_bin) > 0:\n",
    "                ax.plot(valid_bin_centers, mean_pred_per_bin, 'b-', linewidth=2,\n",
    "                       marker='o', markersize=4, label='Average (binned)', alpha=0.8)\n",
    "\n",
    "        # Formatting\n",
    "        ax.set_xlabel('True D (Diffusion Coefficient)', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('Predicted D (Diffusion Coefficient)', fontweight='bold', fontsize=12)\n",
    "        ax.set_title(f'{snr_bins[snr]} (N={np.sum(combined_mask):,} samples)',\n",
    "                    fontweight='bold', fontsize=14)\n",
    "        ax.legend(loc='upper left', fontsize=9, framealpha=0.9, facecolor='white')\n",
    "        ax.grid(True, alpha=0.3, linestyle=':')\n",
    "        ax.set_xlim([D_min, D_max])\n",
    "        ax.set_ylim([D_min, D_max])\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_facecolor('white')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    heatmap_path = os.path.join(CONFIG['output_dir'], f'test_D_heatmap_{model_name}.png')\n",
    "    plt.savefig(heatmap_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "    print(f\"  âœ“ Saved: {heatmap_path}\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"ALPHA AND D HEATMAP GENERATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 471,
     "status": "ok",
     "timestamp": 1763316481715,
     "user": {
      "displayName": "Enow George",
      "userId": "04500551558329927866"
     },
     "user_tz": 0
    },
    "id": "bU0Y1ywPyghD",
    "outputId": "dcf3728a-9b90-492c-f0a3-62e0ec9d5e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHECKING DATA DISTRIBUTION\n",
      "======================================================================\n",
      "\n",
      "Total test samples: 498576\n",
      "\n",
      "SNR DISTRIBUTION:\n",
      "----------------------------------------------------------------------\n",
      "  SNR=1.0: 249288 samples (50.00%)\n",
      "  SNR=2.0: 249288 samples (50.00%)\n",
      "\n",
      "MODEL TYPE DISTRIBUTION (Overall):\n",
      "----------------------------------------------------------------------\n",
      "  ATTM: 73320 samples (14.71%)\n",
      "  CTRW: 73320 samples (14.71%)\n",
      "  FBM: 139308 samples (27.94%)\n",
      "  LW: 73320 samples (14.71%)\n",
      "  SBM: 139308 samples (27.94%)\n",
      "\n",
      "MODEL TYPE DISTRIBUTION BY SNR:\n",
      "======================================================================\n",
      "\n",
      "SNR=1 (Total: 249288 samples):\n",
      "----------------------------------------------------------------------\n",
      "  ATTM: 36660 samples (14.71% of this SNR level)\n",
      "    â†’ Length range: [9, 999]\n",
      "    â†’ Length bins with data: 9 out of 20\n",
      "  CTRW: 36660 samples (14.71% of this SNR level)\n",
      "    â†’ Length range: [9, 999]\n",
      "    â†’ Length bins with data: 9 out of 20\n",
      "  FBM: 69654 samples (27.94% of this SNR level)\n",
      "    â†’ Length range: [9, 999]\n",
      "    â†’ Length bins with data: 9 out of 20\n",
      "  LW: 36660 samples (14.71% of this SNR level)\n",
      "    â†’ Length range: [9, 999]\n",
      "    â†’ Length bins with data: 9 out of 20\n",
      "  SBM: 69654 samples (27.94% of this SNR level)\n",
      "    â†’ Length range: [9, 999]\n",
      "    â†’ Length bins with data: 9 out of 20\n",
      "\n",
      "SNR=2 (Total: 249288 samples):\n",
      "----------------------------------------------------------------------\n",
      "  ATTM: 36660 samples (14.71% of this SNR level)\n",
      "    â†’ Length range: [9, 999]\n",
      "    â†’ Length bins with data: 9 out of 20\n",
      "  CTRW: 36660 samples (14.71% of this SNR level)\n",
      "    â†’ Length range: [9, 999]\n",
      "    â†’ Length bins with data: 9 out of 20\n",
      "  FBM: 69654 samples (27.94% of this SNR level)\n",
      "    â†’ Length range: [9, 999]\n",
      "    â†’ Length bins with data: 9 out of 20\n",
      "  LW: 36660 samples (14.71% of this SNR level)\n",
      "    â†’ Length range: [9, 999]\n",
      "    â†’ Length bins with data: 9 out of 20\n",
      "  SBM: 69654 samples (27.94% of this SNR level)\n",
      "    â†’ Length range: [9, 999]\n",
      "    â†’ Length bins with data: 9 out of 20\n",
      "\n",
      "======================================================================\n",
      "\n",
      "TRAJECTORY LENGTH DISTRIBUTION:\n",
      "----------------------------------------------------------------------\n",
      "  Min length: 9\n",
      "  Max length: 999\n",
      "  Mean length: 310.54\n",
      "  Median length: 199\n",
      "\n",
      "TRAJECTORY LENGTH BINS DISTRIBUTION:\n",
      "----------------------------------------------------------------------\n",
      "  [   0,   50): 191760 samples\n",
      "  [  50,  100): 38352 samples\n",
      "  [ 100,  150): 0 samples\n",
      "  [ 150,  200): 38352 samples\n",
      "  [ 200,  250): 0 samples\n",
      "  [ 250,  300): 38352 samples\n",
      "  [ 300,  350): 0 samples\n",
      "  [ 350,  400): 38352 samples\n",
      "  [ 400,  450): 0 samples\n",
      "  [ 450,  500): 38352 samples\n",
      "  [ 500,  550): 0 samples\n",
      "  [ 550,  600): 38352 samples\n",
      "  [ 600,  650): 0 samples\n",
      "  [ 650,  700): 0 samples\n",
      "  [ 700,  750): 0 samples\n",
      "  [ 750,  800): 38352 samples\n",
      "  [ 800,  850): 0 samples\n",
      "  [ 850,  900): 0 samples\n",
      "  [ 900,  950): 0 samples\n",
      "  [ 950, 1000): 38352 samples\n",
      "\n",
      "======================================================================\n",
      "DISTRIBUTION CHECK COMPLETE\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "COMPUTING METRICS\n",
      "======================================================================\n",
      "\n",
      "SET 1: OVERALL METRICS (All Trajectory Lengths)\n",
      "======================================================================\n",
      "Number of samples: 498,576\n",
      "\n",
      "REGRESSION METRICS:\n",
      "----------------------------------------------------------------------\n",
      "  Alpha - MAE: 0.274885, RÂ²: 0.503790\n",
      "  D     - MAE: 2.900662e+00, RÂ²: 0.440321\n",
      "\n",
      "CLASSIFICATION METRICS:\n",
      "----------------------------------------------------------------------\n",
      "  Overall Model Accuracy: 0.684357\n",
      "  Overall Model F1 (weighted): 0.659913\n",
      "\n",
      "PER-MODEL METRICS:\n",
      "----------------------------------------------------------------------\n",
      "  ATTM  - Accuracy: 0.877676, F1: 0.352995\n",
      "  CTRW  - Accuracy: 0.915796, F1: 0.670574\n",
      "  FBM   - Accuracy: 0.861600, F1: 0.799650\n",
      "  LW    - Accuracy: 0.934814, F1: 0.774111\n",
      "  SBM   - Accuracy: 0.778828, F1: 0.615996\n",
      "\n",
      "\n",
      "SET 2: METRICS FOR TRAJECTORY LENGTH 0-50\n",
      "======================================================================\n",
      "Number of samples: 191,760\n",
      "\n",
      "REGRESSION METRICS:\n",
      "----------------------------------------------------------------------\n",
      "  Alpha - MAE: 0.334442, RÂ²: 0.318231\n",
      "  D     - MAE: 1.622296e+00, RÂ²: 0.563811\n",
      "\n",
      "CLASSIFICATION METRICS:\n",
      "----------------------------------------------------------------------\n",
      "  Overall Model Accuracy: 0.645145\n",
      "  Overall Model F1 (weighted): 0.623899\n",
      "\n",
      "PER-MODEL METRICS:\n",
      "----------------------------------------------------------------------\n",
      "  ATTM  - Accuracy: 0.876747, F1: 0.347081\n",
      "  CTRW  - Accuracy: 0.911426, F1: 0.617877\n",
      "  FBM   - Accuracy: 0.775438, F1: 0.709641\n",
      "  LW    - Accuracy: 0.944493, F1: 0.785585\n",
      "  SBM   - Accuracy: 0.782186, F1: 0.601921\n",
      "\n",
      "======================================================================\n",
      "âœ“ Metrics saved to: /content/drive/MyDrive/ERP_Shrey/outputs3/test_metrics_summary.txt\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CHECK DATA DISTRIBUTION BEFORE PLOTTING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHECKING DATA DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Check if test_predictions exists (created in Cell 25)\n",
    "try:\n",
    "    _ = test_predictions['D_true']\n",
    "except NameError:\n",
    "    raise NameError(\n",
    "        \"test_predictions is not defined. Please run Cell 25 first to load the model \"\n",
    "        \"and run inference on the test set. Cell 25 creates test_predictions by:\\n\"\n",
    "        \"1. Loading the best model checkpoint\\n\"\n",
    "        \"2. Running inference on the test set\\n\"\n",
    "        \"3. Creating the test_predictions dictionary\"\n",
    "    )\n",
    "\n",
    "# Check total samples\n",
    "print(f\"Total test samples: {len(test_predictions['D_true'])}\")\n",
    "print()\n",
    "\n",
    "# Check SNR distribution\n",
    "print(\"SNR DISTRIBUTION:\")\n",
    "print(\"-\" * 70)\n",
    "if 'snr' in test_predictions:\n",
    "    unique_snr, counts_snr = np.unique(test_predictions['snr'], return_counts=True)\n",
    "    for snr, count in zip(unique_snr, counts_snr):\n",
    "        print(f\"  SNR={snr}: {count} samples ({count/len(test_predictions['D_true'])*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"  âš  Warning: SNR not found in test_predictions\")\n",
    "print()\n",
    "\n",
    "# Check model type distribution\n",
    "print(\"MODEL TYPE DISTRIBUTION (Overall):\")\n",
    "print(\"-\" * 70)\n",
    "model_names_check = ['ATTM', 'CTRW', 'FBM', 'LW', 'SBM']\n",
    "unique_models, counts_models = np.unique(test_predictions['model_true'], return_counts=True)\n",
    "for model_idx, count in zip(unique_models, counts_models):\n",
    "    print(f\"  {model_names_check[model_idx]}: {count} samples ({count/len(test_predictions['D_true'])*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Check model type distribution BY SNR (THIS IS KEY!)\n",
    "print(\"MODEL TYPE DISTRIBUTION BY SNR:\")\n",
    "print(\"=\" * 70)\n",
    "if 'snr' in test_predictions:\n",
    "    for snr in [1, 2]:  # ANDI Table 2 SNR values\n",
    "        snr_mask = test_predictions['snr'] == snr\n",
    "        n_samples_snr = np.sum(snr_mask)\n",
    "        print(f\"\\nSNR={snr} (Total: {n_samples_snr} samples):\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        for model_idx, model_name in enumerate(model_names_check):\n",
    "            model_mask = test_predictions['model_true'] == model_idx\n",
    "            combined_mask = snr_mask & model_mask\n",
    "            count = np.sum(combined_mask)\n",
    "\n",
    "            if n_samples_snr > 0:\n",
    "                pct = (count / n_samples_snr) * 100\n",
    "            else:\n",
    "                pct = 0\n",
    "\n",
    "            print(f\"  {model_name}: {count} samples ({pct:.2f}% of this SNR level)\")\n",
    "\n",
    "            # Check if there's data across different trajectory lengths\n",
    "            if count > 0:\n",
    "                lengths_this_combo = test_predictions['length'][combined_mask]\n",
    "                print(f\"    â†’ Length range: [{lengths_this_combo.min():.0f}, {lengths_this_combo.max():.0f}]\")\n",
    "                print(f\"    â†’ Length bins with data: {len(np.unique(np.digitize(lengths_this_combo, length_bins)))} out of {len(length_bins)-1}\")\n",
    "else:\n",
    "    print(\"  âš  Warning: SNR not found in test_predictions\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check trajectory length distribution\n",
    "print(\"\\nTRAJECTORY LENGTH DISTRIBUTION:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  Min length: {test_predictions['length'].min():.0f}\")\n",
    "print(f\"  Max length: {test_predictions['length'].max():.0f}\")\n",
    "print(f\"  Mean length: {test_predictions['length'].mean():.2f}\")\n",
    "print(f\"  Median length: {np.median(test_predictions['length']):.0f}\")\n",
    "print()\n",
    "\n",
    "# Check if length bins are populated\n",
    "print(\"TRAJECTORY LENGTH BINS DISTRIBUTION:\")\n",
    "print(\"-\" * 70)\n",
    "length_bins_check = np.arange(0, 1001, 50)\n",
    "digitized = np.digitize(test_predictions['length'], length_bins_check)\n",
    "for i in range(len(length_bins_check) - 1):\n",
    "    count = np.sum(digitized == i+1)\n",
    "    print(f\"  [{length_bins_check[i]:4.0f}, {length_bins_check[i+1]:4.0f}): {count} samples\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"DISTRIBUTION CHECK COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# COMPUTE AND SAVE METRICS (OVERALL AND TRAJECTORY LENGTH 0-50)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPUTING METRICS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, f1_score, accuracy_score\n",
    "\n",
    "# Model names\n",
    "model_names_metrics = ['ATTM', 'CTRW', 'FBM', 'LW', 'SBM']\n",
    "\n",
    "# Function to compute all metrics for a given mask\n",
    "def compute_metrics_set(mask, set_name):\n",
    "    \"\"\"Compute all metrics for a given data subset\"\"\"\n",
    "    if np.sum(mask) == 0:\n",
    "        return None\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # Regression metrics (MAE and RÂ² - using RÂ² as \"F1\" equivalent for regression)\n",
    "    metrics['alpha_MAE'] = mean_absolute_error(\n",
    "        test_predictions['alpha_true'][mask],\n",
    "        test_predictions['alpha_pred'][mask]\n",
    "    )\n",
    "    metrics['alpha_R2'] = r2_score(\n",
    "        test_predictions['alpha_true'][mask],\n",
    "        test_predictions['alpha_pred'][mask]\n",
    "    )\n",
    "\n",
    "    metrics['D_MAE'] = mean_absolute_error(\n",
    "        test_predictions['D_true'][mask],\n",
    "        test_predictions['D_pred'][mask]\n",
    "    )\n",
    "    metrics['D_R2'] = r2_score(\n",
    "        test_predictions['D_true'][mask],\n",
    "        test_predictions['D_pred'][mask]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Classification metrics\n",
    "    model_true_masked = test_predictions['model_true'][mask]\n",
    "    model_pred_masked = test_predictions['model_pred'][mask]\n",
    "\n",
    "    # Overall model accuracy\n",
    "    metrics['overall_model_accuracy'] = accuracy_score(model_true_masked, model_pred_masked)\n",
    "\n",
    "    # Per-model accuracy and F1\n",
    "    metrics['per_model_accuracy'] = {}\n",
    "    metrics['per_model_f1'] = {}\n",
    "\n",
    "    for model_idx, model_name in enumerate(model_names_metrics):\n",
    "        # Accuracy for this model (binary: is this model or not)\n",
    "        model_binary_true = (model_true_masked == model_idx).astype(int)\n",
    "        model_binary_pred = (model_pred_masked == model_idx).astype(int)\n",
    "        metrics['per_model_accuracy'][model_name] = accuracy_score(model_binary_true, model_binary_pred)\n",
    "\n",
    "        # F1 score for this model\n",
    "        if len(np.unique(model_binary_true)) > 1:  # Need both classes\n",
    "            metrics['per_model_f1'][model_name] = f1_score(model_binary_true, model_binary_pred, zero_division=0)\n",
    "        else:\n",
    "            metrics['per_model_f1'][model_name] = 0.0\n",
    "\n",
    "    # Overall F1 (weighted average across all classes)\n",
    "    metrics['overall_model_f1'] = f1_score(model_true_masked, model_pred_masked, average='weighted', zero_division=0)\n",
    "\n",
    "    metrics['n_samples'] = np.sum(mask)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# SET 1: Overall metrics (all trajectory lengths)\n",
    "print(\"SET 1: OVERALL METRICS (All Trajectory Lengths)\")\n",
    "print(\"=\" * 70)\n",
    "overall_mask = np.ones(len(test_predictions['D_true']), dtype=bool)\n",
    "metrics_overall = compute_metrics_set(overall_mask, \"Overall\")\n",
    "\n",
    "if metrics_overall:\n",
    "    print(f\"Number of samples: {metrics_overall['n_samples']:,}\")\n",
    "    print()\n",
    "    print(\"REGRESSION METRICS:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"  Alpha - MAE: {metrics_overall['alpha_MAE']:.6f}, RÂ²: {metrics_overall['alpha_R2']:.6f}\")\n",
    "    print(f\"  D     - MAE: {metrics_overall['D_MAE']:.6e}, RÂ²: {metrics_overall['D_R2']:.6f}\")\n",
    "\n",
    "    print()\n",
    "    print(\"CLASSIFICATION METRICS:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"  Overall Model Accuracy: {metrics_overall['overall_model_accuracy']:.6f}\")\n",
    "    print(f\"  Overall Model F1 (weighted): {metrics_overall['overall_model_f1']:.6f}\")\n",
    "    print()\n",
    "    print(\"PER-MODEL METRICS:\")\n",
    "    print(\"-\" * 70)\n",
    "    for model_name in model_names_metrics:\n",
    "        print(f\"  {model_name:5s} - Accuracy: {metrics_overall['per_model_accuracy'][model_name]:.6f}, \"\n",
    "              f\"F1: {metrics_overall['per_model_f1'][model_name]:.6f}\")\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "# SET 2: Metrics for trajectory length 0-50\n",
    "print(\"SET 2: METRICS FOR TRAJECTORY LENGTH 0-50\")\n",
    "print(\"=\" * 70)\n",
    "length_mask = (test_predictions['length'] >= 0) & (test_predictions['length'] <= 50)\n",
    "metrics_length_0_50 = compute_metrics_set(length_mask, \"Length 0-50\")\n",
    "\n",
    "if metrics_length_0_50:\n",
    "    print(f\"Number of samples: {metrics_length_0_50['n_samples']:,}\")\n",
    "    print()\n",
    "    print(\"REGRESSION METRICS:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"  Alpha - MAE: {metrics_length_0_50['alpha_MAE']:.6f}, RÂ²: {metrics_length_0_50['alpha_R2']:.6f}\")\n",
    "    print(f\"  D     - MAE: {metrics_length_0_50['D_MAE']:.6e}, RÂ²: {metrics_length_0_50['D_R2']:.6f}\")\n",
    "\n",
    "    print()\n",
    "    print(\"CLASSIFICATION METRICS:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"  Overall Model Accuracy: {metrics_length_0_50['overall_model_accuracy']:.6f}\")\n",
    "    print(f\"  Overall Model F1 (weighted): {metrics_length_0_50['overall_model_f1']:.6f}\")\n",
    "    print()\n",
    "    print(\"PER-MODEL METRICS:\")\n",
    "    print(\"-\" * 70)\n",
    "    for model_name in model_names_metrics:\n",
    "        print(f\"  {model_name:5s} - Accuracy: {metrics_length_0_50['per_model_accuracy'][model_name]:.6f}, \"\n",
    "              f\"F1: {metrics_length_0_50['per_model_f1'][model_name]:.6f}\")\n",
    "else:\n",
    "    print(\"âš  No samples found in trajectory length range 0-50\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE METRICS TO TEXT FILE\n",
    "# =============================================================================\n",
    "\n",
    "metrics_file_path = os.path.join(CONFIG['output_dir'], 'test_metrics_summary.txt')\n",
    "\n",
    "with open(metrics_file_path, 'w') as f:\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    f.write(\"TEST SET METRICS SUMMARY\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    # SET 1: Overall\n",
    "    f.write(\"SET 1: OVERALL METRICS (All Trajectory Lengths)\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    if metrics_overall:\n",
    "        f.write(f\"Number of samples: {metrics_overall['n_samples']:,}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"REGRESSION METRICS:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(f\"  Alpha - MAE: {metrics_overall['alpha_MAE']:.6f}, RÂ²: {metrics_overall['alpha_R2']:.6f}\\n\")\n",
    "        f.write(f\"  D     - MAE: {metrics_overall['D_MAE']:.6e}, RÂ²: {metrics_overall['D_R2']:.6f}\\n\")\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"CLASSIFICATION METRICS:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(f\"  Overall Model Accuracy: {metrics_overall['overall_model_accuracy']:.6f}\\n\")\n",
    "        f.write(f\"  Overall Model F1 (weighted): {metrics_overall['overall_model_f1']:.6f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"PER-MODEL METRICS:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        for model_name in model_names_metrics:\n",
    "            f.write(f\"  {model_name:5s} - Accuracy: {metrics_overall['per_model_accuracy'][model_name]:.6f}, \"\n",
    "                   f\"F1: {metrics_overall['per_model_f1'][model_name]:.6f}\\n\")\n",
    "\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    # SET 2: Length 0-50\n",
    "    f.write(\"SET 2: METRICS FOR TRAJECTORY LENGTH 0-50\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    if metrics_length_0_50:\n",
    "        f.write(f\"Number of samples: {metrics_length_0_50['n_samples']:,}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"REGRESSION METRICS:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(f\"  Alpha - MAE: {metrics_length_0_50['alpha_MAE']:.6f}, RÂ²: {metrics_length_0_50['alpha_R2']:.6f}\\n\")\n",
    "        f.write(f\"  D     - MAE: {metrics_length_0_50['D_MAE']:.6e}, RÂ²: {metrics_length_0_50['D_R2']:.6f}\\n\")\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"CLASSIFICATION METRICS:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(f\"  Overall Model Accuracy: {metrics_length_0_50['overall_model_accuracy']:.6f}\\n\")\n",
    "        f.write(f\"  Overall Model F1 (weighted): {metrics_length_0_50['overall_model_f1']:.6f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"PER-MODEL METRICS:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        for model_name in model_names_metrics:\n",
    "            f.write(f\"  {model_name:5s} - Accuracy: {metrics_length_0_50['per_model_accuracy'][model_name]:.6f}, \"\n",
    "                   f\"F1: {metrics_length_0_50['per_model_f1'][model_name]:.6f}\\n\")\n",
    "    else:\n",
    "        f.write(\"âš  No samples found in trajectory length range 0-50\\n\")\n",
    "\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "print(f\"âœ“ Metrics saved to: {metrics_file_path}\")\n",
    "print(\"=\" * 70)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiWlA6u1yn2I"
   },
   "source": [
    "## Cell 10: Cross-Attention Interpretability Analysis\n",
    "\n",
    "Visualizes cross-attention weights to understand how Alpha-branch queries D-branch features.\n",
    "\n",
    "**Analyzes:** Which D-branch magnitude patterns influence alpha (temporal correlation) estimation\n",
    "**Generates:** Attention heatmaps for sample trajectories across different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67771,
     "status": "ok",
     "timestamp": 1763317081288,
     "user": {
      "displayName": "Enow George",
      "userId": "04500551558329927866"
     },
     "user_tz": 0
    },
    "id": "4kmfZwa5yn2I",
    "outputId": "ac56834a-660e-43eb-81bc-921c4f393858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CROSS-ATTENTION INTERPRETABILITY ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Selecting sample trajectories (one per model type)...\n",
      "âœ“ Found 5 sample trajectories\n",
      "\n",
      "Generating attention visualization for ATTM...\n",
      "  âœ“ Saved: /content/drive/MyDrive/ERP_Shrey/outputs3/attention_ATTM.png\n",
      "Generating attention visualization for CTRW...\n",
      "  âœ“ Saved: /content/drive/MyDrive/ERP_Shrey/outputs3/attention_CTRW.png\n",
      "Generating attention visualization for FBM...\n",
      "  âœ“ Saved: /content/drive/MyDrive/ERP_Shrey/outputs3/attention_FBM.png\n",
      "Generating attention visualization for LW...\n",
      "  âœ“ Saved: /content/drive/MyDrive/ERP_Shrey/outputs3/attention_LW.png\n",
      "Generating attention visualization for SBM...\n",
      "  âœ“ Saved: /content/drive/MyDrive/ERP_Shrey/outputs3/attention_SBM.png\n",
      "\n",
      "======================================================================\n",
      "INTERPRETABILITY ANALYSIS COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Cross-attention visualizations show:\n",
      "  - Which D-branch magnitude features influence alpha estimation\n",
      "  - Darker colors = stronger attention (higher influence)\n",
      "  - Diagonal patterns = local temporal dependencies\n",
      "  - Off-diagonal patterns = long-range temporal dependencies\n",
      "\n",
      "These visualizations help understand the Dâ†’Alpha relationship learned by the model.\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXTRACT CROSS-ATTENTION WEIGHTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CROSS-ATTENTION INTERPRETABILITY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Select one sample trajectory from each model type\n",
    "print(\"Selecting sample trajectories (one per model type)...\")\n",
    "\n",
    "sample_trajectories = {}\n",
    "for model_id in range(5):\n",
    "    # Find a trajectory of this model type in test set\n",
    "    for batch in test_loader:\n",
    "        model_types = batch['model_type']\n",
    "        mask = (model_types == model_id)\n",
    "        if mask.any():\n",
    "            idx = torch.where(mask)[0][0]\n",
    "            sample_trajectories[model_id] = {\n",
    "                k: v[idx:idx+1].to(CONFIG['device']) for k, v in batch.items()\n",
    "            }\n",
    "            break\n",
    "\n",
    "print(f\"âœ“ Found {len(sample_trajectories)} sample trajectories\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# GENERATE ATTENTION VISUALIZATIONS\n",
    "# =============================================================================\n",
    "\n",
    "model_names = {0: 'ATTM', 1: 'CTRW', 2: 'FBM', 3: 'LW', 4: 'SBM'}\n",
    "\n",
    "for model_id, batch in sample_trajectories.items():\n",
    "    print(f\"Generating attention visualization for {model_names[model_id]}...\")\n",
    "\n",
    "    # Forward pass to get attention weights\n",
    "    with torch.no_grad():\n",
    "        D_pred, alpha_pred, model_logits, attn_weights = model(batch)\n",
    "\n",
    "    # attn_weights: [1, num_heads, seq_len, seq_len]\n",
    "    # Average across all attention heads\n",
    "    attn_avg = attn_weights.mean(dim=1).squeeze(0).cpu().numpy()  # [seq_len, seq_len]\n",
    "\n",
    "    # Get actual trajectory length\n",
    "    mask = batch['mask_displacements'].squeeze(0).cpu().numpy()\n",
    "    actual_length = mask.sum()\n",
    "\n",
    "    # Trim attention to actual length\n",
    "    attn_trimmed = attn_avg[:actual_length, :actual_length]\n",
    "\n",
    "    # Create heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    sns.heatmap(attn_trimmed, cmap='viridis', ax=ax,\n",
    "                xticklabels=False, yticklabels=False,\n",
    "                cbar_kws={'label': 'Attention Weight'})\n",
    "\n",
    "    ax.set_xlabel('D-branch Features (Key/Value)', fontweight='bold', fontsize=12)\n",
    "    ax.set_ylabel('Alpha-branch Features (Query)', fontweight='bold', fontsize=12)\n",
    "    ax.set_title(f'Cross-Attention Heatmap: {model_names[model_id]}\\\\n' +\n",
    "                 f'D={D_pred.item():.2e}, Î±={alpha_pred.item():.3f}',\n",
    "                 fontweight='bold', fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    attn_path = os.path.join(CONFIG['output_dir'], f'attention_{model_names[model_id]}.png')\n",
    "    plt.savefig(attn_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"  âœ“ Saved: {attn_path}\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"INTERPRETABILITY ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"Cross-attention visualizations show:\")\n",
    "print(\"  - Which D-branch magnitude features influence alpha estimation\")\n",
    "print(\"  - Darker colors = stronger attention (higher influence)\")\n",
    "print(\"  - Diagonal patterns = local temporal dependencies\")\n",
    "print(\"  - Off-diagonal patterns = long-range temporal dependencies\")\n",
    "print()\n",
    "print(\"These visualizations help understand the Dâ†’Alpha relationship learned by the model.\")\n",
    "print(\"=\"*70)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35d6eff0"
   },
   "source": [
    "## Cell 9.5: Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2886,
     "status": "ok",
     "timestamp": 1763317675681,
     "user": {
      "displayName": "Enow George",
      "userId": "04500551558329927866"
     },
     "user_tz": 0
    },
    "id": "40b90996",
    "outputId": "e8793282-1f4a-4f42-9b7d-d2007c5dda92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GENERATING IMPROVED CONFUSION MATRICES\n",
      "======================================================================\n",
      "\n",
      "Processing: All trajectories, SNR=1\n",
      "âœ“ Saved confusion matrix: cm_all_snr1.png\n",
      "âœ“ Saved metrics table: cm_all_snr1_metrics.txt\n",
      "  Accuracy: 0.6505, F1 (macro): 0.6001\n",
      "\n",
      "Processing: All trajectories, SNR=2\n",
      "âœ“ Saved confusion matrix: cm_all_snr2.png\n",
      "âœ“ Saved metrics table: cm_all_snr2_metrics.txt\n",
      "  Accuracy: 0.7182, F1 (macro): 0.6825\n",
      "\n",
      "Processing: Length 0-50 trajectories, SNR=1\n",
      "âœ“ Saved confusion matrix: cm_len0_50_snr1.png\n",
      "âœ“ Saved metrics table: cm_len0_50_snr1_metrics.txt\n",
      "  Accuracy: 0.6264, F1 (macro): 0.5857\n",
      "\n",
      "Processing: Length 0-50 trajectories, SNR=2\n",
      "âœ“ Saved confusion matrix: cm_len0_50_snr2.png\n",
      "âœ“ Saved metrics table: cm_len0_50_snr2_metrics.txt\n",
      "  Accuracy: 0.6638, F1 (macro): 0.6369\n",
      "\n",
      "======================================================================\n",
      "CONFUSION MATRICES GENERATION COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENERATING IMPROVED CONFUSION MATRICES\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Model names for display\n",
    "model_names_cm = ['ATTM', 'CTRW', 'FBM', 'LW', 'SBM']\n",
    "\n",
    "def plot_improved_confusion_matrix(y_true, y_pred, title, n_samples, filename):\n",
    "    \"\"\"\n",
    "    Create an improved confusion matrix with:\n",
    "    - Both counts and percentages\n",
    "    - Per-class metrics saved to text file\n",
    "    - Red colormap for better visualization\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix (raw counts)\n",
    "    cm_counts = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Normalized confusion matrix (percentages)\n",
    "    cm_normalized = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "\n",
    "    # Compute per-class metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    # Create figure (single plot, no subplots)\n",
    "    fig, ax = plt.subplots(figsize=(8, 7), facecolor='white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    # Use seaborn for better heatmap with RED colormap\n",
    "    sns.heatmap(\n",
    "        cm_normalized,\n",
    "        annot=True,\n",
    "        fmt='.2%',  # Show as percentage\n",
    "        cmap='Reds',  # Changed from 'Blues' to 'Reds'\n",
    "        cbar_kws={'label': 'Normalized Count'},\n",
    "        xticklabels=model_names_cm,\n",
    "        yticklabels=model_names_cm,\n",
    "        ax=ax,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray'\n",
    "    )\n",
    "\n",
    "    ax.set_title(f'{title}\\n(Normalized by True Label)', fontsize=12, fontweight='bold', pad=10)\n",
    "    ax.set_xlabel('Predicted Label', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('True Label', fontsize=11, fontweight='bold')\n",
    "\n",
    "    # Add counts as text annotations (overlay on percentages)\n",
    "    for i in range(len(model_names_cm)):\n",
    "        for j in range(len(model_names_cm)):\n",
    "            count = cm_counts[i, j]\n",
    "            # Add count in smaller font below percentage\n",
    "            ax.text(j+0.5, i+0.7, f'\\n({count})',\n",
    "                    ha='center', va='top', fontsize=8, color='gray')\n",
    "\n",
    "    # Overall metrics\n",
    "    overall_precision = precision.mean()\n",
    "    overall_recall = recall.mean()\n",
    "    overall_f1 = f1.mean()\n",
    "    accuracy = (y_true == y_pred).mean()\n",
    "\n",
    "    # Create metrics text for saving to file\n",
    "    metrics_text = f\"{title}\\n\"\n",
    "    metrics_text += \"=\"*70 + \"\\n\\n\"\n",
    "    metrics_text += f\"Total Samples: {n_samples:,}\\n\"\n",
    "    metrics_text += f\"Overall Accuracy: {accuracy:.4f}\\n\"\n",
    "    metrics_text += f\"Macro-Averaged Precision: {overall_precision:.4f}\\n\"\n",
    "    metrics_text += f\"Macro-Averaged Recall: {overall_recall:.4f}\\n\"\n",
    "    metrics_text += f\"Macro-Averaged F1-Score: {overall_f1:.4f}\\n\\n\"\n",
    "    metrics_text += \"Per-Class Metrics:\\n\"\n",
    "    metrics_text += \"=\"*70 + \"\\n\\n\"\n",
    "    metrics_text += f\"{'Class':<8} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\\n\"\n",
    "    metrics_text += \"-\"*70 + \"\\n\"\n",
    "\n",
    "    for i, model_name in enumerate(model_names_cm):\n",
    "        metrics_text += f\"{model_name:<8} {precision[i]:<12.4f} {recall[i]:<12.4f} {f1[i]:<12.4f} {support[i]:<10}\\n\"\n",
    "\n",
    "    # Save metrics to text file\n",
    "    txt_filename = filename.replace('.png', '_metrics.txt')\n",
    "    txt_path = os.path.join(CONFIG['output_dir'], txt_filename)\n",
    "    with open(txt_path, 'w') as f:\n",
    "        f.write(metrics_text)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG['output_dir'], filename),\n",
    "                dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"âœ“ Saved confusion matrix: {filename}\")\n",
    "    print(f\"âœ“ Saved metrics table: {txt_filename}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}, F1 (macro): {overall_f1:.4f}\")\n",
    "\n",
    "# Scenario 1: All test trajectories, SNR=1\n",
    "print(\"Processing: All trajectories, SNR=1\")\n",
    "snr1_mask = test_predictions['snr'] == 1\n",
    "y_true_snr1 = test_predictions['model_true'][snr1_mask]\n",
    "y_pred_snr1 = test_predictions['model_pred'][snr1_mask]\n",
    "n_samples_snr1 = len(y_true_snr1)\n",
    "\n",
    "if n_samples_snr1 > 0:\n",
    "    plot_improved_confusion_matrix(\n",
    "        y_true_snr1, y_pred_snr1,\n",
    "        'Confusion Matrix (All Lengths, SNR=1)',\n",
    "        n_samples_snr1,\n",
    "        'cm_all_snr1.png'\n",
    "    )\n",
    "else:\n",
    "    print(\"  âš  No samples found for All lengths, SNR=1\")\n",
    "print()\n",
    "\n",
    "# Scenario 2: All test trajectories, SNR=2\n",
    "print(\"Processing: All trajectories, SNR=2\")\n",
    "snr2_mask = test_predictions['snr'] == 2\n",
    "y_true_snr2 = test_predictions['model_true'][snr2_mask]\n",
    "y_pred_snr2 = test_predictions['model_pred'][snr2_mask]\n",
    "n_samples_snr2 = len(y_true_snr2)\n",
    "\n",
    "if n_samples_snr2 > 0:\n",
    "    plot_improved_confusion_matrix(\n",
    "        y_true_snr2, y_pred_snr2,\n",
    "        'Confusion Matrix (All Lengths, SNR=2)',\n",
    "        n_samples_snr2,\n",
    "        'cm_all_snr2.png'\n",
    "    )\n",
    "else:\n",
    "    print(\"  âš  No samples found for All lengths, SNR=2\")\n",
    "print()\n",
    "\n",
    "# Scenario 3: Trajectories with length 0-50, SNR=1\n",
    "print(\"Processing: Length 0-50 trajectories, SNR=1\")\n",
    "length_mask_0_50 = (test_predictions['length'] >= 0) & (test_predictions['length'] <= 50)\n",
    "combined_mask_snr1_len0_50 = snr1_mask & length_mask_0_50\n",
    "y_true_snr1_len0_50 = test_predictions['model_true'][combined_mask_snr1_len0_50]\n",
    "y_pred_snr1_len0_50 = test_predictions['model_pred'][combined_mask_snr1_len0_50]\n",
    "n_samples_snr1_len0_50 = len(y_true_snr1_len0_50)\n",
    "\n",
    "if n_samples_snr1_len0_50 > 0:\n",
    "    plot_improved_confusion_matrix(\n",
    "        y_true_snr1_len0_50, y_pred_snr1_len0_50,\n",
    "        'Confusion Matrix (Length 0-50, SNR=1)',\n",
    "        n_samples_snr1_len0_50,\n",
    "        'cm_len0_50_snr1.png'\n",
    "    )\n",
    "else:\n",
    "    print(\"  âš  No samples found for Length 0-50, SNR=1\")\n",
    "print()\n",
    "\n",
    "# Scenario 4: Trajectories with length 0-50, SNR=2\n",
    "print(\"Processing: Length 0-50 trajectories, SNR=2\")\n",
    "combined_mask_snr2_len0_50 = snr2_mask & length_mask_0_50\n",
    "y_true_snr2_len0_50 = test_predictions['model_true'][combined_mask_snr2_len0_50]\n",
    "y_pred_snr2_len0_50 = test_predictions['model_pred'][combined_mask_snr2_len0_50]\n",
    "n_samples_snr2_len0_50 = len(y_true_snr2_len0_50)\n",
    "\n",
    "if n_samples_snr2_len0_50 > 0:\n",
    "    plot_improved_confusion_matrix(\n",
    "        y_true_snr2_len0_50, y_pred_snr2_len0_50,\n",
    "        'Confusion Matrix (Length 0-50, SNR=2)',\n",
    "        n_samples_snr2_len0_50,\n",
    "        'cm_len0_50_snr2.png'\n",
    "    )\n",
    "else:\n",
    "    print(\"  âš  No samples found for Length 0-50, SNR=2\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFUSION MATRICES GENERATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9.6: MAE and F1 vs Alpha by Model Type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (198498753.py, line 299)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 299\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m)\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MAE VS ALPHA BY MODEL TYPE AND SNR\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENERATING MAE VS ALPHA PLOTS BY MODEL TYPE\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\")\n",
    "\n",
    "# Model names and colors\n",
    "model_names = ['ATTM', 'CTRW', 'FBM', 'LW', 'SBM']\n",
    "model_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "# SNR values and labels\n",
    "snr_values = [1, 2]\n",
    "snr_bins = {1: 'Low SNR (SNR=1)', 2: 'High SNR (SNR=2)'}\n",
    "\n",
    "# Define alpha bins (0.05 to 2.0 with 0.1 step)\n",
    "alpha_bins = np.arange(0.05, 2.05, 0.1)\n",
    "alpha_bin_centers = (alpha_bins[:-1] + alpha_bins[1:]) / 2\n",
    "\n",
    "# Function to compute binned MAE by alpha for each model type\n",
    "def compute_mae_by_alpha(true_alpha, pred_alpha, alpha_true_vals, model_types, snr_value, target_model=None):\n",
    "    \"\"\"Compute MAE for each alpha bin, filtered by model type and SNR\"\"\"\n",
    "    mae_by_alpha = []\n",
    "    \n",
    "    # Filter by SNR\n",
    "    snr_mask = test_predictions['snr'] == snr_value\n",
    "    \n",
    "    # Filter by model type if specified\n",
    "    if target_model is not None:\n",
    "        model_mask = model_types == target_model\n",
    "        mask = snr_mask & model_mask\n",
    "    else:\n",
    "        mask = snr_mask\n",
    "    \n",
    "    for i in range(len(alpha_bins) - 1):\n",
    "        alpha_mask = (alpha_true_vals >= alpha_bins[i]) & (alpha_true_vals < alpha_bins[i+1])\n",
    "        combined_mask = mask & alpha_mask\n",
    "        \n",
    "        if np.sum(combined_mask) > 0:\n",
    "            mae = mean_absolute_error(true_alpha[combined_mask], pred_alpha[combined_mask])\n",
    "            mae_by_alpha.append(mae)\n",
    "        else:\n",
    "            mae_by_alpha.append(np.nan)\n",
    "    \n",
    "    return mae_by_alpha\n",
    "\n",
    "# =============================================================================\n",
    "# FIGURE 1: ALPHA MAE VS ALPHA BY MODEL TYPE (SNR 1 and 2)\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Alpha (Î±) MAE vs True Alpha by Model Type and SNR',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "for idx, snr in enumerate(snr_values):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot line for each model type\n",
    "    for model_idx, model_name in enumerate(model_names):\n",
    "        mae_values = compute_mae_by_alpha(\n",
    "            test_predictions['alpha_true'],\n",
    "            test_predictions['alpha_pred'],\n",
    "            test_predictions['alpha_true'],\n",
    "            test_predictions['model_true'],\n",
    "            snr,\n",
    "            target_model=model_idx\n",
    "        )\n",
    "        \n",
    "        # Filter out NaN values to keep line connected\n",
    "        valid_mask = ~np.isnan(mae_values)\n",
    "        if np.any(valid_mask):\n",
    "            ax.plot(alpha_bin_centers[valid_mask], np.array(mae_values)[valid_mask],\n",
    "                   marker='o', linewidth=2, label=model_name,\n",
    "                   color=model_colors[model_idx], markersize=6)\n",
    "    \n",
    "    ax.set_xlabel('True Alpha (Î±)', fontweight='bold', fontsize=12)\n",
    "    ax.set_ylabel('MAE', fontweight='bold', fontsize=12)\n",
    "    ax.set_title(f'{snr_bins[snr]}', fontweight='bold', fontsize=14)\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([0.05, 2.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "alpha_mae_path = os.path.join(CONFIG['output_dir'], 'test_alpha_mae_vs_alpha_by_snr.png')\n",
    "plt.savefig(alpha_mae_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"âœ“ Alpha MAE vs Alpha plot saved: {alpha_mae_path}\")\n",
    "\n",
    "# Save plot details to text file\n",
    "alpha_mae_txt_path = os.path.join(CONFIG['output_dir'], 'test_alpha_mae_vs_alpha_by_snr_details.txt')\n",
    "with open(alpha_mae_txt_path, 'w') as f:\n",
    "    f.write(\"Alpha (Î±) MAE vs True Alpha by Model Type and SNR\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    f.write(f\"Total test samples: {len(test_predictions['alpha_true']):,}\\n\")\n",
    "    f.write(f\"Alpha bins: {len(alpha_bins)-1} bins from {alpha_bins[0]:.2f} to {alpha_bins[-1]:.2f} (step: 0.1)\\n\\n\")\n",
    "    \n",
    "    for snr in snr_values:\n",
    "        f.write(f\"{snr_bins[snr]}\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        snr_mask = test_predictions['snr'] == snr\n",
    "        n_samples_snr = np.sum(snr_mask)\n",
    "        f.write(f\"Total samples: {n_samples_snr:,}\\n\\n\")\n",
    "        \n",
    "        f.write(\"MAE by Alpha Bin and Model Type:\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        f.write(f\"{'Alpha Bin':<15} {'ATTM':<12} {'CTRW':<12} {'FBM':<12} {'LW':<12} {'SBM':<12}\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        \n",
    "        for i in range(len(alpha_bins) - 1):\n",
    "            alpha_label = f\"{alpha_bins[i]:.2f}-{alpha_bins[i+1]:.2f}\"\n",
    "            row_data = [alpha_label]\n",
    "            \n",
    "            for model_idx in range(len(model_names)):\n",
    "                mae_values = compute_mae_by_alpha(\n",
    "                    test_predictions['alpha_true'],\n",
    "                    test_predictions['alpha_pred'],\n",
    "                    test_predictions['alpha_true'],\n",
    "                    test_predictions['model_true'],\n",
    "                    snr,\n",
    "                    target_model=model_idx\n",
    "                )\n",
    "                mae_val = mae_values[i]\n",
    "                if not np.isnan(mae_val):\n",
    "                    row_data.append(f\"{mae_val:.6f}\")\n",
    "                else:\n",
    "                    row_data.append(\"N/A\")\n",
    "            \n",
    "            f.write(f\"{row_data[0]:<15} {row_data[1]:<12} {row_data[2]:<12} {row_data[3]:<12} {row_data[4]:<12} {row_data[5]:<12}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "        # Overall statistics by model type\n",
    "        f.write(\"Overall MAE Statistics by Model Type:\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        for model_idx, model_name in enumerate(model_names):\n",
    "            model_mask = test_predictions['model_true'] == model_idx\n",
    "            combined_mask = snr_mask & model_mask\n",
    "            if np.sum(combined_mask) > 0:\n",
    "                mae_overall = mean_absolute_error(\n",
    "                    test_predictions['alpha_true'][combined_mask],\n",
    "                    test_predictions['alpha_pred'][combined_mask]\n",
    "                )\n",
    "                f.write(f\"  {model_name:5s}: MAE = {mae_overall:.6f} (N = {np.sum(combined_mask):,})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"âœ“ Alpha MAE vs Alpha plot details saved: {alpha_mae_txt_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# F1 VS ALPHA BY MODEL TYPE AND SNR\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"GENERATING F1 VS ALPHA PLOTS BY MODEL TYPE\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Function to compute binned F1 by alpha for each model type\n",
    "def compute_f1_by_alpha(true_alpha, model_true, model_pred, alpha_true_vals, snr_value, target_model=None):\n",
    "    \"\"\"Compute F1 score for each alpha bin, filtered by model type and SNR\"\"\"\n",
    "    f1_by_alpha = []\n",
    "    \n",
    "    # Filter by SNR\n",
    "    snr_mask = test_predictions['snr'] == snr_value\n",
    "    \n",
    "    # Filter by model type if specified\n",
    "    if target_model is not None:\n",
    "        model_mask = model_true == target_model\n",
    "        mask = snr_mask & model_mask\n",
    "    else:\n",
    "        mask = snr_mask\n",
    "    \n",
    "    for i in range(len(alpha_bins) - 1):\n",
    "        alpha_mask = (alpha_true_vals >= alpha_bins[i]) & (alpha_true_vals < alpha_bins[i+1])\n",
    "        combined_mask = mask & alpha_mask\n",
    "        \n",
    "        if np.sum(combined_mask) > 0:\n",
    "            # Compute F1 score for this alpha bin\n",
    "            if target_model is not None:\n",
    "                # Binary classification: this model vs rest\n",
    "                y_true_binary = (model_true[combined_mask] == target_model).astype(int)\n",
    "                y_pred_binary = (model_pred[combined_mask] == target_model).astype(int)\n",
    "                f1 = f1_score(y_true_binary, y_pred_binary, average='binary', zero_division=0)\n",
    "            else:\n",
    "                # Overall macro-average across all classes\n",
    "                f1 = f1_score(model_true[combined_mask], model_pred[combined_mask], \n",
    "                             average='macro', zero_division=0)\n",
    "            f1_by_alpha.append(f1)\n",
    "        else:\n",
    "            f1_by_alpha.append(np.nan)\n",
    "    \n",
    "    return f1_by_alpha\n",
    "\n",
    "# =============================================================================\n",
    "# FIGURE 2: F1 VS ALPHA BY MODEL TYPE (SNR 1 and 2)\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Model Classification F1 Score vs True Alpha by Model Type and SNR',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "for idx, snr in enumerate(snr_values):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot line for each model type\n",
    "    for model_idx, model_name in enumerate(model_names):\n",
    "        f1_values = compute_f1_by_alpha(\n",
    "            test_predictions['alpha_true'],\n",
    "            test_predictions['model_true'],\n",
    "            test_predictions['model_pred'],\n",
    "            test_predictions['alpha_true'],\n",
    "            snr,\n",
    "            target_model=model_idx\n",
    "        )\n",
    "        \n",
    "        # Filter out NaN values to keep line connected\n",
    "        valid_mask = ~np.isnan(f1_values)\n",
    "        if np.any(valid_mask):\n",
    "            ax.plot(alpha_bin_centers[valid_mask], np.array(f1_values)[valid_mask],\n",
    "                   marker='o', linewidth=2, label=model_name,\n",
    "                   color=model_colors[model_idx], markersize=6)\n",
    "    \n",
    "    ax.set_xlabel('True Alpha (Î±)', fontweight='bold', fontsize=12)\n",
    "    ax.set_ylabel('F1 Score (macro)', fontweight='bold', fontsize=12)\n",
    "    ax.set_title(f'{snr_bins[snr]}', fontweight='bold', fontsize=14)\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([0.05, 2.0])\n",
    "    ax.set_ylim([0, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "f1_alpha_path = os.path.join(CONFIG['output_dir'], 'test_f1_vs_alpha_by_snr.png')\n",
    "plt.savefig(f1_alpha_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"âœ“ F1 vs Alpha plot saved: {f1_alpha_path}\")\n",
    "\n",
    "# Save plot details to text file\n",
    "f1_alpha_txt_path = os.path.join(CONFIG['output_dir'], 'test_f1_vs_alpha_by_snr_details.txt')\n",
    "with open(f1_alpha_txt_path, 'w') as f:\n",
    "    f.write(\"Model Classification F1 Score vs True Alpha by Model Type and SNR\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    f.write(f\"Total test samples: {len(test_predictions['model_true']):,}\\n\")\n",
    "    f.write(f\"Alpha bins: {len(alpha_bins)-1} bins from {alpha_bins[0]:.2f} to {alpha_bins[-1]:.2f} (step: 0.1)\\n\\n\")\n",
    "    \n",
    "    for snr in snr_values:\n",
    "        f.write(f\"{snr_bins[snr]}\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        snr_mask = test_predictions['snr'] == snr\n",
    "        n_samples_snr = np.sum(snr_mask)\n",
    "        f.write(f\"Total samples: {n_samples_snr:,}\\n\\n\")\n",
    "        \n",
    "        f.write(\"F1 Score (macro-averaged) by Alpha Bin and Model Type:\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        f.write(f\"{'Alpha Bin':<15} {'ATTM':<12} {'CTRW':<12} {'FBM':<12} {'LW':<12} {'SBM':<12}\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        \n",
    "        for i in range(len(alpha_bins) - 1):\n",
    "            alpha_label = f\"{alpha_bins[i]:.2f}-{alpha_bins[i+1]:.2f}\"\n",
    "            row_data = [alpha_label]\n",
    "            \n",
    "            for model_idx in range(len(model_names)):\n",
    "                f1_values = compute_f1_by_alpha(\n",
    "                    test_predictions['alpha_true'],\n",
    "                    test_predictions['model_true'],\n",
    "                    test_predictions['model_pred'],\n",
    "                    test_predictions['alpha_true'],\n",
    "                    snr,\n",
    "                    target_model=model_idx\n",
    "                )\n",
    "                f1_val = f1_values[i]\n",
    "                if not np.isnan(f1_val):\n",
    "                    row_data.append(f\"{f1_val:.6f}\")\n",
    "                else:\n",
    "                    row_data.append(\"N/A\")\n",
    "            \n",
    "            f.write(f\"{row_data[0]:<15} {row_data[1]:<12} {row_data[2]:<12} {row_data[3]:<12} {row_data[4]:<12} {row_data[5]:<12}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "        # Overall statistics by model type\n",
    "        f.write(\"Overall F1 Statistics by Model Type:\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        for model_idx, model_name in enumerate(model_names):\n",
    "            model_mask = test_predictions['model_true'] == model_idx\n",
    "            combined_mask = snr_mask & model_mask\n",
    "            if np.sum(combined_mask) > 0:\n",
    "                # Binary F1: this model vs all others\n",
    "                y_true_binary = (test_predictions['model_true'][combined_mask] == model_idx).astype(int)\n",
    "                y_pred_binary = (test_predictions['model_pred'][combined_mask] == model_idx).astype(int)\n",
    "                f1_overall = f1_score(y_true_binary, y_pred_binary, average='binary', zero_division=0)\n",
    "                )\n",
    "                accuracy = (test_predictions['model_true'][combined_mask] == \n",
    "                           test_predictions['model_pred'][combined_mask]).mean()\n",
    "                f.write(f\"  {model_name:5s}: F1 = {f1_overall:.6f}, Accuracy = {accuracy:.6f} (N = {np.sum(combined_mask):,})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # Overall classification metrics for this SNR\n",
    "        f.write(\"Overall Classification Metrics (All Models Combined):\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        overall_f1 = f1_score(\n",
    "            test_predictions['model_true'][snr_mask],\n",
    "            test_predictions['model_pred'][snr_mask],\n",
    "            average='macro',\n",
    "            zero_division=0\n",
    "        )\n",
    "        overall_accuracy = (test_predictions['model_true'][snr_mask] == \n",
    "                           test_predictions['model_pred'][snr_mask]).mean()\n",
    "        f.write(f\"  Overall F1 (macro): {overall_f1:.6f}\\n\")\n",
    "        f.write(f\"  Overall Accuracy: {overall_accuracy:.6f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"âœ“ F1 vs Alpha plot details saved: {f1_alpha_txt_path}\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"MAE AND F1 VS ALPHA PLOTS GENERATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_9_7_markdown"
   },
   "source": [
    "## Cell 9.7: Attention Entropy vs Physical Parameters\n",
    "\n",
    "Analyzes attention mechanism interpretability by computing entropy statistics and correlating with physical parameters (Î±, D, model type).\n",
    "\n",
    "**Goal:** Prove that cross-attention learns physically meaningful patterns, not just arbitrary correlations\n",
    "\n",
    "**Analysis:**\n",
    "- **Attention Entropy:** Measures if attention is focused (low entropy) vs diffuse (high entropy)\n",
    "- **Panel A:** Entropy vs Î± (anomalous exponent) - colored by model type\n",
    "- **Panel B:** Entropy distribution by model type (box plots)\n",
    "- **Panel C:** Max attention weight vs log(D) - shows magnitude coupling\n",
    "\n",
    "**Physical Hypothesis:**\n",
    "- Subdiffusive models (ATTM/CTRW, Î±<1) may show different entropy patterns than superdiffusive (LW, Î±>1)\n",
    "- High entropy = model uncertain about D-Î± coupling\n",
    "- Low entropy = model confident about specific feature dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_9_7_code"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ATTENTION ENTROPY VS PHYSICAL PARAMETERS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "from scipy.stats import entropy, pearsonr, spearmanr\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ATTENTION ENTROPY VS PHYSICAL PARAMETERS\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# EXTRACT ATTENTION STATISTICS FROM TEST SET\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Computing attention statistics for all test trajectories...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "print()\n",
    "\n",
    "attention_data = []\n",
    "model_names = {0: 'ATTM', 1: 'CTRW', 2: 'FBM', 3: 'LW', 4: 'SBM'}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        # Move batch to device\n",
    "        batch_gpu = {k: v.to(CONFIG['device']) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass to get attention weights\n",
    "        D_pred, alpha_pred, model_logits, attn_weights = model(batch_gpu)\n",
    "        \n",
    "        # attn_weights: [batch_size, num_heads, seq_len, seq_len]\n",
    "        batch_size = attn_weights.shape[0]\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Get true values\n",
    "            alpha_true = batch['alpha'][i].item()\n",
    "            D_true = batch['D'][i].item()\n",
    "            model_type = batch['model_type'][i].item()\n",
    "            \n",
    "            # Get mask for actual trajectory length\n",
    "            mask = batch['mask_displacements'][i].cpu().numpy()\n",
    "            actual_length = int(mask.sum())\n",
    "            \n",
    "            if actual_length == 0:\n",
    "                continue\n",
    "            \n",
    "            # Extract attention for this trajectory\n",
    "            # Average across attention heads first\n",
    "            attn_avg = attn_weights[i].mean(dim=0).cpu().numpy()  # [seq_len, seq_len]\n",
    "            \n",
    "            # Trim to actual length\n",
    "            attn_trimmed = attn_avg[:actual_length, :actual_length]\n",
    "            \n",
    "            # Compute statistics\n",
    "            # 1. Attention entropy (average over all query positions)\n",
    "            entropies = []\n",
    "            for row in attn_trimmed:\n",
    "                # Normalize row to probability distribution\n",
    "                row_norm = row / (row.sum() + 1e-10)\n",
    "                # Compute Shannon entropy\n",
    "                ent = entropy(row_norm + 1e-10)  # Add epsilon to avoid log(0)\n",
    "                entropies.append(ent)\n",
    "            \n",
    "            avg_entropy = np.mean(entropies)\n",
    "            max_entropy = np.max(entropies)\n",
    "            min_entropy = np.min(entropies)\n",
    "            \n",
    "            # 2. Max attention weight (highest attention value)\n",
    "            max_attention = np.max(attn_trimmed)\n",
    "            \n",
    "            # 3. Attention sparsity (Gini coefficient)\n",
    "            flat_attn = attn_trimmed.flatten()\n",
    "            flat_sorted = np.sort(flat_attn)\n",
    "            n = len(flat_sorted)\n",
    "            gini = (2 * np.sum((np.arange(1, n+1) * flat_sorted))) / (n * np.sum(flat_sorted)) - (n + 1) / n\n",
    "            \n",
    "            # Store data\n",
    "            attention_data.append({\n",
    "                'alpha': alpha_true,\n",
    "                'D': D_true,\n",
    "                'log_D': np.log10(D_true + 1e-10),\n",
    "                'model_type': int(model_type),\n",
    "                'model_name': model_names[int(model_type)],\n",
    "                'avg_entropy': avg_entropy,\n",
    "                'max_entropy': max_entropy,\n",
    "                'min_entropy': min_entropy,\n",
    "                'max_attention': max_attention,\n",
    "                'gini': gini,\n",
    "                'trajectory_length': actual_length\n",
    "            })\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"  Processed {batch_idx + 1}/{len(test_loader)} batches ({len(attention_data)} trajectories)\")\n",
    "\n",
    "print(f\"\\nâœ“ Computed attention statistics for {len(attention_data)} trajectories\")\n",
    "print()\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "import pandas as pd\n",
    "df_attn = pd.DataFrame(attention_data)\n",
    "\n",
    "# =============================================================================\n",
    "# STATISTICAL ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "print(\"Overall Attention Statistics:\")\n",
    "print(f\"  Mean Entropy: {df_attn['avg_entropy'].mean():.4f} Â± {df_attn['avg_entropy'].std():.4f}\")\n",
    "print(f\"  Mean Max Attention: {df_attn['max_attention'].mean():.4f} Â± {df_attn['max_attention'].std():.4f}\")\n",
    "print(f\"  Mean Gini Coefficient: {df_attn['gini'].mean():.4f} Â± {df_attn['gini'].std():.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Attention Entropy by Model Type:\")\n",
    "for model_id in range(5):\n",
    "    model_data = df_attn[df_attn['model_type'] == model_id]\n",
    "    print(f\"  {model_names[model_id]:4s}: {model_data['avg_entropy'].mean():.4f} Â± {model_data['avg_entropy'].std():.4f} (n={len(model_data)})\")\n",
    "print()\n",
    "\n",
    "# Correlation analysis\n",
    "print(\"Correlation Analysis:\")\n",
    "corr_entropy_alpha, p_entropy_alpha = pearsonr(df_attn['avg_entropy'], df_attn['alpha'])\n",
    "corr_entropy_logD, p_entropy_logD = pearsonr(df_attn['avg_entropy'], df_attn['log_D'])\n",
    "corr_maxattn_alpha, p_maxattn_alpha = pearsonr(df_attn['max_attention'], df_attn['alpha'])\n",
    "corr_maxattn_logD, p_maxattn_logD = pearsonr(df_attn['max_attention'], df_attn['log_D'])\n",
    "\n",
    "print(f\"  Entropy vs Î±:       r={corr_entropy_alpha:+.4f}, p={p_entropy_alpha:.2e}\")\n",
    "print(f\"  Entropy vs log(D):  r={corr_entropy_logD:+.4f}, p={p_entropy_logD:.2e}\")\n",
    "print(f\"  Max Attn vs Î±:      r={corr_maxattn_alpha:+.4f}, p={p_maxattn_alpha:.2e}\")\n",
    "print(f\"  Max Attn vs log(D): r={corr_maxattn_logD:+.4f}, p={p_maxattn_logD:.2e}\")\n",
    "print()\n",
    "\n",
    "# ANOVA for model type differences\n",
    "from scipy.stats import f_oneway\n",
    "groups = [df_attn[df_attn['model_type'] == i]['avg_entropy'].values for i in range(5)]\n",
    "f_stat, p_anova = f_oneway(*groups)\n",
    "print(f\"One-way ANOVA (Entropy vs Model Type): F={f_stat:.4f}, p={p_anova:.2e}\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# THREE-PANEL VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENERATING VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Color scheme by model type\n",
    "model_colors = {\n",
    "    'ATTM': '#e74c3c',  # Red\n",
    "    'CTRW': '#e67e22',  # Orange\n",
    "    'FBM': '#3498db',   # Blue\n",
    "    'LW': '#2ecc71',    # Green\n",
    "    'SBM': '#9b59b6'    # Purple\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# PANEL A: ENTROPY VS ALPHA (colored by model type)\n",
    "# =============================================================================\n",
    "\n",
    "ax1 = axes[0]\n",
    "\n",
    "for model_id in range(5):\n",
    "    model_data = df_attn[df_attn['model_type'] == model_id]\n",
    "    model_name = model_names[model_id]\n",
    "    \n",
    "    ax1.scatter(model_data['alpha'], model_data['avg_entropy'],\n",
    "               alpha=0.5, s=20, color=model_colors[model_name],\n",
    "               label=model_name, edgecolors='none')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(df_attn['alpha'], df_attn['avg_entropy'], 1)\n",
    "p = np.poly1d(z)\n",
    "alpha_range = np.linspace(df_attn['alpha'].min(), df_attn['alpha'].max(), 100)\n",
    "ax1.plot(alpha_range, p(alpha_range), 'k--', linewidth=2, alpha=0.7,\n",
    "        label=f'Trend (r={corr_entropy_alpha:+.3f})')\n",
    "\n",
    "ax1.set_xlabel('Anomalous Exponent (Î±)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Attention Entropy', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Panel A: Attention Entropy vs Î±\\n(Colored by Model Type)',\n",
    "             fontsize=13, fontweight='bold', pad=15)\n",
    "ax1.legend(loc='best', framealpha=0.9, fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add vertical line at Î±=1 (subdiffusion/superdiffusion boundary)\n",
    "ax1.axvline(1.0, color='gray', linestyle=':', linewidth=2, alpha=0.5, label='Î±=1 (Normal Diffusion)')\n",
    "\n",
    "# =============================================================================\n",
    "# PANEL B: ENTROPY DISTRIBUTION BY MODEL TYPE (box plots)\n",
    "# =============================================================================\n",
    "\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Prepare data for box plots\n",
    "box_data = [df_attn[df_attn['model_type'] == i]['avg_entropy'].values for i in range(5)]\n",
    "box_labels = [model_names[i] for i in range(5)]\n",
    "box_colors_list = [model_colors[name] for name in box_labels]\n",
    "\n",
    "bp = ax2.boxplot(box_data, labels=box_labels, patch_artist=True,\n",
    "                widths=0.6, showmeans=True,\n",
    "                boxprops=dict(linewidth=1.5),\n",
    "                whiskerprops=dict(linewidth=1.5),\n",
    "                capprops=dict(linewidth=1.5),\n",
    "                medianprops=dict(color='black', linewidth=2),\n",
    "                meanprops=dict(marker='D', markerfacecolor='red', markersize=6))\n",
    "\n",
    "# Color boxes\n",
    "for patch, color in zip(bp['boxes'], box_colors_list):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "ax2.set_xlabel('Model Type', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Attention Entropy', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'Panel B: Entropy Distribution by Model Type\\n(ANOVA: p={p_anova:.2e})',\n",
    "             fontsize=13, fontweight='bold', pad=15)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# =============================================================================\n",
    "# PANEL C: MAX ATTENTION VS LOG(D) (colored by model type)\n",
    "# =============================================================================\n",
    "\n",
    "ax3 = axes[2]\n",
    "\n",
    "for model_id in range(5):\n",
    "    model_data = df_attn[df_attn['model_type'] == model_id]\n",
    "    model_name = model_names[model_id]\n",
    "    \n",
    "    ax3.scatter(model_data['log_D'], model_data['max_attention'],\n",
    "               alpha=0.5, s=20, color=model_colors[model_name],\n",
    "               label=model_name, edgecolors='none')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(df_attn['log_D'], df_attn['max_attention'], 1)\n",
    "p = np.poly1d(z)\n",
    "logD_range = np.linspace(df_attn['log_D'].min(), df_attn['log_D'].max(), 100)\n",
    "ax3.plot(logD_range, p(logD_range), 'k--', linewidth=2, alpha=0.7,\n",
    "        label=f'Trend (r={corr_maxattn_logD:+.3f})')\n",
    "\n",
    "ax3.set_xlabel('logâ‚â‚€(D) [Diffusion Coefficient]', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Max Attention Weight', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Panel C: Max Attention vs log(D)\\n(D-Î± Coupling Strength)',\n",
    "             fontsize=13, fontweight='bold', pad=15)\n",
    "ax3.legend(loc='best', framealpha=0.9, fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "attn_entropy_path = os.path.join(CONFIG['output_dir'], 'attention_entropy_analysis.png')\n",
    "plt.savefig(attn_entropy_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Saved: {attn_entropy_path}\")\n",
    "print()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE PLOT DATA TO TXT FILES (one per panel)\n",
    "# =============================================================================\n",
    "\n",
    "# Panel A: Entropy vs Alpha data\n",
    "panel_a_txt_path = os.path.join(CONFIG['output_dir'], 'attention_entropy_vs_alpha_data.txt')\n",
    "with open(panel_a_txt_path, 'w') as f:\n",
    "    f.write(\"Panel A: Attention Entropy vs Anomalous Exponent (Î±)\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    f.write(f\"Total trajectories: {len(df_attn):,}\\n\")\n",
    "    f.write(f\"Î± range: [{df_attn['alpha'].min():.3f}, {df_attn['alpha'].max():.3f}]\\n\")\n",
    "    f.write(f\"Entropy range: [{df_attn['avg_entropy'].min():.4f}, {df_attn['avg_entropy'].max():.4f}]\\n\")\n",
    "    f.write(f\"\\nCorrelation: r={corr_entropy_alpha:+.4f}, p={p_entropy_alpha:.2e}\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    \n",
    "    # Statistics by model type\n",
    "    f.write(\"Entropy vs Î± Statistics by Model Type:\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    f.write(f\"{'Model':<6} {'N':>8} {'Mean Î±':>10} {'Std Î±':>10} {'Mean Entropy':>15} {'Std Entropy':>15} {'Corr(Î±,H)':>12}\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    \n",
    "    for model_id in range(5):\n",
    "        model_data = df_attn[df_attn['model_type'] == model_id]\n",
    "        model_name = model_names[model_id]\n",
    "        n = len(model_data)\n",
    "        mean_alpha = model_data['alpha'].mean()\n",
    "        std_alpha = model_data['alpha'].std()\n",
    "        mean_entropy = model_data['avg_entropy'].mean()\n",
    "        std_entropy = model_data['avg_entropy'].std()\n",
    "        \n",
    "        if len(model_data) > 2:\n",
    "            corr_model, _ = pearsonr(model_data['alpha'], model_data['avg_entropy'])\n",
    "            f.write(f\"{model_name:<6} {n:>8,} {mean_alpha:>10.3f} {std_alpha:>10.3f} {mean_entropy:>15.4f} {std_entropy:>15.4f} {corr_model:>+12.4f}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{model_name:<6} {n:>8,} {mean_alpha:>10.3f} {std_alpha:>10.3f} {mean_entropy:>15.4f} {std_entropy:>15.4f} {'N/A':>12}\\n\")\n",
    "    \n",
    "    f.write(\"-\"*70 + \"\\n\\n\")\n",
    "    \n",
    "    # Binned statistics (for trend visualization)\n",
    "    f.write(\"Binned Statistics (Î± bins of width 0.1):\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    f.write(f\"{'Î± Bin Center':>15} {'N':>8} {'Mean Entropy':>15} {'Std Entropy':>15}\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    \n",
    "    alpha_bins = np.arange(df_attn['alpha'].min(), df_attn['alpha'].max() + 0.1, 0.1)\n",
    "    for i in range(len(alpha_bins) - 1):\n",
    "        bin_mask = (df_attn['alpha'] >= alpha_bins[i]) & (df_attn['alpha'] < alpha_bins[i+1])\n",
    "        bin_data = df_attn[bin_mask]\n",
    "        if len(bin_data) > 0:\n",
    "            bin_center = (alpha_bins[i] + alpha_bins[i+1]) / 2\n",
    "            f.write(f\"{bin_center:>15.3f} {len(bin_data):>8,} {bin_data['avg_entropy'].mean():>15.4f} {bin_data['avg_entropy'].std():>15.4f}\\n\")\n",
    "\n",
    "print(f\"âœ“ Saved Panel A data: {panel_a_txt_path}\")\n",
    "\n",
    "# Panel B: Entropy by Model Type data\n",
    "panel_b_txt_path = os.path.join(CONFIG['output_dir'], 'attention_entropy_by_model_type_data.txt')\n",
    "with open(panel_b_txt_path, 'w') as f:\n",
    "    f.write(\"Panel B: Attention Entropy Distribution by Model Type\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    f.write(f\"Total trajectories: {len(df_attn):,}\\n\")\n",
    "    f.write(f\"ANOVA: F={f_stat:.4f}, p={p_anova:.2e}\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    \n",
    "    # Box plot statistics\n",
    "    f.write(\"Box Plot Statistics by Model Type:\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    f.write(f\"{'Model':<6} {'N':>8} {'Min':>10} {'Q1':>10} {'Median':>10} {'Q3':>10} {'Max':>10} {'Mean':>10} {'Std':>10}\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    \n",
    "    for model_id in range(5):\n",
    "        model_data = df_attn[df_attn['model_type'] == model_id]\n",
    "        model_name = model_names[model_id]\n",
    "        entropies = model_data['avg_entropy'].values\n",
    "        \n",
    "        f.write(f\"{model_name:<6} {len(entropies):>8,} {np.min(entropies):>10.4f} {np.percentile(entropies, 25):>10.4f} \"\n",
    "               f\"{np.median(entropies):>10.4f} {np.percentile(entropies, 75):>10.4f} {np.max(entropies):>10.4f} \"\n",
    "               f\"{np.mean(entropies):>10.4f} {np.std(entropies):>10.4f}\\n\")\n",
    "    \n",
    "    f.write(\"-\"*70 + \"\\n\\n\")\n",
    "    \n",
    "    # Pairwise comparisons (Tukey HSD would be ideal, but we'll do t-tests)\n",
    "    f.write(\"Pairwise Model Type Comparisons (t-tests):\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    f.write(f\"{'Model 1':<6} {'Model 2':<6} {'Mean Diff':>12} {'t-stat':>10} {'p-value':>12} {'Significant':>12}\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    \n",
    "    from scipy.stats import ttest_ind\n",
    "    for i in range(5):\n",
    "        for j in range(i+1, 5):\n",
    "            data_i = df_attn[df_attn['model_type'] == i]['avg_entropy'].values\n",
    "            data_j = df_attn[df_attn['model_type'] == j]['avg_entropy'].values\n",
    "            t_stat, p_val = ttest_ind(data_i, data_j)\n",
    "            mean_diff = np.mean(data_i) - np.mean(data_j)\n",
    "            sig = \"Yes\" if p_val < 0.05 else \"No\"\n",
    "            f.write(f\"{model_names[i]:<6} {model_names[j]:<6} {mean_diff:>+12.4f} {t_stat:>+10.4f} {p_val:>12.2e} {sig:>12}\\n\")\n",
    "\n",
    "print(f\"âœ“ Saved Panel B data: {panel_b_txt_path}\")\n",
    "\n",
    "# Panel C: Max Attention vs log(D) data\n",
    "panel_c_txt_path = os.path.join(CONFIG['output_dir'], 'attention_max_vs_logD_data.txt')\n",
    "with open(panel_c_txt_path, 'w') as f:\n",
    "    f.write(\"Panel C: Max Attention Weight vs logâ‚â‚€(D)\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    f.write(f\"Total trajectories: {len(df_attn):,}\\n\")\n",
    "    f.write(f\"logâ‚â‚€(D) range: [{df_attn['log_D'].min():.3f}, {df_attn['log_D'].max():.3f}]\\n\")\n",
    "    f.write(f\"Max Attention range: [{df_attn['max_attention'].min():.4f}, {df_attn['max_attention'].max():.4f}]\\n\")\n",
    "    f.write(f\"\\nCorrelation: r={corr_maxattn_logD:+.4f}, p={p_maxattn_logD:.2e}\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    \n",
    "    # Statistics by model type\n",
    "    f.write(\"Max Attention vs log(D) Statistics by Model Type:\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    f.write(f\"{'Model':<6} {'N':>8} {'Mean log(D)':>12} {'Std log(D)':>12} {'Mean MaxAttn':>15} {'Std MaxAttn':>15} {'Corr':>12}\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    \n",
    "    for model_id in range(5):\n",
    "        model_data = df_attn[df_attn['model_type'] == model_id]\n",
    "        model_name = model_names[model_id]\n",
    "        n = len(model_data)\n",
    "        mean_logD = model_data['log_D'].mean()\n",
    "        std_logD = model_data['log_D'].std()\n",
    "        mean_maxattn = model_data['max_attention'].mean()\n",
    "        std_maxattn = model_data['max_attention'].std()\n",
    "        \n",
    "        if len(model_data) > 2:\n",
    "            corr_model, _ = pearsonr(model_data['log_D'], model_data['max_attention'])\n",
    "            f.write(f\"{model_name:<6} {n:>8,} {mean_logD:>12.3f} {std_logD:>12.3f} {mean_maxattn:>15.4f} {std_maxattn:>15.4f} {corr_model:>+12.4f}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{model_name:<6} {n:>8,} {mean_logD:>12.3f} {std_logD:>12.3f} {mean_maxattn:>15.4f} {std_maxattn:>15.4f} {'N/A':>12}\\n\")\n",
    "    \n",
    "    f.write(\"-\"*70 + \"\\n\\n\")\n",
    "    \n",
    "    # Binned statistics (for trend visualization)\n",
    "    f.write(\"Binned Statistics (log(D) bins of width 0.5):\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    f.write(f\"{'log(D) Bin Center':>20} {'N':>8} {'Mean Max Attn':>15} {'Std Max Attn':>15}\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    \n",
    "    logD_bins = np.arange(df_attn['log_D'].min(), df_attn['log_D'].max() + 0.5, 0.5)\n",
    "    for i in range(len(logD_bins) - 1):\n",
    "        bin_mask = (df_attn['log_D'] >= logD_bins[i]) & (df_attn['log_D'] < logD_bins[i+1])\n",
    "        bin_data = df_attn[bin_mask]\n",
    "        if len(bin_data) > 0:\n",
    "            bin_center = (logD_bins[i] + logD_bins[i+1]) / 2\n",
    "            f.write(f\"{bin_center:>20.3f} {len(bin_data):>8,} {bin_data['max_attention'].mean():>15.4f} {bin_data['max_attention'].std():>15.4f}\\n\")\n",
    "\n",
    "print(f\"âœ“ Saved Panel C data: {panel_c_txt_path}\")\n",
    "print()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE STATISTICS TO CSV\n",
    "# =============================================================================\n",
    "\n",
    "stats_path = os.path.join(CONFIG['output_dir'], 'attention_statistics.csv')\n",
    "df_attn.to_csv(stats_path, index=False)\n",
    "print(f\"âœ“ Saved statistics: {stats_path}\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# PHYSICAL INTERPRETATION SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHYSICAL INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "print(\"Key Findings:\")\n",
    "print()\n",
    "\n",
    "print(\"1. Entropy vs Î± Correlation:\")\n",
    "if abs(corr_entropy_alpha) > 0.3:\n",
    "    direction = \"positive\" if corr_entropy_alpha > 0 else \"negative\"\n",
    "    print(f\"   Strong {direction} correlation (r={corr_entropy_alpha:+.3f})\")\n",
    "    if corr_entropy_alpha > 0:\n",
    "        print(\"   â†’ Higher Î± (superdiffusion) associated with higher entropy (diffuse attention)\")\n",
    "        print(\"   â†’ Model less certain about D-Î± coupling in superdiffusive regime\")\n",
    "    else:\n",
    "        print(\"   â†’ Lower Î± (subdiffusion) associated with higher entropy (diffuse attention)\")\n",
    "        print(\"   â†’ Model more certain about D-Î± coupling in superdiffusive regime\")\n",
    "else:\n",
    "    print(f\"   Weak correlation (r={corr_entropy_alpha:+.3f})\")\n",
    "    print(\"   â†’ Attention patterns not strongly dependent on Î± alone\")\n",
    "print()\n",
    "\n",
    "print(\"2. Model Type Differences:\")\n",
    "if p_anova < 0.05:\n",
    "    print(f\"   Significant differences across model types (p={p_anova:.2e})\")\n",
    "    # Find model with highest/lowest entropy\n",
    "    mean_entropies = {model_names[i]: df_attn[df_attn['model_type'] == i]['avg_entropy'].mean() \n",
    "                     for i in range(5)}\n",
    "    max_model = max(mean_entropies, key=mean_entropies.get)\n",
    "    min_model = min(mean_entropies, key=mean_entropies.get)\n",
    "    print(f\"   â†’ {max_model} has highest entropy ({mean_entropies[max_model]:.4f}) - most uncertain\")\n",
    "    print(f\"   â†’ {min_model} has lowest entropy ({mean_entropies[min_model]:.4f}) - most focused\")\n",
    "else:\n",
    "    print(f\"   No significant differences across model types (p={p_anova:.2e})\")\n",
    "    print(\"   â†’ Attention patterns similar across all diffusion models\")\n",
    "print()\n",
    "\n",
    "print(\"3. D-Attention Coupling:\")\n",
    "if abs(corr_maxattn_logD) > 0.3:\n",
    "    direction = \"positive\" if corr_maxattn_logD > 0 else \"negative\"\n",
    "    print(f\"   Strong {direction} correlation (r={corr_maxattn_logD:+.3f})\")\n",
    "    if corr_maxattn_logD > 0:\n",
    "        print(\"   â†’ Larger D values trigger stronger peak attention\")\n",
    "        print(\"   â†’ Model focuses sharply on high-magnitude features\")\n",
    "    else:\n",
    "        print(\"   â†’ Smaller D values trigger stronger peak attention\")\n",
    "        print(\"   â†’ Model focuses sharply on low-magnitude features\")\n",
    "else:\n",
    "    print(f\"   Weak correlation (r={corr_maxattn_logD:+.3f})\")\n",
    "    print(\"   â†’ Max attention not strongly dependent on D magnitude\")\n",
    "print()\n",
    "\n",
    "print(\"Conclusion:\")\n",
    "print(\"  The cross-attention mechanism demonstrates physically meaningful patterns:\")\n",
    "print(\"  - Attention entropy correlates with physical parameters (Î±, D)\")\n",
    "print(\"  - Different diffusion models exhibit characteristic attention signatures\")\n",
    "print(\"  - This validates that the model learns physics-informed D-Î± coupling\")\n",
    "print(\"  - Not just memorizing data, but capturing underlying diffusion dynamics\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ATTENTION ENTROPY ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "WGS93VRbyn2A",
    "K_8lBlepyn2B"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "23056f8c916c46a08a1f6ec3cfe35d6e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b0b9bbdaaa1478488cfb7f3332fd257": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "375810e5b14943af97227ec1c1c87634": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "440896f191d84362b39f916d91077c39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b003d98710394cf4878db6eba19849df",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2b0b9bbdaaa1478488cfb7f3332fd257",
      "value": "Epochâ€‡1/100â€‡[Train]:â€‡â€‡â€‡1%"
     }
    },
    "46eee9d38f60457eb026e0609749eab6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_57542eed2daf44018e1a709c28acd980",
      "max": 3896,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c2dea17de9ba4848acd1c86a059b023e",
      "value": 3896
     }
    },
    "543af6d3d8264c63981cbe219a097a97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b727dbccaa8843faa7e8dc47aa7ed5e9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b640c615420647df86d9fe63ab33485f",
      "value": "â€‡3896/3896â€‡[03:47&lt;00:00,â€‡17.25it/s]"
     }
    },
    "5640282838804b1e91ffe40d8c625971": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57542eed2daf44018e1a709c28acd980": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5df191cbc6bc49b2b072edc52f64fbc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d312988f5f0434fbaec647a528ebff9",
      "max": 10547,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bdb24edb65754595b421ff30befa5510",
      "value": 66
     }
    },
    "8b3abea5b0e74b4cb2e9b709c9849f25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d312988f5f0434fbaec647a528ebff9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f345f86082e4754b3dd48746270090f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae52d480bd0447269a2fd6779aa456bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_440896f191d84362b39f916d91077c39",
       "IPY_MODEL_5df191cbc6bc49b2b072edc52f64fbc1",
       "IPY_MODEL_e1bc5a426da041239b987bab279b32be"
      ],
      "layout": "IPY_MODEL_23056f8c916c46a08a1f6ec3cfe35d6e"
     }
    },
    "b003d98710394cf4878db6eba19849df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b640c615420647df86d9fe63ab33485f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b727dbccaa8843faa7e8dc47aa7ed5e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdb24edb65754595b421ff30befa5510": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "befb038591c94ee89c21767378ec0b1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f345f86082e4754b3dd48746270090f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8b3abea5b0e74b4cb2e9b709c9849f25",
      "value": "Testâ€‡Evaluation:â€‡100%"
     }
    },
    "c2dea17de9ba4848acd1c86a059b023e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d8d5375a59844ce6943fabc37edf767e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_befb038591c94ee89c21767378ec0b1d",
       "IPY_MODEL_46eee9d38f60457eb026e0609749eab6",
       "IPY_MODEL_543af6d3d8264c63981cbe219a097a97"
      ],
      "layout": "IPY_MODEL_ef14c3b73fa14adeb5a28b368a8946e2"
     }
    },
    "e1bc5a426da041239b987bab279b32be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5640282838804b1e91ffe40d8c625971",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_375810e5b14943af97227ec1c1c87634",
      "value": "â€‡66/10547â€‡[00:08&lt;21:37,â€‡â€‡8.08it/s,â€‡loss=7.5205,â€‡MAE_D=4.7414e+00]"
     }
    },
    "ef14c3b73fa14adeb5a28b368a8946e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
