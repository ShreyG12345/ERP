{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f5f265",
   "metadata": {},
   "source": [
    "# ANDI Dataset Generation Pipeline\n",
    "\n",
    "Generates training data for the Dual CNN Cross-Attention architecture.\n",
    "\n",
    "Dataset specifications:\n",
    "- 2,000,000 trajectories (67.5% train / 7.5% val / 25% test)\n",
    "- 5 diffusion models: ATTM, CTRW, FBM, LW, SBM\n",
    "- 39 discrete alpha values: [0.05, 0.10, ..., 2.00]\n",
    "- Variable trajectory lengths: [10, 1000] across 12 bins\n",
    "- Ground truth α from ANDI generation, D from MSD-based Langevin estimator\n",
    "- Dual preprocessing: scaled displacements (alpha-branch) + raw displacements (D-branch)\n",
    "- HDF5 output with gzip compression (float16 for size reduction)\n",
    "- Clean training data (noise only in test set)\n",
    "\n",
    "Estimated runtime: 4-7 hours for full 2M dataset.\n",
    "\n",
    "References:\n",
    "- Firbas et al. (2023): ConvTransformer baseline\n",
    "- Korabel & Waigh (2023): MSD-based D estimation\n",
    "- ANDI Challenge specifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9183f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Running on local machine\n",
      "✓ Base directory: /home/magjun/Documents/ERP_Shrey/Report_V2_Preprocessing_and_training\n",
      "\n",
      "======================================================================\n",
      "INSTALLING REQUIRED PACKAGES\n",
      "======================================================================\n",
      "\n",
      "Installing andi-datasets...\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Installing hurst...\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Installing additional packages...\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "✓ All packages installed successfully\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Installation and environment setup\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")\n",
    "\n",
    "# Mount Google Drive if in Colab\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_DIR = '/content/drive/MyDrive/ERP_Shrey'\n",
    "    WORK_DIR = BASE_DIR\n",
    "else:\n",
    "    BASE_DIR = os.getcwd()\n",
    "    WORK_DIR = BASE_DIR\n",
    "\n",
    "# Create data directories\n",
    "os.makedirs(os.path.join(BASE_DIR, 'data/andi'), exist_ok=True)\n",
    "if IN_COLAB:\n",
    "    os.makedirs(os.path.join(WORK_DIR, 'temp_data'), exist_ok=True)\n",
    "\n",
    "# Install packages\n",
    "# andi-datasets requires numpy<=1.26.4\n",
    "if IN_COLAB:\n",
    "    print(\"Installing compatible numpy version...\")\n",
    "    !pip install -q \"numpy<=1.26.4\" --force-reinstall --no-deps\n",
    "    !pip install -q \"numpy<=1.26.4\"\n",
    "\n",
    "print(\"Installing andi-datasets...\")\n",
    "!pip install -q andi-datasets\n",
    "\n",
    "print(\"Installing hurst...\")\n",
    "!pip install -q hurst\n",
    "\n",
    "print(\"Installing additional packages...\")\n",
    "if IN_COLAB:\n",
    "    !pip install -q tqdm h5py scikit-learn pandas matplotlib seaborn\n",
    "else:\n",
    "    !pip install -q tqdm h5py scikit-learn pandas numpy matplotlib seaborn\n",
    "\n",
    "print(\"Packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbae48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n",
      "\n",
      "======================================================================\n",
      "PATH CONFIGURATION\n",
      "======================================================================\n",
      "\n",
      "Data directory: /home/magjun/Documents/ERP_Shrey/Report_V2_Preprocessing_and_training/data/andi\n",
      "\n",
      "✓ Directories configured\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and setup paths\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    from andi_datasets.datasets_theory import datasets_theory\n",
    "    import hurst as hurst_module\n",
    "    andi = datasets_theory()\n",
    "except ImportError as e:\n",
    "    raise RuntimeError(f\"Required library not found: {e}. Run Cell 1 first.\")\n",
    "\n",
    "from hurst import compute_Hc\n",
    "\n",
    "# Setup output directories\n",
    "if IN_COLAB:\n",
    "    TEMP_OUTPUT_DIR = os.path.join(WORK_DIR, 'temp_data')\n",
    "    FINAL_OUTPUT_DIR = os.path.join(BASE_DIR, 'data/andi')\n",
    "else:\n",
    "    TEMP_OUTPUT_DIR = os.path.join(BASE_DIR, 'data/andi')\n",
    "    FINAL_OUTPUT_DIR = TEMP_OUTPUT_DIR\n",
    "\n",
    "os.makedirs(TEMP_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(FINAL_OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6270b30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ WARNING: TEST MODE ENABLED\n",
      "Generating small dataset for testing (100K trajectories)\n",
      "Set TEST_MODE = False for production 2M dataset\n",
      "\n",
      "======================================================================\n",
      "DATASET CONFIGURATION - ANDI TABLE 2 COMPLIANT\n",
      "======================================================================\n",
      "Mode: TEST\n",
      "Total trajectories: 100,000\n",
      "  - Train: 0 (0.0%)\n",
      "  - Val: 0 (0.0%)\n",
      "  - Test: 100,000 (100.0%)\n",
      "\n",
      "Models: ['ATTM', 'CTRW', 'FBM', 'LW', 'SBM']\n",
      "\n",
      "ANDI Table 2 Test Set Specifications:\n",
      "  - Unique permutations: 1,768\n",
      "  - Replications per permutation: 56\n",
      "  - Lengths: 13 values [10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 600, 800, 1000]\n",
      "  - SNR levels: [0, 0]\n",
      "  - Alpha ranges:\n",
      "      ATTM: [0.10 to 1.00] (10 values)\n",
      "      CTRW: [0.10 to 1.00] (10 values)\n",
      "      FBM: [0.10 to 1.90] (19 values)\n",
      "      LW: [1.00 to 1.90] (10 values)\n",
      "      SBM: [0.10 to 1.90] (19 values)\n",
      "\n",
      "Train/Val Set Specifications:\n",
      "  - Total (model, α) combinations: 68\n",
      "  - Trajectories per combination: ~0\n",
      "  - Length sampling: Weighted distribution from bins\n",
      "  - Noise: None (clean data)\n",
      "\n",
      "Max length (padding): 1000\n",
      "Dimensionality: 1D\n",
      "\n",
      "Ground truth methods:\n",
      "  - H (FBM): Theoretical (H = α/2)\n",
      "  - H (others): R/S analysis\n",
      "  - D (all): Variance-based MSD\n",
      "\n",
      "Noise protocol:\n",
      "  - Train/Val: Clean data (no noise)\n",
      "  - Test: Clean data (no noise)\n",
      "\n",
      "Batch size: 10,000 trajectories\n",
      "Random seed: 42\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataset configuration\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Model mapping (ANDI standard)\n",
    "# 0: ATTM, 1: CTRW, 2: FBM, 3: LW, 4: SBM\n",
    "MODELS = [0, 1, 2, 3, 4]\n",
    "MODEL_NAMES = {0: \"ATTM\", 1: \"CTRW\", 2: \"FBM\", 3: \"LW\", 4: \"SBM\"}\n",
    "\n",
    "# Dataset size\n",
    "TEST_MODE = True  # Set False for production\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(\"TEST MODE: generating 100K trajectories\")\n",
    "    TOTAL_TRAJECTORIES = 100_000\n",
    "    BATCH_SIZE = 10_000\n",
    "else:\n",
    "    TOTAL_TRAJECTORIES = 500_000\n",
    "    BATCH_SIZE = 10_000\n",
    "\n",
    "# Split ratios (Firbas et al. 2023)\n",
    "TRAIN_RATIO = 0.67  # 67.5%\n",
    "VAL_RATIO = 0.075    # 7.5%\n",
    "TEST_RATIO = 0.25    # 25.0%\n",
    "\n",
    "# Calculate split counts based on TEST_MODE\n",
    "if TEST_MODE:\n",
    "    # In test mode, use ratios to calculate from TOTAL_TRAJECTORIES\n",
    "    N_TRAIN = int(TOTAL_TRAJECTORIES * TRAIN_RATIO)\n",
    "    N_VAL = int(TOTAL_TRAJECTORIES * VAL_RATIO)\n",
    "    N_TEST = int(TOTAL_TRAJECTORIES * TEST_RATIO)\n",
    "else:\n",
    "    # Production mode: Fixed targets\n",
    "    N_TRAIN = 1200000  # Fixed target\n",
    "    N_VAL = 200000      # Fixed target\n",
    "    N_TEST = 500000     # Fixed target\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# ALPHA CONFIGURATION - ANDI TABLE 2 SPECIFICATION\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Different alpha ranges per model (step 0.1, stored with 2 decimal precision)\n",
    "# This follows ANDI Challenge Table 2 exactly\n",
    "\n",
    "ANDI_ALPHA_SPECS = {\n",
    "    0: np.round(np.arange(0.10, 1.01, 0.1), 2),   # ATTM: [0.10 to 1.00] → 10 values\n",
    "    1: np.round(np.arange(0.10, 1.01, 0.1), 2),   # CTRW: [0.10 to 1.00] → 10 values\n",
    "    2: np.round(np.arange(0.10, 1.91, 0.1), 2),   # FBM:  [0.10 to 1.90] → 19 values\n",
    "    3: np.round(np.arange(1.00, 1.91, 0.1), 2),   # LW:   [1.00 to 1.90] → 10 values\n",
    "    4: np.round(np.arange(0.10, 1.91, 0.1), 2),   # SBM:  [0.10 to 1.90] → 19 values\n",
    "}\n",
    "\n",
    "# Total unique (model, alpha) combinations\n",
    "N_MODEL_ALPHA_COMBINATIONS = sum(len(alphas) for alphas in ANDI_ALPHA_SPECS.values())  # 68\n",
    "\n",
    "# Model-specific alpha constraints (for validation)\n",
    "ALPHA_CONSTRAINTS = {\n",
    "    0: (0.10, 1.00),   # ATTM: sub-diffusive only\n",
    "    1: (0.10, 1.00),   # CTRW: sub-diffusive only\n",
    "    2: (0.10, 1.90),   # FBM: sub & super-diffusive\n",
    "    3: (1.00, 1.90),   # LW: super-diffusive only\n",
    "    4: (0.10, 1.90),   # SBM: full range\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# TRAJECTORY LENGTH CONFIGURATION\n",
    "# --------------------------------------------------------------------------- #\n",
    "# For TRAIN/VAL: Variable lengths with weighted distribution (Firbas approach)\n",
    "LENGTH_BINS = [\n",
    "    (10, 20), (21, 30), (31, 40), (41, 50),\n",
    "    (51, 100), (101, 200), (201, 300), (301, 400),\n",
    "    (401, 500), (501, 600), (601, 800), (801, 1000)\n",
    "]\n",
    "\n",
    "# Length distribution: Emphasize short trajectories (biologically relevant)\n",
    "LENGTH_BIN_WEIGHTS = np.array([\n",
    "    0.10, 0.10, 0.10, 0.10,  # Short (40%)\n",
    "    0.10, 0.10, 0.10, 0.10,  # Medium (40%)\n",
    "    0.05, 0.05, 0.05, 0.05   # Long (20%)\n",
    "])\n",
    "\n",
    "# For TEST: ANDI Table 2 fixed lengths\n",
    "ANDI_TEST_LENGTHS = [10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 600, 800, 1000]  # 13 values\n",
    "\n",
    "MAX_LENGTH = 1000  # For padding in HDF5\n",
    "\n",
    "# Dimensionality\n",
    "DIM = 1  # 1D trajectories (Firbas approach for benchmarking)\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# SNR CONFIGURATION - ANDI TABLE 2 SPECIFICATION\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Signal-to-Noise Ratio levels for test set\n",
    "ANDI_SNR_LEVELS = [0, 0]  # No noise (clean data)\n",
    "\n",
    "# For TRAIN/VAL: Clean data (no noise)\n",
    "# For TEST: Clean data (no noise)\n",
    "\n",
    "# Calculate total test permutations:\n",
    "# ATTM: 13 lengths × 2 SNR × 10 alpha = 260\n",
    "# CTRW: 13 lengths × 2 SNR × 10 alpha = 260\n",
    "# FBM:  13 lengths × 2 SNR × 19 alpha = 494\n",
    "# LW:   13 lengths × 2 SNR × 10 alpha = 260\n",
    "# SBM:  13 lengths × 2 SNR × 19 alpha = 494\n",
    "# TOTAL: 1,768 unique permutations\n",
    "\n",
    "N_TEST_PERMUTATIONS = (\n",
    "    len(ANDI_TEST_LENGTHS) * len(ANDI_SNR_LEVELS) *\n",
    "    (len(ANDI_ALPHA_SPECS[0]) + len(ANDI_ALPHA_SPECS[1]) +\n",
    "     len(ANDI_ALPHA_SPECS[2]) + len(ANDI_ALPHA_SPECS[3]) +\n",
    "     len(ANDI_ALPHA_SPECS[4]))\n",
    ")\n",
    "\n",
    "N_REPS_PER_TEST_PERMUTATION = N_TEST // N_TEST_PERMUTATIONS  # ~283 replications\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# TRAIN/VAL GENERATION STRATEGY\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Balanced generation: Equal trajectories per (model, alpha) combination\n",
    "N_TRAIN_VAL_TOTAL = N_TRAIN + N_VAL  # 1,500,000\n",
    "N_TRAJS_PER_MODEL_ALPHA = N_TRAIN_VAL_TOTAL // N_MODEL_ALPHA_COMBINATIONS  # ~22,058\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# GROUND TRUTH CALCULATION METHODS\n",
    "# --------------------------------------------------------------------------- #\n",
    "# H estimation methods:\n",
    "# - FBM: Use theoretical H = α/2 (exact)\n",
    "# - Others: Use R/S (Rescaled Range) analysis (Korabel et al.)\n",
    "\n",
    "# USE_THEORETICAL_H_FOR_FBM = True  # H = α/2 for FBM only # DEPRECATED: Using alpha directly from ANDI\n",
    "# USE_RS_ANALYSIS_FOR_OTHERS = True  # R/S for CTRW, ATTM, SBM, LW # DEPRECATED: Using alpha directly from ANDI\n",
    "\n",
    "# D estimation: MSD-based Langevin method (unbiased for any α)\n",
    "D_ESTIMATION_MAX_TAU = 50  # Maximum lag time for MSD calculation\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# PRINT CONFIGURATION SUMMARY\n",
    "# --------------------------------------------------------------------------- #\n",
    "print(\"=\" * 70)\n",
    "print(\"DATASET CONFIGURATION - ANDI TABLE 2 COMPLIANT\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Mode: {'TEST' if TEST_MODE else 'PRODUCTION'}\")\n",
    "print(f\"Total trajectories: {TOTAL_TRAJECTORIES:,}\")\n",
    "print(f\"  - Train: {N_TRAIN:,} ({N_TRAIN/TOTAL_TRAJECTORIES*100:.1f}%)\")\n",
    "print(f\"  - Val: {N_VAL:,} ({N_VAL/TOTAL_TRAJECTORIES*100:.1f}%)\")\n",
    "print(f\"  - Test: {N_TEST:,} ({N_TEST/TOTAL_TRAJECTORIES*100:.1f}%)\")\n",
    "print(f\"\\nModels: {list(MODEL_NAMES.values())}\")\n",
    "print()\n",
    "print(\"ANDI Table 2 Test Set Specifications:\")\n",
    "print(f\"  - Unique permutations: {N_TEST_PERMUTATIONS:,}\")\n",
    "print(f\"  - Replications per permutation: {N_REPS_PER_TEST_PERMUTATION}\")\n",
    "print(f\"  - Lengths: {len(ANDI_TEST_LENGTHS)} values {ANDI_TEST_LENGTHS}\")\n",
    "print(f\"  - SNR levels: {ANDI_SNR_LEVELS}\")\n",
    "print(f\"  - Alpha ranges:\")\n",
    "for model_id, model_name in MODEL_NAMES.items():\n",
    "    alphas = ANDI_ALPHA_SPECS[model_id]\n",
    "    print(f\"      {model_name}: [{alphas[0]:.2f} to {alphas[-1]:.2f}] ({len(alphas)} values)\")\n",
    "print()\n",
    "print(\"Train/Val Set Specifications:\")\n",
    "print(f\"  - Total (model, α) combinations: {N_MODEL_ALPHA_COMBINATIONS}\")\n",
    "print(f\"  - Trajectories per combination: ~{N_TRAJS_PER_MODEL_ALPHA:,}\")\n",
    "print(f\"  - Length sampling: Weighted distribution from bins\")\n",
    "print(f\"  - Noise: None (clean data)\")\n",
    "print()\n",
    "print(f\"Max length (padding): {MAX_LENGTH}\")\n",
    "print(f\"Dimensionality: {DIM}D\")\n",
    "print()\n",
    "print(\"Ground truth methods:\")\n",
    "print(\"  - H (FBM): Theoretical (H = α/2)\")\n",
    "print(\"  - H (others): R/S analysis\")\n",
    "print(\"  - D (all): Variance-based MSD\")\n",
    "print()\n",
    "print(\"Noise protocol:\")\n",
    "print(\"  - Train/Val: Clean data (no noise)\")\n",
    "print(f\"  - Test: Clean data (no noise)\")\n",
    "print()\n",
    "print(f\"Batch size: {BATCH_SIZE:,} trajectories\")\n",
    "print(f\"Random seed: {SEED}\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Save configuration for reference\n",
    "CONFIG = {\n",
    "    \"seed\": SEED,\n",
    "    \"test_mode\": TEST_MODE,\n",
    "    \"andi_table2_compliant\": True,\n",
    "    \"total_trajectories\": TOTAL_TRAJECTORIES,\n",
    "    \"split_counts\": {\"train\": N_TRAIN, \"val\": N_VAL, \"test\": N_TEST},\n",
    "    \"models\": {int(k): v for k, v in MODEL_NAMES.items()},\n",
    "    \"andi_alpha_specs\": {int(k): v.tolist() for k, v in ANDI_ALPHA_SPECS.items()},\n",
    "    \"alpha_constraints\": {int(k): v for k, v in ALPHA_CONSTRAINTS.items()},\n",
    "    \"test_config\": {\n",
    "        \"lengths\": ANDI_TEST_LENGTHS,\n",
    "        \"snr_levels\": ANDI_SNR_LEVELS,\n",
    "        \"n_permutations\": N_TEST_PERMUTATIONS,\n",
    "        \"n_reps_per_permutation\": N_REPS_PER_TEST_PERMUTATION\n",
    "    },\n",
    "    \"train_val_config\": {\n",
    "        \"n_model_alpha_combinations\": N_MODEL_ALPHA_COMBINATIONS,\n",
    "        \"n_trajs_per_combination\": N_TRAJS_PER_MODEL_ALPHA,\n",
    "        \"length_bins\": LENGTH_BINS,\n",
    "        \"length_bin_weights\": LENGTH_BIN_WEIGHTS.tolist()\n",
    "    },\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"dimension\": DIM,\n",
    "    \"ground_truth_methods\": {\n",
    "        \"H_FBM\": \"theoretical\",\n",
    "        \"H_others\": \"R/S_analysis\",\n",
    "        \"D_all\": \"variance_based_MSD\"\n",
    "    },\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6fa3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HELPER FUNCTIONS LOADED\n",
      "======================================================================\n",
      "✓ [DEPRECATED] estimate_H_rescaled_range() - Use alpha directly from ANDI\n",
      "✓ [DEPRECATED] estimate_H_theoretical() - Use alpha directly from ANDI\n",
      "✓ [DEPRECATED] estimate_D_from_variance() - Biased for α≠1\n",
      "✓ estimate_D_from_msd() - MSD-based Langevin estimator (ACTIVE)\n",
      "✓ compute_dual_preprocessing() - Scaled + Raw displacements\n",
      "✓ Trajectory sampling helpers\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for ground truth calculation\n",
    "\n",
    "def estimate_H_rescaled_range(trajectory, min_window=8):\n",
    "    \"\"\"\n",
    "    Estimate Hurst exponent using R/S analysis.\n",
    "    Used for non-FBM models (CTRW, ATTM, SBM, LW).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        traj = np.squeeze(trajectory)\n",
    "        if traj.ndim != 1:\n",
    "            traj = traj[:, 0]\n",
    "        \n",
    "        T = len(traj)\n",
    "        if T < 2 * min_window:\n",
    "            return np.nan\n",
    "        \n",
    "        H, c, data = hurst_module.compute_Hc(traj, kind='random_walk', simplified=False)\n",
    "        \n",
    "        if not (0 <= H <= 1):\n",
    "            H_simple, _, _ = hurst_module.compute_Hc(traj, kind='random_walk', simplified=True)\n",
    "            H = H_simple if (0 <= H_simple <= 1) else np.nan\n",
    "        \n",
    "        return float(H)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def estimate_H_theoretical(alpha, model_id):\n",
    "    \"\"\"Calculate theoretical H for FBM: H = α/2\"\"\"\n",
    "    if model_id == 2:\n",
    "        return alpha / 2.0\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def estimate_D_from_variance(trajectory, delta_t=1.0):\n",
    "    \"\"\"\n",
    "    Estimate D from displacement variance: D = Var(Δx) / (2 * Δt)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        traj = np.squeeze(trajectory)\n",
    "        if traj.ndim != 1:\n",
    "            traj = traj[:, 0]\n",
    "        \n",
    "        # Calculate displacements\n",
    "        delta_x = np.diff(traj)\n",
    "        \n",
    "        if len(delta_x) < 2:\n",
    "            return np.nan\n",
    "        \n",
    "        # Variance of displacements\n",
    "        sigma_sq = np.var(delta_x)\n",
    "        \n",
    "        # D = σ² / (2Δt)\n",
    "        D = sigma_sq / (2.0 * delta_t)\n",
    "        \n",
    "        return float(D) if D > 0 else np.nan\n",
    "    \n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def estimate_D_from_msd(trajectory, alpha, max_tau=50, delta_t=1.0):\n",
    "    \"\"\"\n",
    "    Estimate D from MSD curve fitting: MSD(τ) = 2D τ^α\n",
    "    Uses log-linear regression on MSD vs lag time.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        traj = np.squeeze(trajectory)\n",
    "        if traj.ndim != 1:\n",
    "            traj = traj[:, 0]\n",
    "        \n",
    "        T = len(traj)\n",
    "        if T < 10:\n",
    "            return np.nan\n",
    "        \n",
    "        # Limit max_tau to available data\n",
    "        max_tau = min(max_tau, T // 4)\n",
    "        if max_tau < 3:\n",
    "            return np.nan\n",
    "        \n",
    "        # Calculate MSD for different lag times\n",
    "        taus = np.arange(1, max_tau + 1)\n",
    "        msds = []\n",
    "        \n",
    "        for tau in taus:\n",
    "            displacements = traj[tau:] - traj[:-tau]\n",
    "            if len(displacements) > 0:\n",
    "                msds.append(np.mean(displacements ** 2))\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        msds = np.array(msds)\n",
    "        taus = taus[:len(msds)]\n",
    "        \n",
    "        # Filter valid MSD values\n",
    "        valid = np.isfinite(msds) & (msds > 0)\n",
    "        if valid.sum() < 3:\n",
    "            return np.nan\n",
    "        \n",
    "        # MSD(τ) = 2D τ^α → log(MSD) = log(2D) + α log(τ)\n",
    "        log_msd = np.log(msds[valid])\n",
    "        log_tau = np.log(taus[valid] * delta_t)\n",
    "        \n",
    "        # Intercept = log(2D) → D = exp(intercept) / 2\n",
    "        intercept = np.mean(log_msd - alpha * log_tau)\n",
    "        D = 0.5 * np.exp(intercept)\n",
    "        \n",
    "        return float(D) if D > 0 else np.nan\n",
    "    \n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def compute_dual_preprocessing(trajectory):\n",
    "    \"\"\"\n",
    "    Generate dual displacement arrays for Dual CNN architecture.\n",
    "    Returns raw displacements (D-branch) and scaled displacements (alpha-branch).\n",
    "    \"\"\"\n",
    "    traj = np.squeeze(trajectory)\n",
    "    if traj.ndim != 1:\n",
    "        traj = traj[:, 0]\n",
    "    \n",
    "    delta_x_raw = np.diff(traj)\n",
    "    traj_range = np.ptp(traj)\n",
    "    \n",
    "    if traj_range > 0:\n",
    "        delta_x_scaled = delta_x_raw / traj_range\n",
    "    else:\n",
    "        delta_x_scaled = np.zeros_like(delta_x_raw)\n",
    "        traj_range = 1.0\n",
    "    \n",
    "    return delta_x_raw, delta_x_scaled, traj_range\n",
    "\n",
    "\n",
    "def sample_trajectory_length(length_bin_idx, rng):\n",
    "    \"\"\"Sample trajectory length from given bin.\"\"\"\n",
    "    min_len, max_len = LENGTH_BINS[length_bin_idx]\n",
    "    return rng.randint(min_len, max_len + 1)\n",
    "\n",
    "\n",
    "def sample_valid_alpha_for_model(model_id, rng):\n",
    "    \"\"\"Sample valid alpha for model from discrete values.\"\"\"\n",
    "    alpha_min, alpha_max = ALPHA_CONSTRAINTS[model_id]\n",
    "    valid_alphas = ALPHA_VALUES[(ALPHA_VALUES >= alpha_min) & (ALPHA_VALUES <= alpha_max)]\n",
    "    \n",
    "    if len(valid_alphas) == 0:\n",
    "        raise ValueError(f\"No valid alpha values for model {model_id}\")\n",
    "    \n",
    "    return rng.choice(valid_alphas)\n",
    "\n",
    "\n",
    "def assign_length_bin(length):\n",
    "    \"\"\"Assign trajectory to length bin.\"\"\"\n",
    "    for bin_idx, (min_len, max_len) in enumerate(LENGTH_BINS):\n",
    "        if min_len <= length <= max_len:\n",
    "            return bin_idx\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e34ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SNR AND ANDI TABLE 2 HELPER FUNCTIONS LOADED\n",
      "======================================================================\n",
      "✓ add_noise_snr() - Add SNR-based Gaussian noise to trajectories\n",
      "✓ verify_snr() - Verify actual SNR achieved\n",
      "✓ generate_andi_test_permutations() - Generate all test set permutations\n",
      "✓ sample_alpha_for_model_balanced() - Sample alpha for train/val\n",
      "✓ get_model_alpha_pairs() - Get all (model, α) pairs for balanced generation\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SNR and ANDI test set helper functions\n",
    "\n",
    "def add_noise_snr(trajectory, snr_target, random_state=None):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to achieve target SNR.\n",
    "    SNR = σ_signal / σ_noise\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    \n",
    "    # Calculate signal standard deviation\n",
    "    sigma_signal = np.std(trajectory)\n",
    "    \n",
    "    # Calculate required noise level to achieve target SNR\n",
    "    sigma_noise = sigma_signal / snr_target\n",
    "    \n",
    "    # Generate Gaussian noise\n",
    "    noise = rng.normal(0, sigma_noise, size=trajectory.shape)\n",
    "    \n",
    "    # Add noise to trajectory\n",
    "    noisy_trajectory = trajectory + noise\n",
    "    \n",
    "    return noisy_trajectory, sigma_noise\n",
    "\n",
    "\n",
    "def verify_snr(clean_trajectory, noisy_trajectory):\n",
    "    \"\"\"Verify actual SNR achieved after adding noise.\"\"\"\n",
    "    sigma_signal = np.std(clean_trajectory)\n",
    "    noise = noisy_trajectory - clean_trajectory\n",
    "    sigma_noise = np.std(noise)\n",
    "    \n",
    "    if sigma_noise == 0:\n",
    "        return np.inf\n",
    "    \n",
    "    snr_actual = sigma_signal / sigma_noise\n",
    "    return snr_actual\n",
    "\n",
    "\n",
    "def generate_andi_test_permutations(verbose=True):\n",
    "    \"\"\"\n",
    "    Generate all permutations for ANDI Table 2 test set.\n",
    "    Each permutation: (model, length, snr, alpha)\n",
    "    \"\"\"\n",
    "    permutations = []\n",
    "    \n",
    "    # ANDI Table 2 specifications\n",
    "    lengths = ANDI_TEST_LENGTHS\n",
    "    snr_levels = ANDI_SNR_LEVELS\n",
    "    alpha_specs = ANDI_ALPHA_SPECS\n",
    "    \n",
    "    # Generate all permutations\n",
    "    for model_id in range(5):\n",
    "        for length in lengths:\n",
    "            for snr in snr_levels:\n",
    "                for alpha in alpha_specs[model_id]:\n",
    "                    permutations.append({\n",
    "                        'model_id': model_id,\n",
    "                        'length': length,\n",
    "                        'snr': snr,\n",
    "                        'alpha': round(float(alpha), 2)  # Ensure 2 decimals\n",
    "                    })\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Generated {len(permutations):,} test permutations\")\n",
    "        for model_id in range(5):\n",
    "            model_perms = [p for p in permutations if p['model_id'] == model_id]\n",
    "            n_alphas = len(alpha_specs[model_id])\n",
    "            n_lengths = len(lengths)\n",
    "            n_snrs = len(snr_levels)\n",
    "            expected = n_alphas * n_lengths * n_snrs\n",
    "            print(f\"  {MODEL_NAMES[model_id]}: {len(model_perms):,} \"\n",
    "                  f\"({n_alphas} α × {n_lengths} T × {n_snrs} SNR = {expected})\")\n",
    "    \n",
    "    return permutations\n",
    "\n",
    "\n",
    "def sample_alpha_for_model_balanced(model_id, random_state=None):\n",
    "    \"\"\"Sample alpha for model from ANDI-compliant ranges.\"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    alpha_values = ANDI_ALPHA_SPECS[model_id]\n",
    "    alpha = rng.choice(alpha_values)\n",
    "    return round(float(alpha), 2)\n",
    "\n",
    "\n",
    "def get_model_alpha_pairs():\n",
    "    \"\"\"Get all (model_id, alpha) pairs for balanced train/val generation.\"\"\"\n",
    "    pairs = []\n",
    "    for model_id in range(5):\n",
    "        for alpha in ANDI_ALPHA_SPECS[model_id]:\n",
    "            pairs.append((model_id, round(float(alpha), 2)))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e89f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GENERATING ANDI TABLE 2 TEST SET\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ANDI TABLE 2 TEST PERMUTATIONS GENERATED\n",
      "======================================================================\n",
      "Total unique permutations: 1,768\n",
      "\n",
      "Breakdown by model:\n",
      "  ATTM: 260 (10 α × 13 T × 2 SNR = 260)\n",
      "  CTRW: 260 (10 α × 13 T × 2 SNR = 260)\n",
      "  FBM: 494 (19 α × 13 T × 2 SNR = 494)\n",
      "  LW: 260 (10 α × 13 T × 2 SNR = 260)\n",
      "  SBM: 494 (19 α × 13 T × 2 SNR = 494)\n",
      "======================================================================\n",
      "\n",
      "Generating 100,000 test trajectories...\n",
      "  - 1,768 unique permutations\n",
      "  - 56 replications per permutation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Permutations: 100%|██████████| 1768/1768 [01:05<00:00, 27.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST SET GENERATION COMPLETE\n",
      "======================================================================\n",
      "Total trajectories generated: 99,008\n",
      "Time elapsed: 66.41 seconds (1.11 minutes)\n",
      "\n",
      "Data shapes:\n",
      "  Trajectories: (99008, 1000, 1)\n",
      "  Displacements (raw): (99008, 999, 1)\n",
      "  Displacements (scaled): (99008, 999, 1)\n",
      "  Metadata: (99008, 6)\n",
      "\n",
      "Metadata summary:\n",
      "  Models: {2: 27664, 4: 27664, 0: 14560, 1: 14560, 3: 14560}\n",
      "  Lengths: [9, 19, 29, 39, 49, 99, 199, 299, 399, 499, 599, 799, 999]\n",
      "  SNR levels: {0: 99008}\n",
      "\n",
      "           model_id         alpha             D        length      snr  \\\n",
      "count  99008.000000  99008.000000  8.887700e+04  99008.000000  99008.0   \n",
      "mean       2.264706      0.933824  4.349667e+00    310.538462      0.0   \n",
      "std        1.389227      0.537593  1.560327e+01    315.470625      0.0   \n",
      "min        0.000000      0.100000  1.876581e-60      9.000000      0.0   \n",
      "25%        1.000000      0.500000  9.997586e-03     39.000000      0.0   \n",
      "50%        2.000000      0.900000  1.075369e-01    199.000000      0.0   \n",
      "75%        4.000000      1.400000  8.366643e-01    499.000000      0.0   \n",
      "max        4.000000      1.900000  4.861440e+02    999.000000      0.0   \n",
      "\n",
      "       sigma_noise  \n",
      "count      99008.0  \n",
      "mean           0.0  \n",
      "std            0.0  \n",
      "min            0.0  \n",
      "25%            0.0  \n",
      "50%            0.0  \n",
      "75%            0.0  \n",
      "max            0.0  \n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate ANDI Table 2 test set\n",
    "\n",
    "import time as time_module\n",
    "\n",
    "test_permutations = generate_andi_test_permutations(verbose=True)\n",
    "\n",
    "# Storage for test set data\n",
    "test_trajectories = []\n",
    "test_displacements_raw = []\n",
    "test_displacements_scaled = []\n",
    "test_metadata = []\n",
    "\n",
    "# Progress tracking\n",
    "total_test_trajs = N_TEST\n",
    "trajs_generated = 0\n",
    "start_time = time_module.time()\n",
    "\n",
    "print(f\"Generating {total_test_trajs:,} test trajectories...\")\n",
    "print(f\"  - {len(test_permutations):,} unique permutations\")\n",
    "print(f\"  - {N_REPS_PER_TEST_PERMUTATION} replications per permutation\")\n",
    "print()\n",
    "\n",
    "# Generate test set\n",
    "batch_progress = 0\n",
    "for perm_idx, perm in enumerate(tqdm(test_permutations, desc=\"Test Permutations\")):\n",
    "    model_id = perm['model_id']\n",
    "    length = perm['length']\n",
    "    snr = perm['snr']\n",
    "    alpha = perm['alpha']  # Alpha comes directly from ANDI\n",
    "    \n",
    "    # Generate N replications for this permutation\n",
    "    for rep in range(N_REPS_PER_TEST_PERMUTATION):\n",
    "        try:\n",
    "            # Generate CLEAN trajectory first\n",
    "            traj_clean = andi.create_dataset(\n",
    "                T=length,\n",
    "                N_models=1,\n",
    "                exponents=[alpha],\n",
    "                models=[model_id],\n",
    "                dimension=DIM\n",
    "            )\n",
    "            \n",
    "            # Extract trajectory positions (skip first 3 metadata elements)\n",
    "            traj_clean = traj_clean[0][3:]\n",
    "            \n",
    "            # Ensure correct shape [T, 1]\n",
    "            if traj_clean.ndim == 1:\n",
    "                traj_clean = traj_clean.reshape(-1, 1)\n",
    "            \n",
    "            actual_length = len(traj_clean)\n",
    "            \n",
    "            # =================================================================\n",
    "            # GROUND TRUTH CALCULATION (on CLEAN trajectory)\n",
    "            # =================================================================\n",
    "            \n",
    "            # Alpha: Already known from ANDI generation (no estimation needed)\n",
    "            # D: Estimate using MSD-based Langevin method (unbiased for any α)\n",
    "            D = estimate_D_from_msd(traj_clean, alpha)\n",
    "            \n",
    "            # =================================================================\n",
    "            # NO NOISE - Use clean trajectory\n",
    "            # =================================================================\n",
    "            \n",
    "            traj_noisy = traj_clean  # No noise added\n",
    "            sigma_noise = 0.0  # No noise standard deviation\n",
    "            \n",
    "            # =================================================================\n",
    "            # Store test trajectory and metadata\n",
    "            # =================================================================\n",
    "            \n",
    "            # Calculate raw displacements: dx = x[t+1] - x[t]\n",
    "            displacements_raw = np.diff(traj_noisy, axis=0)\n",
    "            \n",
    "            # Calculate scaled displacements: dx / sqrt(dt)\n",
    "            # For unit time step (dt=1): dx / sqrt(1) = dx\n",
    "            displacements_scaled = displacements_raw / np.sqrt(1.0)\n",
    "            \n",
    "            # Pad trajectory to MAX_LENGTH\n",
    "            traj_padded = np.zeros((MAX_LENGTH, DIM), dtype=np.float32)\n",
    "            traj_padded[:actual_length] = traj_noisy\n",
    "            \n",
    "            # Pad displacements to MAX_LENGTH-1\n",
    "            disp_raw_padded = np.zeros((MAX_LENGTH - 1, DIM), dtype=np.float32)\n",
    "            disp_raw_padded[:actual_length-1] = displacements_raw\n",
    "            \n",
    "            disp_scaled_padded = np.zeros((MAX_LENGTH - 1, DIM), dtype=np.float32)\n",
    "            disp_scaled_padded[:actual_length-1] = displacements_scaled\n",
    "            \n",
    "            # Store data\n",
    "            test_trajectories.append(traj_padded)\n",
    "            test_displacements_raw.append(disp_raw_padded)\n",
    "            test_displacements_scaled.append(disp_scaled_padded)\n",
    "            \n",
    "            # Store metadata (alpha from ANDI, D from MSD estimation, SNR fields)\n",
    "            test_metadata.append({\n",
    "                'model_id': model_id,\n",
    "                'alpha': alpha,  # From ANDI generation\n",
    "                'D': D,          # From MSD estimation\n",
    "                'length': actual_length,\n",
    "                'snr': snr,\n",
    "                'sigma_noise': sigma_noise\n",
    "            })\n",
    "            \n",
    "            trajs_generated += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError generating trajectory for \"\n",
    "                  f\"{MODEL_NAMES[model_id]}, α={alpha:.2f}, T={length}, SNR={snr}\")\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "\n",
    "# Convert to arrays\n",
    "test_trajectories = np.array(test_trajectories, dtype=np.float32)\n",
    "test_displacements_raw = np.array(test_displacements_raw, dtype=np.float32)\n",
    "test_displacements_scaled = np.array(test_displacements_scaled, dtype=np.float32)\n",
    "\n",
    "# Convert metadata to DataFrame\n",
    "df_test_metadata = pd.DataFrame(test_metadata)\n",
    "\n",
    "# Summary\n",
    "elapsed_time = time_module.time() - start_time\n",
    "print(f\"\\nTest set complete: {len(test_trajectories):,} trajectories in {elapsed_time/60:.2f} min\")\n",
    "print(f\"Shapes: trajectories {test_trajectories.shape}, metadata {df_test_metadata.shape}\")\n",
    "print(f\"Models: {df_test_metadata['model_id'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e96df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GENERATING BALANCED TRAIN/VAL SET\n",
      "======================================================================\n",
      "\n",
      "Total (model, α) combinations: 68\n",
      "Trajectories per combination: 0\n",
      "Total train/val trajectories: 0\n",
      "\n",
      "Generating 0 train/val trajectories...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Model, α) Pairs: 100%|██████████| 68/68 [00:00<00:00, 953888.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAIN/VAL SET GENERATION COMPLETE\n",
      "======================================================================\n",
      "✓ Generated: 0 trajectories\n",
      "✓ Time elapsed: 0.0 minutes\n",
      "✓ Average rate: 0.0 trajectories/second\n",
      "\n",
      "Train/Val set distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 156\u001b[39m\n\u001b[32m    153\u001b[39m df_train_val_metadata = pd.DataFrame(train_val_metadata)\n\u001b[32m    155\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrain/Val set distribution:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Models: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf_train_val_metadata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_name\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.value_counts().to_dict()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    157\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  SNR levels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_train_val_metadata[\u001b[33m'\u001b[39m\u001b[33msnr\u001b[39m\u001b[33m'\u001b[39m].value_counts().to_dict()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    158\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Length range: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_train_val_metadata[\u001b[33m'\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m'\u001b[39m].min()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_train_val_metadata[\u001b[33m'\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m'\u001b[39m].max()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ERP_Shrey/venv/lib/python3.11/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ERP_Shrey/venv/lib/python3.11/site-packages/pandas/core/indexes/range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'model_name'"
     ]
    }
   ],
   "source": [
    "# Generate balanced train/val set\n",
    "\n",
    "import time as time_module\n",
    "\n",
    "model_alpha_pairs = get_model_alpha_pairs()\n",
    "print(f\"Generating {N_TRAIN + N_VAL:,} train/val trajectories from {len(model_alpha_pairs)} (model, α) pairs\")\n",
    "\n",
    "# Storage for train/val data\n",
    "train_val_trajectories = []\n",
    "train_val_displacements_raw = []\n",
    "train_val_displacements_scaled = []\n",
    "train_val_metadata = []\n",
    "\n",
    "# Progress tracking\n",
    "trajs_generated = 0\n",
    "start_time = time_module.time()\n",
    "\n",
    "print(f\"Generating {N_TRAIN + N_VAL:,} train/val trajectories...\")\n",
    "print()\n",
    "\n",
    "# Generate trajectories for each (model, alpha) pair\n",
    "for pair_idx, (model_id, alpha) in enumerate(tqdm(model_alpha_pairs, desc=\"(Model, α) Pairs\")):\n",
    "    \n",
    "    # Generate N_TRAJS_PER_MODEL_ALPHA trajectories for this (model, alpha) pair\n",
    "    for rep in range(N_TRAJS_PER_MODEL_ALPHA):\n",
    "        try:\n",
    "            # Sample trajectory length from weighted distribution\n",
    "            length_bin_idx = np.random.choice(len(LENGTH_BINS), p=LENGTH_BIN_WEIGHTS)\n",
    "            length_min, length_max = LENGTH_BINS[length_bin_idx]\n",
    "            length = np.random.randint(length_min, length_max + 1)\n",
    "            \n",
    "            # Generate CLEAN trajectory\n",
    "            traj_clean = andi.create_dataset(\n",
    "                T=length,\n",
    "                N_models=1,\n",
    "                exponents=[alpha],\n",
    "                models=[model_id],\n",
    "                dimension=DIM\n",
    "            )\n",
    "            \n",
    "            # Extract trajectory positions\n",
    "            traj_clean = traj_clean[0][3:]\n",
    "            \n",
    "            # Ensure correct shape [T, 1]\n",
    "            if traj_clean.ndim == 1:\n",
    "                traj_clean = traj_clean.reshape(-1, 1)\n",
    "            \n",
    "            actual_length = len(traj_clean)\n",
    "            \n",
    "            # =================================================================\n",
    "            # GROUND TRUTH CALCULATION (on CLEAN trajectory)\n",
    "            # =================================================================\n",
    "            \n",
    "            # Alpha: from ANDI generation (ground truth)\n",
    "            # D: MSD-based Langevin estimator (unbiased for any α)\n",
    "            D = estimate_D_from_msd(traj_clean, alpha)\n",
    "            \n",
    "            # =================================================================\n",
    "            # NO NOISE - Use clean trajectory\n",
    "            # =================================================================\n",
    "            \n",
    "            snr = 0  # Indicator for no noise\n",
    "            traj_noisy = traj_clean  # No noise added\n",
    "            sigma_noise = 0.0  # No noise standard deviation\n",
    "            \n",
    "            # =================================================================\n",
    "            # Calculate displacements and pad data\n",
    "            # =================================================================\n",
    "            \n",
    "            # Calculate raw displacements: dx = x[t+1] - x[t]\n",
    "            displacements_raw = np.diff(traj_noisy, axis=0)\n",
    "            \n",
    "            # Calculate scaled displacements: dx / sqrt(dt)\n",
    "            displacements_scaled = displacements_raw / np.sqrt(1.0)\n",
    "            \n",
    "            # Pad trajectory to MAX_LENGTH\n",
    "            traj_padded = np.zeros((MAX_LENGTH, DIM), dtype=np.float32)\n",
    "            traj_padded[:actual_length] = traj_noisy\n",
    "            \n",
    "            # Pad displacements\n",
    "            disp_raw_padded = np.zeros((MAX_LENGTH - 1, DIM), dtype=np.float32)\n",
    "            disp_raw_padded[:actual_length-1] = displacements_raw\n",
    "            \n",
    "            disp_scaled_padded = np.zeros((MAX_LENGTH - 1, DIM), dtype=np.float32)\n",
    "            disp_scaled_padded[:actual_length-1] = displacements_scaled\n",
    "            \n",
    "            # Store data\n",
    "            train_val_trajectories.append(traj_padded)\n",
    "            train_val_displacements_raw.append(disp_raw_padded)\n",
    "            train_val_displacements_scaled.append(disp_scaled_padded)\n",
    "            \n",
    "            # Store metadata (alpha from ANDI, D from MSD estimation, SNR fields)\n",
    "            train_val_metadata.append({\n",
    "                'traj_id': trajs_generated,\n",
    "                'model_id': model_id,\n",
    "                'model_name': MODEL_NAMES[model_id],\n",
    "                'alpha': alpha,\n",
    "                'D': D,\n",
    "                'length': actual_length,\n",
    "                'snr': snr,\n",
    "                'sigma_noise': sigma_noise\n",
    "            })\n",
    "            \n",
    "            trajs_generated += 1\n",
    "            \n",
    "            # Progress update every 10K trajectories\n",
    "            if trajs_generated % 10_000 == 0:\n",
    "                elapsed = time_module.time() - start_time\n",
    "                rate = trajs_generated / elapsed\n",
    "                remaining = ((N_TRAIN + N_VAL) - trajs_generated) / rate\n",
    "                print(f\"  Generated {trajs_generated:,}/{N_TRAIN + N_VAL:,} \"\n",
    "                      f\"({trajs_generated/(N_TRAIN + N_VAL)*100:.1f}%) - \"\n",
    "                      f\"Rate: {rate:.0f} traj/s - \"\n",
    "                      f\"ETA: {remaining/60:.1f} min\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠ Warning: Failed to generate trajectory for \"\n",
    "                  f\"{MODEL_NAMES[model_id]}, α={alpha:.2f}, T={length}\")\n",
    "            print(f\"  Error: {e}\")\n",
    "            continue\n",
    "\n",
    "# Convert to arrays\n",
    "train_val_trajectories = np.array(train_val_trajectories, dtype=np.float32)\n",
    "train_val_displacements_raw = np.array(train_val_displacements_raw, dtype=np.float32)\n",
    "train_val_displacements_scaled = np.array(train_val_displacements_scaled, dtype=np.float32)\n",
    "\n",
    "# Summary\n",
    "elapsed_total = time_module.time() - start_time\n",
    "df_train_val_metadata = pd.DataFrame(train_val_metadata)\n",
    "\n",
    "print(f\"\\nTrain/val set complete: {trajs_generated:,} trajectories in {elapsed_total/60:.1f} min\")\n",
    "print(f\"Rate: {trajs_generated/elapsed_total:.1f} traj/s\")\n",
    "print(f\"Models: {df_train_val_metadata['model_name'].value_counts().to_dict()}\")\n",
    "print(f\"Length range: [{df_train_val_metadata['length'].min()}, {df_train_val_metadata['length'].max()}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA QUALITY VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Total trajectories: 99,688\n",
      "\n",
      "1. ALPHA COVERAGE CHECK\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ATTM (model_id=0):\n",
      "  Expected alpha range: [0.10, 1.00] (10 values)\n",
      "  Actual alpha range:   [0.10, 1.00] (10 values)\n",
      "  Trajectories: 14,660\n",
      "  ✓ All expected alphas present\n",
      "\n",
      "CTRW (model_id=1):\n",
      "  Expected alpha range: [0.10, 1.00] (10 values)\n",
      "  Actual alpha range:   [0.10, 1.00] (10 values)\n",
      "  Trajectories: 14,660\n",
      "  ✓ All expected alphas present\n",
      "\n",
      "FBM (model_id=2):\n",
      "  Expected alpha range: [0.10, 1.90] (19 values)\n",
      "  Actual alpha range:   [0.10, 1.90] (19 values)\n",
      "  Trajectories: 27,854\n",
      "  ✓ All expected alphas present\n",
      "\n",
      "LW (model_id=3):\n",
      "  Expected alpha range: [1.00, 1.90] (10 values)\n",
      "  Actual alpha range:   [1.00, 1.90] (10 values)\n",
      "  Trajectories: 14,660\n",
      "  ✓ All expected alphas present\n",
      "\n",
      "SBM (model_id=4):\n",
      "  Expected alpha range: [0.10, 1.90] (19 values)\n",
      "  Actual alpha range:   [0.10, 1.90] (19 values)\n",
      "  Trajectories: 27,854\n",
      "  ✓ All expected alphas present\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "2. MODEL BALANCE CHECK\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Trajectories per model:\n",
      "  FBM       :   27,854 (27.94%)\n",
      "  SBM       :   27,854 (27.94%)\n",
      "  ATTM      :   14,660 (14.71%)\n",
      "  CTRW      :   14,660 (14.71%)\n",
      "  LW        :   14,660 (14.71%)\n",
      "\n",
      "Imbalance: 13.24%\n",
      "  ⚠️  WARNING: Models are imbalanced (>13.2% difference)\n",
      "\n",
      "======================================================================\n",
      "⚠️  VALIDATION FAILED: Gaps or issues found in dataset coverage\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA QUALITY VALIDATION: Check for gaps in alpha and model coverage\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA QUALITY VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Combine all metadata\n",
    "df_all_metadata = pd.concat([\n",
    "    df_test_metadata,\n",
    "    df_train_val_metadata\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f\"Total trajectories: {len(df_all_metadata):,}\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CHECK ALPHA COVERAGE PER MODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"1. ALPHA COVERAGE CHECK\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "all_gaps_found = False\n",
    "\n",
    "for model_id, model_name in MODEL_NAMES.items():\n",
    "    # Get expected alphas for this model\n",
    "    expected_alphas = set(ANDI_ALPHA_SPECS[model_id])\n",
    "    \n",
    "    # Get actual alphas in dataset\n",
    "    model_data = df_all_metadata[df_all_metadata['model_id'] == model_id]\n",
    "    actual_alphas = set(np.round(model_data['alpha'].unique(), 2))\n",
    "    \n",
    "    # Find gaps\n",
    "    missing_alphas = expected_alphas - actual_alphas\n",
    "    extra_alphas = actual_alphas - expected_alphas\n",
    "    \n",
    "    # Report\n",
    "    print(f\"\\n{model_name} (model_id={model_id}):\")\n",
    "    print(f\"  Expected alpha range: [{min(expected_alphas):.2f}, {max(expected_alphas):.2f}] ({len(expected_alphas)} values)\")\n",
    "    print(f\"  Actual alpha range:   [{min(actual_alphas):.2f}, {max(actual_alphas):.2f}] ({len(actual_alphas)} values)\")\n",
    "    print(f\"  Trajectories: {len(model_data):,}\")\n",
    "    \n",
    "    if missing_alphas:\n",
    "        print(f\"  ⚠️  MISSING ALPHAS: {sorted(missing_alphas)}\")\n",
    "        all_gaps_found = True\n",
    "    else:\n",
    "        print(f\"  ✓ All expected alphas present\")\n",
    "    \n",
    "    if extra_alphas:\n",
    "        print(f\"  ⚠️  EXTRA ALPHAS (not in ANDI spec): {sorted(extra_alphas)}\")\n",
    "        all_gaps_found = True\n",
    "\n",
    "print()\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. CHECK MODEL BALANCE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n2. MODEL BALANCE CHECK\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "model_counts = df_all_metadata['model_name'].value_counts()\n",
    "print(\"\\nTrajectories per model:\")\n",
    "for model_name, count in model_counts.items():\n",
    "    percentage = 100 * count / len(df_all_metadata)\n",
    "    print(f\"  {model_name:10s}: {count:8,} ({percentage:5.2f}%)\")\n",
    "\n",
    "# Check if balanced (should be roughly 20% each)\n",
    "min_percentage = 100 * model_counts.min() / len(df_all_metadata)\n",
    "max_percentage = 100 * model_counts.max() / len(df_all_metadata)\n",
    "imbalance = max_percentage - min_percentage\n",
    "\n",
    "print(f\"\\nImbalance: {imbalance:.2f}%\")\n",
    "if imbalance > 5.0:\n",
    "    print(f\"  ⚠️  WARNING: Models are imbalanced (>{imbalance:.1f}% difference)\")\n",
    "    all_gaps_found = True\n",
    "else:\n",
    "    print(f\"  ✓ Models are reasonably balanced\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL VERDICT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if all_gaps_found:\n",
    "    print(\"⚠️  VALIDATION FAILED: Gaps or issues found in dataset coverage\")\n",
    "else:\n",
    "    print(\"✅ VALIDATION PASSED: Dataset has complete coverage\")\n",
    "print(\"=\" * 70)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jtbwh82ofk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SPLITTING TRAIN/VAL SET\n",
      "======================================================================\n",
      "\n",
      "Test set: 99,008 trajectories (already separated)\n",
      "Train/Val set: 0 trajectories (needs splitting)\n",
      "Target split: 0 train / 0 val\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Create stratification labels (model_id + alpha combination)\u001b[39;00m\n\u001b[32m     21\u001b[39m df_train_val_metadata[\u001b[33m'\u001b[39m\u001b[33mstrat_label\u001b[39m\u001b[33m'\u001b[39m] = (\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[43mdf_train_val_metadata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.astype(\u001b[38;5;28mstr\u001b[39m) + \u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m + \n\u001b[32m     23\u001b[39m     df_train_val_metadata[\u001b[33m'\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m     24\u001b[39m )\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Split train/val with stratification\u001b[39;00m\n\u001b[32m     27\u001b[39m train_idx, val_idx = train_test_split(\n\u001b[32m     28\u001b[39m     np.arange(\u001b[38;5;28mlen\u001b[39m(df_train_val_metadata)),\n\u001b[32m     29\u001b[39m     test_size=N_VAL / (N_TRAIN + N_VAL),  \u001b[38;5;66;03m# 10% for val\u001b[39;00m\n\u001b[32m     30\u001b[39m     random_state=SEED,\n\u001b[32m     31\u001b[39m     stratify=df_train_val_metadata[\u001b[33m'\u001b[39m\u001b[33mstrat_label\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     32\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ERP_Shrey/venv/lib/python3.11/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ERP_Shrey/venv/lib/python3.11/site-packages/pandas/core/indexes/range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'model_id'"
     ]
    }
   ],
   "source": [
    "# Cell 7: Split Train/Val Set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# =============================================================================\n",
    "# STRATIFIED TRAIN/VAL SPLIT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SPLITTING TRAIN/VAL SET\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Test set is already separate, we only need to split train/val\n",
    "print(f\"Test set: {len(test_metadata):,} trajectories (already separated)\")\n",
    "print(f\"Train/Val set: {len(train_val_metadata):,} trajectories (needs splitting)\")\n",
    "print(f\"Target split: {N_TRAIN:,} train / {N_VAL:,} val\")\n",
    "print()\n",
    "\n",
    "# Create stratification labels (model_id + alpha combination)\n",
    "df_train_val_metadata['strat_label'] = (\n",
    "    df_train_val_metadata['model_id'].astype(str) + \"_\" + \n",
    "    df_train_val_metadata['alpha'].astype(str)\n",
    ")\n",
    "\n",
    "# Split train/val with stratification\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(df_train_val_metadata)),\n",
    "    test_size=N_VAL / (N_TRAIN + N_VAL),  # 10% for val\n",
    "    random_state=SEED,\n",
    "    stratify=df_train_val_metadata['strat_label']\n",
    ")\n",
    "\n",
    "# Assign split labels\n",
    "df_train_val_metadata['split'] = 'train'\n",
    "df_train_val_metadata.loc[val_idx, 'split'] = 'val'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SPLIT COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Train: {len(train_idx):,} ({len(train_idx)/len(df_train_val_metadata)*100:.2f}%)\")\n",
    "print(f\"Val:   {len(val_idx):,} ({len(val_idx)/len(df_train_val_metadata)*100:.2f}%)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Verify stratification\n",
    "print(\"Model distribution across train/val splits:\")\n",
    "print(pd.crosstab(df_train_val_metadata['model_name'], df_train_val_metadata['split'], normalize='columns') * 100)\n",
    "print()\n",
    "\n",
    "# Combine metadata\n",
    "# Add 'split' column to test metadata\n",
    "df_test_metadata['split'] = 'test'\n",
    "\n",
    "# Combine all metadata\n",
    "all_metadata = pd.concat([\n",
    "    df_train_val_metadata,\n",
    "    df_test_metadata\n",
    "], ignore_index=True)\n",
    "\n",
    "# Reassign traj_ids to be sequential\n",
    "all_metadata['traj_id'] = np.arange(len(all_metadata))\n",
    "\n",
    "print(f\"Total dataset: {len(all_metadata):,} trajectories\")\n",
    "print(f\"  Train: {(all_metadata['split'] == 'train').sum():,}\")\n",
    "print(f\"  Val:   {(all_metadata['split'] == 'val').sum():,}\")\n",
    "print(f\"  Test:  {(all_metadata['split'] == 'test').sum():,}\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f3dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPORTING TO HDF5 FORMAT\n",
      "======================================================================\n",
      "Creating PyTorch-ready HDF5 dataset with:\n",
      "  - Padded trajectories (positions)\n",
      "  - Dual-preprocessed displacements (raw + scaled)\n",
      "  - Padding masks for variable lengths\n",
      "  - Ground truth labels (D, alpha, model_id)\n",
      "  - SNR and sigma_noise for test set\n",
      "  - Separate train/val/test groups\n",
      "  - ANDI Table 2 compliant\n",
      "======================================================================\n",
      "\n",
      "Output file: /home/magjun/Documents/ERP_Shrey/Report_V2_Preprocessing_and_training/data/andi/andi_dataset_table2_20251130_165225.h5\n",
      "\n",
      "Preparing data arrays...\n",
      "✓ Total trajectories: 99,688\n",
      "  Train: 67,442\n",
      "  Val:   7,494\n",
      "  Test:  24,752\n",
      "\n",
      "Processing train split...\n",
      "  Writing positions...\n",
      "  Writing raw displacements...\n",
      "  Writing scaled displacements...\n",
      "  Writing ground truth labels...\n",
      "  Writing SNR metadata...\n",
      "✓ train: 67,442 trajectories written\n",
      "\n",
      "Processing val split...\n",
      "  Writing positions...\n",
      "  Writing raw displacements...\n",
      "  Writing scaled displacements...\n",
      "  Writing ground truth labels...\n",
      "  Writing SNR metadata...\n",
      "✓ val: 7,494 trajectories written\n",
      "\n",
      "Processing test split...\n",
      "  Writing positions...\n",
      "  Writing raw displacements...\n",
      "  Writing scaled displacements...\n",
      "  Writing ground truth labels...\n",
      "  Writing SNR metadata...\n",
      "✓ test: 24,752 trajectories written\n",
      "\n",
      "Writing metadata...\n",
      "✓ Metadata written\n",
      "\n",
      "======================================================================\n",
      "HDF5 EXPORT COMPLETE\n",
      "======================================================================\n",
      "✓ File: /home/magjun/Documents/ERP_Shrey/Report_V2_Preprocessing_and_training/data/andi/andi_dataset_table2_20251130_165225.h5\n",
      "✓ Size: 0.11 GB\n",
      "✓ Time: 0.1 minutes\n",
      "======================================================================\n",
      "\n",
      "Verifying HDF5 structure...\n",
      "\n",
      "Groups:\n",
      "  /metadata\n",
      "  /test\n",
      "    Datasets:\n",
      "      D: (24752,) float32\n",
      "      alpha: (24752,) float32\n",
      "      displacements_raw: (24752, 999, 1) float16\n",
      "      displacements_scaled: (24752, 999, 1) float16\n",
      "      length: (24752,) int32\n",
      "      mask_displacements: (24752, 999) bool\n",
      "      mask_positions: (24752, 1000) bool\n",
      "      model_id: (24752,) int32\n",
      "      positions: (24752, 1000, 1) float16\n",
      "      sigma_noise: (24752,) float32\n",
      "      snr: (24752,) float32\n",
      "      traj_id: (24752,) int32\n",
      "  /train\n",
      "    Datasets:\n",
      "      D: (67442,) float32\n",
      "      alpha: (67442,) float32\n",
      "      displacements_raw: (67442, 999, 1) float16\n",
      "      displacements_scaled: (67442, 999, 1) float16\n",
      "      length: (67442,) int32\n",
      "      mask_displacements: (67442, 999) bool\n",
      "      mask_positions: (67442, 1000) bool\n",
      "      model_id: (67442,) int32\n",
      "      positions: (67442, 1000, 1) float16\n",
      "      sigma_noise: (67442,) float32\n",
      "      snr: (67442,) float32\n",
      "      traj_id: (67442,) int32\n",
      "  /val\n",
      "    Datasets:\n",
      "      D: (7494,) float32\n",
      "      alpha: (7494,) float32\n",
      "      displacements_raw: (7494, 999, 1) float16\n",
      "      displacements_scaled: (7494, 999, 1) float16\n",
      "      length: (7494,) int32\n",
      "      mask_displacements: (7494, 999) bool\n",
      "      mask_positions: (7494, 1000) bool\n",
      "      model_id: (7494,) int32\n",
      "      positions: (7494, 1000, 1) float16\n",
      "      sigma_noise: (7494,) float32\n",
      "      snr: (7494,) float32\n",
      "      traj_id: (7494,) int32\n",
      "\n",
      "✓ Verification complete\n",
      "======================================================================\n",
      "\n",
      "✓ Dataset saved locally at: /home/magjun/Documents/ERP_Shrey/Report_V2_Preprocessing_and_training/data/andi/andi_dataset_table2_20251130_165225.h5\n",
      "\n",
      "\n",
      "======================================================================\n",
      "🎉 DATA GENERATION PIPELINE COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Next steps:\n",
      "1. If on Colab: Download the HDF5 file to your local machine\n",
      "2. Update training notebook dataset path\n",
      "3. Run training on local machine or Colab\n",
      "\n",
      "Final dataset location: /home/magjun/Documents/ERP_Shrey/Report_V2_Preprocessing_and_training/data/andi/andi_dataset_table2_20251130_165225.h5\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Export to HDF5 with ANDI Table 2 Structure\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "import time as time_module\n",
    "import shutil\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT TO HDF5 FORMAT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPORTING TO HDF5 FORMAT\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Creating PyTorch-ready HDF5 dataset with:\")\n",
    "print(\"  - Padded trajectories (positions)\")\n",
    "print(\"  - Dual-preprocessed displacements (raw + scaled)\")\n",
    "print(\"  - Padding masks for variable lengths\")\n",
    "print(\"  - Ground truth labels (D, alpha, model_id)\")\n",
    "print(\"  - SNR and sigma_noise for test set\")\n",
    "print(\"  - Separate train/val/test groups\")\n",
    "print(\"  - ANDI Table 2 compliant\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Determine output path based on environment\n",
    "if IN_COLAB:\n",
    "    # Generate in fast local storage first\n",
    "    output_dir = TEMP_OUTPUT_DIR\n",
    "else:\n",
    "    # Local machine - use final directory directly\n",
    "    output_dir = FINAL_OUTPUT_DIR\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate filename with timestamp\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "h5_filename = f'andi_dataset_table2_{timestamp}.h5'\n",
    "h5_path = os.path.join(output_dir, h5_filename)\n",
    "\n",
    "print(f\"Output file: {h5_path}\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# PREPARE DATA ARRAYS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Preparing data arrays...\")\n",
    "\n",
    "# Combine trajectories and metadata by split\n",
    "train_indices = all_metadata[all_metadata['split'] == 'train'].index.tolist()\n",
    "val_indices = all_metadata[all_metadata['split'] == 'val'].index.tolist()\n",
    "test_indices = all_metadata[all_metadata['split'] == 'test'].index.tolist()\n",
    "\n",
    "# Combine all trajectory data\n",
    "all_trajectories = train_val_trajectories + test_trajectories\n",
    "all_displacements_raw = train_val_displacements_raw + test_displacements_raw\n",
    "all_displacements_scaled = train_val_displacements_scaled + test_displacements_scaled\n",
    "\n",
    "print(f\"✓ Total trajectories: {len(all_trajectories):,}\")\n",
    "print(f\"  Train: {len(train_indices):,}\")\n",
    "print(f\"  Val:   {len(val_indices):,}\")\n",
    "print(f\"  Test:  {len(test_indices):,}\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE HDF5 FILE\n",
    "# =============================================================================\n",
    "\n",
    "start_time = time_module.time()\n",
    "\n",
    "with h5py.File(h5_path, 'w') as hf:\n",
    "    \n",
    "    # Process each split\n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        print(f\"Processing {split_name} split...\")\n",
    "        \n",
    "        # Get indices for this split\n",
    "        split_indices = all_metadata[all_metadata['split'] == split_name].index.tolist()\n",
    "        n_split = len(split_indices)\n",
    "        \n",
    "        if n_split == 0:\n",
    "            print(f\"  ⚠ Warning: No data for {split_name} split, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Create group for this split\n",
    "        split_group = hf.create_group(split_name)\n",
    "        \n",
    "        # Get metadata for this split\n",
    "        split_meta = all_metadata.iloc[split_indices]\n",
    "        \n",
    "        # 1. POSITIONS (padded to MAX_LENGTH)\n",
    "        print(f\"  Writing positions...\")\n",
    "        positions_padded = np.zeros((n_split, MAX_LENGTH, DIM), dtype=np.float16)  # float16 for 50% size reduction)\n",
    "        mask_positions = np.zeros((n_split, MAX_LENGTH), dtype=bool)\n",
    "        \n",
    "        for i, idx in enumerate(split_indices):\n",
    "            traj = all_trajectories[idx]\n",
    "            T = len(traj)\n",
    "            positions_padded[i, :T, :] = traj\n",
    "            mask_positions[i, :T] = True\n",
    "        \n",
    "        split_group.create_dataset(\n",
    "            'positions',\n",
    "            data=positions_padded,\n",
    "            compression=\"gzip\",  # Enable compression\n",
    "            compression_opts=4   # Level 4 = good balance\n",
    "        )\n",
    "        split_group.create_dataset('mask_positions', data=mask_positions, compression='gzip', compression_opts=4)\n",
    "        \n",
    "        # 2. DISPLACEMENTS RAW (padded to MAX_LENGTH - 1)\n",
    "        print(f\"  Writing raw displacements...\")\n",
    "        disp_raw_padded = np.zeros((n_split, MAX_LENGTH - 1, DIM), dtype=np.float16)  # float16 for 50% size reduction)\n",
    "        disp_scaled_padded = np.zeros((n_split, MAX_LENGTH - 1, DIM), dtype=np.float16)  # float16 for 50% size reduction\n",
    "        mask_disp = np.zeros((n_split, MAX_LENGTH - 1), dtype=bool)\n",
    "        \n",
    "        for i, idx in enumerate(split_indices):\n",
    "            disp = all_displacements_raw[idx]\n",
    "            T = len(disp)\n",
    "            # Ensure disp is 2D (T, 1) for proper broadcasting\n",
    "            if disp.ndim == 1:\n",
    "                disp = disp.reshape(-1, 1)\n",
    "            disp_raw_padded[i, :T, :] = disp\n",
    "            mask_disp[i, :T] = True\n",
    "        \n",
    "        split_group.create_dataset(\n",
    "            'displacements_raw',\n",
    "            data=disp_raw_padded,\n",
    "            compression=\"gzip\",\n",
    "            compression_opts=4\n",
    "        )\n",
    "        \n",
    "        # 3. DISPLACEMENTS SCALED\n",
    "        print(f\"  Writing scaled displacements...\")\n",
    "        disp_scaled_padded = np.zeros((n_split, MAX_LENGTH - 1, DIM), dtype=np.float16) # float16 for 50% size reduction)\n",
    "        \n",
    "        for i, idx in enumerate(split_indices):\n",
    "            disp = all_displacements_scaled[idx]\n",
    "            T = len(disp)\n",
    "            # Ensure disp is 2D (T, 1) for proper broadcasting\n",
    "            if disp.ndim == 1:\n",
    "                disp = disp.reshape(-1, 1)\n",
    "            disp_scaled_padded[i, :T, :] = disp\n",
    "        \n",
    "        split_group.create_dataset(\n",
    "            'displacements_scaled',\n",
    "            data=disp_scaled_padded,\n",
    "            compression=\"gzip\",\n",
    "            compression_opts=4\n",
    "        )\n",
    "        split_group.create_dataset('mask_displacements', data=mask_disp, compression='gzip', compression_opts=4)\n",
    "        \n",
    "        # 4. GROUND TRUTH LABELS\n",
    "        print(f\"  Writing ground truth labels...\")\n",
    "        \n",
    "        # Handle D values for float32 safety (prevent underflow/overflow)\n",
    "        D_values = split_meta['D'].values.copy()\n",
    "        \n",
    "        # Replace NaN and inf with a safe default value\n",
    "        D_values = np.nan_to_num(D_values, nan=1e-10, posinf=1e10, neginf=1e-10)\n",
    "        \n",
    "        # Clip to safe range before casting (float32 min ~1e-38, but use 1e-10 for safety)\n",
    "        D_values = np.clip(D_values, 1e-10, 1e10)\n",
    "        \n",
    "        # Now safe to cast to float32\n",
    "        D_values = D_values.astype(np.float32)\n",
    "        \n",
    "        # H field removed - using alpha directly\n",
    "        split_group.create_dataset('D', data=D_values)\n",
    "        split_group.create_dataset('alpha', data=split_meta['alpha'].values.astype(np.float32))\n",
    "        split_group.create_dataset('model_id', data=split_meta['model_id'].values.astype(np.int32))\n",
    "        split_group.create_dataset('length', data=split_meta['length'].values.astype(np.int32))\n",
    "        split_group.create_dataset('traj_id', data=split_meta['traj_id'].values.astype(np.int32))\n",
    "        \n",
    "        # 5. SNR FIELDS (ALL SPLITS: train/val/test all have SNR=1 or SNR=2)\n",
    "        print(f\"  Writing SNR metadata...\")\n",
    "        # Handle any potential np.inf values: convert to a large finite value for float32 storage\n",
    "        # (Note: train/val/test should all have SNR=1 or SNR=2, but this handles edge cases)\n",
    "        snr_values = split_meta['snr'].values.copy()\n",
    "        snr_values = np.where(np.isinf(snr_values), 1e6, snr_values)  # Replace inf with 1e6 (if any)\n",
    "        split_group.create_dataset('snr', data=snr_values.astype(np.float32))\n",
    "        split_group.create_dataset('sigma_noise', data=split_meta['sigma_noise'].values.astype(np.float32))\n",
    "        \n",
    "        print(f\"✓ {split_name}: {n_split:,} trajectories written\")\n",
    "        print()\n",
    "    \n",
    "    # =============================================================================\n",
    "    # METADATA\n",
    "    # =============================================================================\n",
    "    \n",
    "    print(\"Writing metadata...\")\n",
    "    metadata_group = hf.create_group('metadata')\n",
    "    \n",
    "    # Model information\n",
    "    metadata_group.create_dataset('model_names', data=np.array(list(MODEL_NAMES.values()), dtype='S10'))\n",
    "    metadata_group.create_dataset('model_ids', data=np.array(list(MODEL_NAMES.keys()), dtype=np.int32))\n",
    "    \n",
    "    # ANDI Table 2 specifications\n",
    "    metadata_group.create_dataset('andi_test_lengths', data=np.array(ANDI_TEST_LENGTHS, dtype=np.int32))\n",
    "    metadata_group.create_dataset('andi_snr_levels', data=np.array(ANDI_SNR_LEVELS, dtype=np.int32))\n",
    "    \n",
    "    # Alpha ranges per model\n",
    "    for model_id in range(5):\n",
    "        metadata_group.create_dataset(\n",
    "            f'alpha_range_model_{model_id}',\n",
    "            data=ANDI_ALPHA_SPECS[model_id].astype(np.float32)\n",
    "        )\n",
    "    \n",
    "    # Configuration attributes\n",
    "    metadata_group.attrs['seed'] = SEED\n",
    "    metadata_group.attrs['max_length'] = MAX_LENGTH\n",
    "    metadata_group.attrs['dimension'] = DIM\n",
    "    metadata_group.attrs['n_models'] = len(MODELS)\n",
    "    metadata_group.attrs['andi_table2_compliant'] = True\n",
    "    metadata_group.attrs['test_permutations'] = N_TEST_PERMUTATIONS\n",
    "    metadata_group.attrs['test_reps_per_permutation'] = N_REPS_PER_TEST_PERMUTATION\n",
    "    metadata_group.attrs['timestamp'] = timestamp\n",
    "    \n",
    "    print(\"✓ Metadata written\")\n",
    "    print()\n",
    "\n",
    "elapsed = time_module.time() - start_time\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HDF5 EXPORT COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"✓ File: {h5_path}\")\n",
    "print(f\"✓ Size: {os.path.getsize(h5_path) / 1e9:.2f} GB\")\n",
    "print(f\"✓ Time: {elapsed/60:.1f} minutes\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# VERIFICATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Verifying HDF5 structure...\")\n",
    "with h5py.File(h5_path, 'r') as hf:\n",
    "    print(\"\\nGroups:\")\n",
    "    for key in hf.keys():\n",
    "        print(f\"  /{key}\")\n",
    "        if key in ['train', 'val', 'test']:\n",
    "            print(f\"    Datasets:\")\n",
    "            for dset in hf[key].keys():\n",
    "                shape = hf[key][dset].shape\n",
    "                dtype = hf[key][dset].dtype\n",
    "                print(f\"      {dset}: {shape} {dtype}\")\n",
    "    \n",
    "    print(\"\\n✓ Verification complete\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# =============================================================================\n",
    "# COPY TO GOOGLE DRIVE (COLAB ONLY)\n",
    "# =============================================================================\n",
    "\n",
    "if IN_COLAB:\n",
    "    print()\n",
    "    print(\"=\" * 70)\n",
    "    print(\"COPYING TO GOOGLE DRIVE FOR PERSISTENCE\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    \n",
    "    final_path = os.path.join(FINAL_OUTPUT_DIR, h5_filename)\n",
    "    \n",
    "    if h5_path != final_path:\n",
    "        print(f\"Copying from: {h5_path}\")\n",
    "        print(f\"Copying to:   {final_path}\")\n",
    "        print(\"⏳ This may take 5-10 minutes for large files...\")\n",
    "        print()\n",
    "        \n",
    "        copy_start = time_module.time()\n",
    "        shutil.copy(h5_path, final_path)\n",
    "        copy_elapsed = time_module.time() - copy_start\n",
    "        \n",
    "        print(f\"✓ File copied to Google Drive in {copy_elapsed/60:.1f} minutes\")\n",
    "        print(f\"✓ File will persist after Colab session ends\")\n",
    "        print()\n",
    "        \n",
    "        # Clean up local copy to save space\n",
    "        print(\"Cleaning up local temporary file...\")\n",
    "        os.remove(h5_path)\n",
    "        print(\"✓ Local temporary file removed\")\n",
    "        print()\n",
    "        \n",
    "        # Update path reference\n",
    "        h5_path = final_path\n",
    "    else:\n",
    "        print(\"✓ File already in Google Drive\")\n",
    "    \n",
    "    print()\n",
    "    print(\"📥 TO DOWNLOAD TO YOUR LOCAL MACHINE:\")\n",
    "    print(\"Run this in a new cell:\")\n",
    "    print(\"---\")\n",
    "    print(\"from google.colab import files\")\n",
    "    print(f\"files.download('{final_path}')\")\n",
    "    print(\"---\")\n",
    "    print()\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "else:\n",
    "    print()\n",
    "    print(f\"✓ Dataset saved locally at: {h5_path}\")\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"🎉 DATA GENERATION PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Next steps:\")\n",
    "print(\"1. If on Colab: Download the HDF5 file to your local machine\")\n",
    "print(\"2. Update training notebook dataset path\")\n",
    "print(\"3. Run training on local machine or Colab\")\n",
    "print()\n",
    "print(f\"Final dataset location: {h5_path}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
